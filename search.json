[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Helloüëã and welcome to my blog!"
  },
  {
    "objectID": "posts/08_nntrain_setup/index.html",
    "href": "posts/08_nntrain_setup/index.html",
    "title": "nntrain (0/n): Preliminaries",
    "section": "",
    "text": "In this series, I want to discuss the creation of a small library for training neural networks: nntrain. It‚Äôs based off the excellent part 2 of Practical Deep Learning for Coders by Jeremy Howard, in which from lessons 13 to 18 (roughly) the development of the miniai library is discussed.\nThe library will build upon PyTorch. We‚Äôll try as much as possible to build from scratch to understand how it all works. Once the main functionality of components are implemented and verified, we can switch over to PyTorch‚Äôs version. This is similar to how things are done in the course. However, this is not just a ‚Äúcopy / paste‚Äù of the course: on many occasions I take a different route, and most of the code is my own. That is not to say that all of this is meant to be extremely innovative, instead I had the following goals:\nnb_dev is another great project from the fastai community, which allows python libraries to be written in jupyter notebooks. This may sound a bit weird and controversial, but it has the advantage that we can create the source code for our library in the very same environment in which we want to experiment and interact with our methods, objects and structure while we are building the library. For more details on why this is a good idea and other nice features of nb_dev, see here.\nSo without further ado, let‚Äôs start with some data!"
  },
  {
    "objectID": "posts/08_nntrain_setup/index.html#data",
    "href": "posts/08_nntrain_setup/index.html#data",
    "title": "nntrain (0/n): Preliminaries",
    "section": "Data",
    "text": "Data\nTo keep things simple, let‚Äôs use the fashion-mnist dataset. We can get the data from the huggingface datasets library:\n\nfrom datasets import load_dataset,load_dataset_builder\n\nname = \"fashion_mnist\"\nds_builder = load_dataset_builder(name)\nprint(ds_builder.info.description)\n\n\n\n\n\n\n\nFashion-MNIST is a dataset of Zalando's article images‚Äîconsisting of a training set of\n60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image,\nassociated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in\nreplacement for the original MNIST dataset for benchmarking machine learning algorithms.\nIt shares the same image size and structure of training and testing splits.\n\n\n\n\nds = load_dataset(name, split='train')\n\nDownloading and preparing dataset fashion_mnist/fashion_mnist (download: 29.45 MiB, generated: 34.84 MiB, post-processed: Unknown size, total: 64.29 MiB) to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1...\nDataset fashion_mnist downloaded and prepared to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1. Subsequent calls will reuse this data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nds is a Dataset object. These kind of objects appear in many Deep Learning libraries and have two main functionalities: you can index into them and they have a length:\n\nds[0]\n\n{'image': &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;,\n 'label': 9}\n\n\n\nlen(ds)\n\n60000\n\n\n\nds\n\nDataset({\n    features: ['image', 'label'],\n    num_rows: 60000\n})\n\n\nHugginface datasets (as opposed to PyTorch datasets) also have some properties, in this case num_rows, which is the length of the dataset (60000) and features, a dictionary giving metadata on what is returned when we index into the dataset:\n\nds.features\n\n{'image': Image(decode=True, id=None),\n 'label': ClassLabel(num_classes=10, names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)}\n\n\nLet‚Äôs visualize one single item:\n\nimport matplotlib.pyplot as plt\n\nimage = ds[0]['image']\nlabel = ds[0]['label']\n\nfigure, axs = plt.subplots()\n\naxs.imshow(ds[0]['image'], cmap='Greys')\naxs.set_title(f'Image of the first item in the dataset: label={label} -&gt; \"{ds.features[\"label\"].int2str(label)}\"');\naxs.axis('off');\n\n\n\n\nSince we want to start simple, and only later get to Datsets and Dataloaders: let‚Äôs pull out the data into a tensor so we can build simple linear layers.\n\nimport torchvision.transforms.functional as TF   # to transform from PIL to tensor\nimport torch\n\nx_train = [TF.to_tensor(i).view(-1) for i in ds['image']]\ny_train = [torch.tensor(i) for i in ds['label']]\n\nlen(x_train), len(y_train), len(x_train[0])\n\n(60000, 60000, 784)\n\n\nSo x_train and y_train are both lists of length 60000, and an element in x_train has length 784 (28x28 pixels)."
  },
  {
    "objectID": "posts/08_nntrain_setup/index.html#linear-layers",
    "href": "posts/08_nntrain_setup/index.html#linear-layers",
    "title": "nntrain (0/n): Preliminaries",
    "section": "Linear layers",
    "text": "Linear layers\nNow that we have the data, let‚Äôs create our very first network operation: a linear layer which takes the 784 long flattened out image vector, and maps it to an output vector of length 10\n\nimport torch\n\ndef lin(x, a, b):\n    return x@a + b\n\na = torch.randn(784, 10)\nb = torch.randn(10)\n\nout = lin(x_train[0], a, b)\nout.shape\n\ntorch.Size([10])\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor details on matrix multiplications, check out this post I wrote earlier.\n\n\nLet‚Äôs do the same for all our training data at once:\n\nx_train = torch.stack(x_train)\nout = lin(x_train, a,b)\nout.shape\n\ntorch.Size([60000, 10])\n\n\nNice, that‚Äôs basically a forward pass through our model on all our training data!\nNow if we want to increase the depth of our network by adding an additional layer, we need to add a non-linearity in the middle. Why? See for example the first paragraphs of this answer.\nLet‚Äôs add a ReLu nonlinearity:\n\ndef relu(x):\n    return x.clamp_min(0.0)\n\nAnd let‚Äôs combine these into our first ‚Äúmodel‚Äù, consisting of two linear layers and a relu nonlinearity in the middle:\n\nn_in = 784 # number of input units (28x28)\nn_h = 50   # number of hidden units\nn_out = 10 # number of output units\n\nw1 = torch.randn(n_in, n_h)\nb1 = torch.zeros(n_h)\nw2 = torch.randn(n_h, n_out)\nb2 = torch.zeros(n_out)\n\ndef model(x):\n    a1 = lin(x, w1, b1)\n    z1 = relu(a1)\n    return lin(z1, w2, b2)\n\n\nout = model(x_train)\n\n\nout.shape\n\ntorch.Size([60000, 10])\n\n\nOur ‚Äúmodel‚Äù currently only does a forward pass through the network. And as a matter of fact, it‚Äôs doing a forward pass with random weights. When training a neural network, we want to change these parameters in a way that the outputs of the network align with the outputs (y_train). I will not go into the details of this, but here is a great video by Andrej Karpathy which in my opinion gives one of the best explanations into how this works.\nBefore doing a backward pass, we first have to calculate the loss. Since the outputs represent any of the 10 classes the image corresponds with, cross entropy is a straight forward loss function. Some details about cross entropy loss can be found in a post I wrote earlier. However, since we want to add the backpropagation ourselves and I don‚Äôt know how to backpropagate through cross entropy (and I don‚Äôt feel like spending a lot of time on it), let‚Äôs use a much easier loss function for now: mean squared error (MSE). This obviously doesn‚Äôt make any sense in the context of our data, but mathematically it‚Äôs possible. We just have to end up with a single activation of our model instead of 10:\n\nn_out = 1  # number of output units changed to 1\n\nw2 = torch.randn(n_h, n_out)\nb2 = torch.zeros(n_out)\n\ndef model(x):\n    a1 = lin(x, w1, b1)\n    z1 = relu(a1)\n    return lin(z1, w2, b2)\n\nout = model(x_train)\n\n\nout.shape\n\ntorch.Size([60000, 1])\n\n\nFrom which we see that the outputs have an empty trailing dimension. y_train doesn‚Äôt have this, so we have to squeeze out this empty dimension when computing the MSE:\n\ndef mse(pred, targ): \n    return (pred.squeeze(-1)-targ).pow(2).mean() \n\ny_train = torch.stack(y_train)\nmse(out, y_train)\n\ntensor(3015.2351)\n\n\nThe next step will be to add the backward pass. But let‚Äôs refactor our code to put things into classes, that way the backward pass can be added more easily:\n\nclass Linear():\n    def __init__(self, n_in, n_out):\n        self.w = torch.randn(n_in, n_out)\n        self.b = torch.zeros(n_out)\n    \n    def __call__(self, x):\n        self.inp = x                      # storing this for the backward pass\n        self.out = x@self.w + self.b      # storing this for the backward pass\n        return self.out\n    \nclass Relu():\n    def __call__(self, x):\n        self.inp = x                      # storing this for the backward pass\n        self.out = x.clamp_min(0.)        # storing this for the backward pass\n        return self.out\n    \nclass MSE():\n    def __call__(self, pred, targ):\n        self.pred = pred                   # storing this for the backward pass\n        self.targ = targ                   # storing this for the backward pass\n        self.out = (pred.squeeze(-1)-targ).pow(2).mean()\n        return self.out\n    \nclass Model():\n    def __init__(self, n_in, n_h, n_out):\n        self.layers = [Linear(n_in, n_h), Relu(), Linear(n_h, n_out)]\n        self.loss = MSE()\n        \n    def __call__(self, x, y):\n        for l in self.layers: x = l(x)\n        return self.loss(x, y)\n\n\nx_train.shape\n\ntorch.Size([60000, 784])\n\n\n\nm = Model(n_in, n_h, n_out)\nl = m(x_train, y_train)\n\nTo add in the functionality for the backward pass, redefining the whole class is a nuisance. So instead we‚Äôll patch the classes. We can do this very easily by using the fastcore library. Let‚Äôs see a small example:\n\nimport fastcore.all as fc\n\nclass A():\n    def hi(self): print('hello üòé')\n    \na = A()\na.hi()\n\n@fc.patch\ndef hi(self:A): print('howdy ü§†')\n\na.hi()\n\nhello üòé\nhowdy ü§†\n\n\nSo with fc.patch we can extend or change the behavior of Classes that have been defined elsewhere, even on instances of the objects that are already created. Nice!\n\n@fc.patch\ndef backward(self: Linear):\n    self.inp.g = self.out.g @ self.w.t()\n    self.w.g = self.inp.t() @ self.out.g\n    self.b.g = self.out.g.sum(0)\n    \n@fc.patch\ndef backward(self: Relu):\n    self.inp.g = (self.inp&gt;0).float() * self.out.g\n    \n@fc.patch\ndef backward(self: MSE):\n    self.pred.g = 2. * (self.pred.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]\n    \n@fc.patch\ndef backward(self: Model):\n    self.loss.backward()\n    for l in reversed(self.layers): l.backward()\n\n\nm = Model(n_in, n_h, n_out)\nl = m(x_train, y_train)\nm.backward()\n\nNow the actual operations in the backward methods you will just have to take for granted as I am not going to derive them. If you want, you can have some fun (?) to try and derive it yourself. What I think is most important about these formulas:\n\nNotice that each layer has a reference to it‚Äôs inputs and it‚Äôs outputs\nDuring the backward pass, each layer uses the gradient from the outputs and uses it to set the gradient on the inputs\nThe inputs from layer \\(n\\) are the outputs from layer \\(n-1\\), so when the gradients are being set on the inputs from layer \\(n\\), this means that layer \\(n-1\\) it‚Äôs outputs are being set at the same time\nThis is the fundamental point about backpropagation of the gradient: in reverse order, layer by layer the gradients are being propagated back through the network using the chain rule\nAlthough we don‚Äôt derive the operations, we can see that that there exist operations that do this. These operations are not magical, they are just the result of calculus: not very different from the fact that if \\(f(x) = x^2\\) then \\(f'(x) = 2x\\) and if \\(h(x) = f(g(x))\\) then \\(h'(x) = f'(g(x)) * g'(x)\\)"
  },
  {
    "objectID": "posts/08_nntrain_setup/index.html#first-refactor-module-baseclass-and-training-loop",
    "href": "posts/08_nntrain_setup/index.html#first-refactor-module-baseclass-and-training-loop",
    "title": "nntrain (0/n): Preliminaries",
    "section": "First refactor: Module baseclass and training loop",
    "text": "First refactor: Module baseclass and training loop\nNow let‚Äôs see how we can make this a little better. One thing that seems a bit silly is that in each of the Linear, MSE and Relu classes, we are storing explicitly the inputs and outputs when doing a forward call. As mentioned, we need this to backpropagate the gradients. However, we rather not store that explicitly all the time when creating a new layer.\nSo let‚Äôs create a base class that takes care of this:\n\nPack the forward functionality of each layer in a dedicated forward method\nlet the storing of inputs and ouputs be done in the __call__ method of the baseclass, and call the self.forward method in between.\n\nThis works, but there is one caveat: most layers just have one input when they are called (x), but the loss has 2 (pred and targ). To make this storing of the inputs generic we can store them as an array on the base class, and also pass them as positional arguments to _backward. This way, forward and _backward have the same arguments.\n\nclass Module():\n    def __call__(self, *args):\n        self.args = args\n        self.out = self.forward(*args)\n        return self.out\n    \n    def backward(self): self._backward(*self.args)\n\n    \nclass Linear(Module):\n    def __init__(self, n_in, n_out):\n        self.w = torch.randn(n_in, n_out)\n        self.b = torch.zeros(n_out)\n    \n    def forward(self, x):\n        return x@self.w + self.b\n    \n    def _backward(self, inp):\n        inp.g = self.out.g @ self.w.t()\n        self.w.g = inp.t() @ self.out.g\n        self.b.g = self.out.g.sum(0)\n    \n    \nclass Relu(Module):\n    def forward(self, x):\n        return x.clamp_min(0.)\n    \n    def _backward(self, inp):\n        inp.g = (inp&gt;0).float() * self.out.g\n\n    \nclass MSE(Module):\n    def forward(self, pred, targ):\n        return (pred.squeeze(-1)-targ).pow(2).mean()\n    \n    def _backward(self, pred, targ):\n        pred.g = 2. * (pred.squeeze() - targ).unsqueeze(-1) / targ.shape[0]\n    \n    \nclass Model(Module):\n    def __init__(self, n_in, n_h, n_out):\n        self.layers = [Linear(n_in, n_h), Relu(), Linear(n_h, n_out)]\n        self.loss = MSE()\n        \n    def forward(self, x, y):\n        for l in self.layers: x = l(x)\n        return self.loss(x, y)\n    \n    def backward(self):\n        self.loss.backward()\n        for l in reversed(self.layers): l.backward()\n\nWith these objects, let‚Äôs create our first training loop:\n\nepochs = 5                              # train for nr of epochs\nbs     = 1024                           # batch-size\nlr     = 0.01                           # learning rate\nm = Model(n_in, n_h, n_out)             # instantiate our model\n\nfor epoch in range(epochs):             # iterate through epochs\n    for i in range(0,len(x_train), bs): # iterate through the batches\n        xb = x_train[i:i+bs]            # get minibatch \n        yb = y_train[i:i+bs]\n        \n        loss = m(xb, yb)                # forward pass\n        m.backward()                    # backward pass\n        \n        for l in m.layers:              # iterate through the layers\n            if isinstance(l, Linear):   # only update the linear layers\n                l.w += - lr * l.w.g     # update the weights\n                l.b += - lr * l.b.g     # update the bias\n\n                l.w.g = None            # reset the gradients\n                l.b.g = None\n    print(f'{epoch=} | {loss=:.1f}')\n\nepoch=0 | loss=14242.1\nepoch=1 | loss=1329.6\nepoch=2 | loss=135.2\nepoch=3 | loss=21.2\nepoch=4 | loss=9.7\n\n\nAwesome, the loss is decreasing i.e.¬†the model is training!"
  },
  {
    "objectID": "posts/08_nntrain_setup/index.html#second-refactor-simplify-the-weight-update",
    "href": "posts/08_nntrain_setup/index.html#second-refactor-simplify-the-weight-update",
    "title": "nntrain (0/n): Preliminaries",
    "section": "Second refactor: simplify the weight update",
    "text": "Second refactor: simplify the weight update\nLet‚Äôs try to simplify our training loop, and make it more generic. By adding functionality to our Module class so that it has a reference to it‚Äôs trainable parameters, we can update the weights as shown below.\n\ndef fit(epochs):\n    for epoch in range(epochs):\n        for i in range(0,len(x_train), bs):\n            xb = x_train[i:i+bs]\n            yb = y_train[i:i+bs]\n\n            loss = m(xb, yb)\n            m.backward()\n\n            for p in m.parameters():    # model has a reference to the trainable parameters\n                p -= lr * p.g           \n            m.zero_grad()               # model can reset the gradients\n        print(f'{epoch=} | {loss=:.1f}')\n\nTo do so, we will create a new baseclass (NNModule), from which our model and all the layers will inherit. We have the following conditions and properties:\n\nThe class will hold a dictionary _named_args, in which all the named arguments are stored that are set on the Module.\nThis is done by defining a __setattr__ method, which stores any named argument that doesn‚Äôt start with an _ in this dictionary\nFor the Linear, these named arguments will be the parameters w and b\nFor the Model, these named arguments will be layers (an array containing the layer objects) and loss containing the MSE object.\nBecause we want to get the parameters directly out of a layer, as well as out of the model, we need to implement some logic in _parameters() to iterate through the lowest ‚Äúlevel‚Äù and get the actual parameters out\nLast but not least we have to implement a zero_grad() method to zero the gradients on the parameters\n\n\nclass NNModule:\n    def __init__(self):\n        self._named_args = {}                           # [1]\n        \n    def __setattr__(self, name, value):                 # [2]\n        if not name.startswith(\"_\"): self._named_args[name] = value\n        super().__setattr__(name, value)\n        \n    def _parameters(self, obj):                         # [5]\n        for i in obj:\n            if isinstance(i, torch.Tensor): yield i\n            if isinstance(i, NNModule):\n                yield from iter(self._parameters(i._named_args.values()))\n            if isinstance(i, list):\n                yield from iter(self._parameters(i))\n        \n    def parameters(self):\n        return list(self._parameters(self._named_args.values()))\n    \n    def zero_grad(self):\n        for p in self.parameters():\n            p.g = None                                   # [6]\n        \n    def __call__(self, *args):\n        self._args = args                                # NOT stored under _named_args as \\\n        self._out = self.forward(*args)                  # it starts with \"_\"\n        return self._out\n    \n    def backward(self): self._backward(*self._args)\n\n\nclass Linear(NNModule):\n    def __init__(self, n_in, n_out):\n        super().__init__()\n        self.w = torch.randn(n_in, n_out)               # [3] stored under _named_args \n        self.b = torch.zeros(n_out)                     # [3] stored under _named_args\n    \n    def forward(self, x):\n        return x@self.w + self.b\n    \n    def _backward(self, inp):\n        inp.g = self._out.g @ self.w.t()\n        self.w.g = inp.t() @ self._out.g\n        self.b.g = self._out.g.sum(0)\n        \n        \nclass Relu(NNModule):\n    def forward(self, x):\n        return x.clamp_min(0.)\n    \n    def _backward(self, inp):\n        inp.g = (inp&gt;0).float() * self._out.g\n\n    \nclass MSE(NNModule):\n    def forward(self, pred, targ):\n        return (pred.squeeze(-1)-targ).pow(2).mean()\n    \n    def _backward(self, pred, targ):\n        pred.g = 2. * (pred.squeeze() - targ).unsqueeze(-1) / targ.shape[0]\n        \n        \nclass Model(NNModule):\n    def __init__(self, n_in, n_h, n_out):\n        super().__init__()\n        self.layers = [Linear(n_in, n_h), Relu(), Linear(n_h, n_out)]\n        self.loss = MSE()                              # [4] &lt; and ^ are stored under _named_args\n        \n    def forward(self, x, y):\n        for l in self.layers: x = l(x)\n        return self.loss(x, y)\n    \n    def backward(self):\n        self.loss.backward()\n        for l in reversed(self.layers): l.backward()\n\nAnd now we can indeed call parameters on both the model as well as on individual layers:\n\nm = Model(n_in, n_h, n_out)\n[p.shape for p in m.parameters()]\n\n[torch.Size([784, 50]), torch.Size([50]), torch.Size([50, 1]), torch.Size([1])]\n\n\n\n[p.shape for p in Linear(n_in, n_h).parameters()]\n\n[torch.Size([784, 50]), torch.Size([50])]\n\n\nLet‚Äôs fit with our new training loop:\n\nfit(5)\n\nepoch=0 | loss=2118316928.0\nepoch=1 | loss=195283376.0\nepoch=2 | loss=18002500.0\nepoch=3 | loss=1659511.5\nepoch=4 | loss=152958.9"
  },
  {
    "objectID": "posts/08_nntrain_setup/index.html#third-refactor-use-nn.module",
    "href": "posts/08_nntrain_setup/index.html#third-refactor-use-nn.module",
    "title": "nntrain (0/n): Preliminaries",
    "section": "Third refactor: use nn.Module",
    "text": "Third refactor: use nn.Module\nFinally we are in a position to use PyTorch‚Äôs nn.Module, since we understand all of it‚Äôs behavior! We can simplify:\n\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, n_in, n_h, n_out):\n        super().__init__()\n        self.layers = [nn.Linear(n_in, n_h), nn.ReLU(), nn.Linear(n_h, n_out)]\n        for i,l in enumerate(self.layers):               # ^ we use the nn.Linear and nn.ReLU from PyTorch\n            self.add_module(f'layer_{i}', l)             # we need to register the modules explicitly\n        self.loss = nn.MSELoss()                         # we use the MSELoss from PyTorch\n        \n    def forward(self, x, y):\n        for l in self.layers: x = l(x)\n        return self.loss(x.squeeze(-1), y)\n\n\n# Autograd needs all tensors to be float\nx_train = x_train.to(torch.float32)\ny_train = y_train.to(torch.float32)\nm = Model(n_in, n_h, n_out)\n\n\ndef fit(epochs):\n    for epoch in range(epochs):\n        for i in range(0,len(x_train), bs):\n            xb = x_train[i:i+bs]\n            yb = y_train[i:i+bs]\n\n            loss = m(xb, yb)\n            loss.backward()\n\n            with torch.no_grad():\n                for p in m.parameters():\n                    p -= lr * p.grad\n                m.zero_grad()\n        print(f'{epoch=} | {loss=:.1f}')"
  },
  {
    "objectID": "posts/08_nntrain_setup/index.html#fourth-refactor-nn.modulelist-and-nn.sequential",
    "href": "posts/08_nntrain_setup/index.html#fourth-refactor-nn.modulelist-and-nn.sequential",
    "title": "nntrain (0/n): Preliminaries",
    "section": "Fourth refactor: nn.ModuleList and nn.Sequential",
    "text": "Fourth refactor: nn.ModuleList and nn.Sequential\nTo simplify the storing of the layers array and the registration of the modules, we can use nn.ModuleList. Up till now, we compute the loss as part of the forward pass of the model, let‚Äôs change that and let the model return the predictions. With these predictions we can now also compute a metric: accuracy, which will represent the percentage of images correctly classified by the model.\n\nclass Model(nn.Module):\n    def __init__(self, n_in, n_h, n_out):\n        super().__init__()\n        self.layers = nn.ModuleList([nn.Linear(n_in, n_h), nn.ReLU(), nn.Linear(n_h, n_out)])\n        \n    def forward(self, x, y):\n        for l in self.layers: x = l(x)\n        return x\n\nThis turns out to be such an elementary operation, that PyTorch has a module for it: nn.Sequential.\n\nimport torch.nn.functional as F\n\n\nlayers = [nn.Linear(n_in,n_h), nn.ReLU(), nn.Linear(n_h, n_out)]\nmodel = nn.Sequential(*layers)\n\nAnd let‚Äôs update our training loop as we mentioned:\n\nThe loss needs to be computed separately, since we took it out of the model\nLet‚Äôs now also use a loss function that actually makes sense: cross entropy loss instead of MSE\nWe then need to switch back to using 10 output activations conforming with the 10 categories\n\n\nn_out = 10\n\nlayers = [nn.Linear(n_in,n_h), nn.ReLU(), nn.Linear(n_h, n_out)]\nmodel = nn.Sequential(*layers)\n\nLet‚Äôs also add a metric: accuracy, to see how our model is doing. For this, we need to find the class that our model predicts. However, the model is outputting not a single class, it outputs logits: the unweighted predictions for any of the 10 classes. When applying a softmax to these logits, we turn them into 10 probabilities: the probability that our model assigns to each class.\nWhen computing the accuracy, we don‚Äôt actually just use the logits instead of the probabilities, since the softmax is a monotonically increasing we largest logit, will also have the largest probability.\n\nx0 = x_train[0]\nlogits = model(x0)\n\nprint(f'{logits=}')                           # Logit output of the model\n\nprobs = logits.softmax(dim=0)\n\nprint(f'{probs=}')                            # class probabilites\n\nassert torch.allclose(probs.sum(),            # probabilities sum to 1\n                      torch.tensor(1.0))      \n\nassert torch.all(probs &gt; 0)                   # no negative probabilities\n\nassert (logits.argmax() == probs.argmax())\n\nlogits=tensor([-0.1345,  0.1549, -0.0635,  0.0619,  0.0516, -0.0358,  0.1625, -0.0322,\n        -0.0614,  0.1931], grad_fn=&lt;AddBackward0&gt;)\nprobs=tensor([0.0844, 0.1127, 0.0906, 0.1027, 0.1016, 0.0931, 0.1136, 0.0935, 0.0908,\n        0.1171], grad_fn=&lt;SoftmaxBackward0&gt;)\n\n\n\ndef accuracy(preds, targs):\n    return (preds.argmax(dim=1) == targs).float().mean()\n\n\nloss_func = F.cross_entropy\ny_train = y_train.to(torch.long)\n\nfor epoch in range(epochs):\n    for i in range(0,len(x_train), bs):\n        xb = x_train[i:i+bs]\n        yb = y_train[i:i+bs]\n\n        preds = model(xb)\n        acc = accuracy(preds, yb)\n        loss = loss_func(preds, yb)\n        loss.backward()\n\n        with torch.no_grad():\n            for p in model.parameters():\n                p -= lr * p.grad\n            model.zero_grad()\n    print(f'{epoch=} | {loss=:.3f} | {acc=:.3f}')\n\nepoch=0 | loss=1.071 | acc=0.684\nepoch=1 | loss=0.992 | acc=0.681\nepoch=2 | loss=0.934 | acc=0.688\nepoch=3 | loss=0.889 | acc=0.697\nepoch=4 | loss=0.853 | acc=0.706"
  },
  {
    "objectID": "posts/08_nntrain_setup/index.html#fifth-refactor-add-an-optimizer",
    "href": "posts/08_nntrain_setup/index.html#fifth-refactor-add-an-optimizer",
    "title": "nntrain (0/n): Preliminaries",
    "section": "Fifth refactor: add an Optimizer",
    "text": "Fifth refactor: add an Optimizer\nWe can further refactor the model by adding an Optimizer, this is an object that will have access to the parameters and does the updating of the weights (step) and zeroing the gradient. Most notably, we want to go from:\n\n# ...\n# with torch.no_grad():\n#     for p in model.parameters():\n#         p -= lr * p.grad\n#     model.zero_grad()\n# ...\n\nto:\n\n# opt.step()\n# opt.zero_grad()\n\nSo that the training loop becomes:\n\ndef fit(epochs):\n    for epoch in range(epochs):\n        for i in range(0,len(x_train), bs):\n            xb = x_train[i:i+bs]\n            yb = y_train[i:i+bs]\n\n            preds = model(xb)\n            acc = accuracy(preds, yb)\n            loss = loss_func(preds, yb)\n            loss.backward()\n\n            opt.step()                       # optimizer takes care of the weight update\n            opt.zero_grad()                  # as well as zeroing the grad\n        print(f'{epoch=} | {loss=:.3f} | {acc=:.3f}')\n\nSo we introduce the Optimizer, which has exactly these two methods:\n\nclass Optimizer():\n    def __init__(self, params, lr=0.5):\n        self.params = list(params)\n        self.lr = lr\n        \n    def step(self):\n        with torch.no_grad():\n            for p in self.params: p -= self.lr * p.grad\n        \n    def zero_grad(self):\n        with torch.no_grad():\n            for p in self.params: p.grad.zero_()\n\n\nlayers = [nn.Linear(n_in,n_h), nn.ReLU(), nn.Linear(n_h, n_out)]\nmodel = model = nn.Sequential(*layers)\nopt = Optimizer(model.parameters(), lr)\n\n\nfit(5)\n\nepoch=0 | loss=2.074 | acc=0.447\nepoch=1 | loss=1.832 | acc=0.582\nepoch=2 | loss=1.571 | acc=0.653\nepoch=3 | loss=1.354 | acc=0.676\nepoch=4 | loss=1.195 | acc=0.673\n\n\nThe optimizer we just created is basically the SGD optimizer from PyTorch so let‚Äôs use that:\n\ndef get_model():\n    layers = [nn.Linear(n_in, n_h), nn.ReLU(), nn.Linear(n_h, n_out)]\n    model = nn.Sequential(*layers)\n    \n    opt = torch.optim.SGD(model.parameters(), lr)\n    \n    return model, opt\n\nmodel, opt = get_model()\nfit(5)\n\nepoch=0 | loss=2.026 | acc=0.456\nepoch=1 | loss=1.751 | acc=0.559\nepoch=2 | loss=1.502 | acc=0.605\nepoch=3 | loss=1.314 | acc=0.630\nepoch=4 | loss=1.179 | acc=0.635"
  },
  {
    "objectID": "posts/08_nntrain_setup/index.html#end",
    "href": "posts/08_nntrain_setup/index.html#end",
    "title": "nntrain (0/n): Preliminaries",
    "section": "End",
    "text": "End\nWe have come a long way, and covered a lot of ground. We have seen many of the fundamental components of training a neural network: the data, a simple model, training loops, loss functions, metrics and optimizers. We have seen why things like nn.Module exist, and understand it‚Äôs behavior. Furthermore, we have seen that the need for nn.Module and torch.optim comes out of the need for simplifying things in the training loop.\nIn the next post, we will get to datasets and dataloaders as a way to further improve the training loop, and we will start adding our first things into the nntrain library üï∫."
  },
  {
    "objectID": "posts/06_stable_diffusion_basics/index.html",
    "href": "posts/06_stable_diffusion_basics/index.html",
    "title": "Introduction to Stable Diffusion - Concepts",
    "section": "",
    "text": "Stable Diffusion, a generative deep learning algorithm developed in 2022, is capable of creating images from prompts. For example, when presented the prompt: A group of people having lunch on the moon, the algorithm creates the following image:\nAnd although this image isn‚Äôt perfect, it‚Äôs pretty amazing that it took less then 30 seconds to create this image. The algorithm ‚Äúimagined‚Äù that people on the moon should be wearing space suits, and that lunch is generally eaten in a sitting position and around a table. Also, the surroundings look indeed pretty moonish. Not bad at all!\nIn this post, we will have a look at the main components involved in creating this image, and follows largely the steps of Lesson 9 of Deep Learning for Coders."
  },
  {
    "objectID": "posts/06_stable_diffusion_basics/index.html#intuition",
    "href": "posts/06_stable_diffusion_basics/index.html#intuition",
    "title": "Introduction to Stable Diffusion - Concepts",
    "section": "Intuition",
    "text": "Intuition\nConsider some kind of black box system that takes some input data, and based on it creates some output data. Let‚Äôs say, it takes an image of a handwritten digit as input, and outputs the probability that the image is indeed a hand written digit. Visually something like this:\n\n\n\n\n\nIn statistics, we would call this a probability density function. It‚Äôs a function that takes data as input, and gives the probability that\n\nif the input data is coming indeed from the distribution,\nwhat‚Äôs the probability that we see this data?\n\nApplied to our use-case: if the presented image is indeed from the distribution (of images) that represent hand written digits, what‚Äôs the probability that we observe the presented image?\nWith such a system, we could start with an image made up of pure noise and iteratively do:\n\nget the probability \\(p_0\\) of the image being a handwritten digit from the black box system\nchange the value of one of the pixels at random\nget the new probability \\(p_1\\) whether the image is a handwritten digit from the black box system\nwhen \\(p_1 &gt; p_0\\) update the image with the changed pixel value\n\nWhen following this procedure long enough and thus updating pixel for pixel, we would gradually change all the values of our pixels of our image, until eventually it will start to resemble a handwritten digit.\nIn principle, this is the simple intuition behind stable diffusion."
  },
  {
    "objectID": "posts/06_stable_diffusion_basics/index.html#the-main-component-unet",
    "href": "posts/06_stable_diffusion_basics/index.html#the-main-component-unet",
    "title": "Introduction to Stable Diffusion - Concepts",
    "section": "The main component: Unet",
    "text": "The main component: Unet\nSo how are we going to create this system that will return the probability that an image is depicting a handwritten digit? Let‚Äôs try to create a model, that will do so. To get the training data, we need lots of images that depict handwritten digits. Something like this:\n\n\n\n\n\nSince these images represent actual hand-written digits, the system will need to output a probability close to 1 for these images. But how do we get images that ‚Äúsomewhat‚Äù or ‚Äúrarely‚Äù represent handwritten digits and are associated with lower probability values? We somehow have to ‚Äúcrappify‚Äù these existing images. We can do this by using these same images and sprinkle them with different amounts of noise. The more noise we add, the less the image will resemble a handwritten digit. Visually:\n\n\n\n\n\nNow we can train a network which we feed the noisified images as input and use the noise image as label. So instead of predicting the probability that an image depicts a handwritten digit, the model will predict the noise. By using a simple MSE loss on the actual noise (labels) and the predictions the model will learn how to predict the noise from looking at a noisified images.\nThe idea behind this model is that once this model is trained, we could run inference on some random noise. The model will give us a prediction of all the noise in the image, which when removed from the input, renders an image of a digit.\nIt turns out that this process works much better if, instead of removing all the noise that was predicted by the model at once, we just remove a little bit of the noise that was predicted. This way, we end up with an image which is just a bit less noisy then what we started with. We then feed this less noisy image again into our network, and thus iteratively remove more and more noise from the image, until after a certain amount of steps (50 for example) we end-up with an image that is free of noise.\nOne model architecture that is takes images as input and also outputs images is called a Unet and forms the first component of our Stable Diffusion system:\n\n\n\n\n\n\n\n\nComponent\nInputs\nOutputs\n\n\n\n\nUnet\nNoisy images\nNoise"
  },
  {
    "objectID": "posts/06_stable_diffusion_basics/index.html#compression-variational-autoencoder",
    "href": "posts/06_stable_diffusion_basics/index.html#compression-variational-autoencoder",
    "title": "Introduction to Stable Diffusion - Concepts",
    "section": "Compression: Variational Autoencoder",
    "text": "Compression: Variational Autoencoder\nWhen working with images in neural networks we often reduce the resolution of images or use smaller patches of the original image to make sure everything fits on the GPU. With stable diffusion, we naturally want to output images of high resolution, so we either need very large GPUs, or instead we use a compression trick by making use of a Variational Autoencoder (VAE).\nA VAE is a network architecture having an encoder and a decoder. In the encoder the image input is being transformed through a series of convolutional layers into a compressed representation, the latent. In the decoder this compressed latent is passed through a series of layers that are trying to reconstruct the original image. Visually:\n\n\n\n\n\nThis might look like a boring network architecture at first. But it‚Äôs actually a very neat way to compress things: We can feed this model all the different noisified images mentioned earlier, and use an MSE loss on the inputs and outputs. This will train the model to create compressed representations of our images (the latents) that can be used by the decoder to recreate the original image. This means that the latent representation carries close to the same amount of ‚Äúinformation‚Äù as our full-size images.\nWith this, we can now train the previously discussed Unet on all the latents instead of the full size images!\nDuring inference the combined architecture looks like this: we run any input first through the encoder returning a highly compressed version of our input (i.e.¬†the latents). We then run it through the Unet, which will output a latent representation of the noise. If we (partly) subtract the noise latent from the noisy image latent, we end up with a latent representation of our image which is a bit less noisy then what we started with. Finally, to move from latent representation to full-size images, we can use the decoder of the VAE. Visually:\n\n\n\n\n\nTo summarize:\n\n\n\n\n\n\n\n\nComponent\nInputs\nOutputs\n\n\n\n\nVAE encoder\nNoisy image\nNoisy image latents\n\n\nUnet\nNoisy image latents\nNoise latents\n\n\nVAE decoder\nNoise latents\nNoise"
  },
  {
    "objectID": "posts/06_stable_diffusion_basics/index.html#prompting-clip",
    "href": "posts/06_stable_diffusion_basics/index.html#prompting-clip",
    "title": "Introduction to Stable Diffusion - Concepts",
    "section": "Prompting: CLIP",
    "text": "Prompting: CLIP\nSo how can we create prompting? Let‚Äôs start simple and imagine we just want to specify which handwritten digit we would like to generate, so any number between 0 and 9. We could do this by training the Unet not only on the noisy image (input) and noise (output), but instead also give it a representation of the digit we sprinkled the noise on as input. The most generic way to do this, would be to create a one-hot encoded representation of the digit, visually:\n\n\n\n\n\nTo create an image depicting the digit ‚Äúthree‚Äù from pure noise, we would then start with a random noise latent and feed it together with the one-hot encoded representation of the digit into the Unet. This way, the Unet is ‚Äúguided‚Äù to create an image of digit ‚Äúthree‚Äù and not just any image, visually:\n\n\n\n\n\nTo continue, how are we going to scale this for any text prompt besides our 10 digits? We can‚Äôt possibly create a one-hot encoding of any possible prompt, that would make our vector infinitely large. Instead, we want to compress the encoding in some finite, high dimensional space, e.g.¬†we want to create an embedding encoding of our prompt.\nTo create these embeddings, we first of all need again lots of data. For example by capturing a lot of images from the internet, these image generally have a textual description in the HTML tag.\nWe can feed the text and images into two separate encoders. These encoders take the text and image respectively and output a vector. Next, we can align the vector representations in a matrix and take the dot-product between them. We want the text and image vectors of the same ‚Äúobject‚Äù to align, this means their dot-product should be large. Also, we want the vectors of different objects to not align, so their dot-product should be small. Visually:\n\n\n\n\n\nA loss function that does exactly this, is called the Contrastive Loss. And the model described here is called Contrastive Language Image Pre-training (CLIP).\nDuring inference, we can use the trained text-encoder and apply it to the prompt. The outputted embedding can then be used as the encoding we feed into our Unet in combination with the noisy image latent.\nTo summarize:\n\n\n\n\n\n\n\n\nComponent\nInputs\nOutputs\n\n\n\n\nCLIP text encoder\nPrompt\nEmbedding\n\n\nVAE encoder\nNoisy image\nNoisy image latents\n\n\nUnet\nNoisy image latents + Prompt embedding\nNoise latents\n\n\nVAE decoder\nNoise latents\nNoise"
  },
  {
    "objectID": "posts/06_stable_diffusion_basics/index.html#noise-scheduler",
    "href": "posts/06_stable_diffusion_basics/index.html#noise-scheduler",
    "title": "Introduction to Stable Diffusion - Concepts",
    "section": "Noise scheduler",
    "text": "Noise scheduler\nAbove it was stated, that ‚Äúdifferent‚Äù amounts of noise are sprinkled on our images during training, and during inference ‚Äúsome‚Äù amount of noise is being subtracted from the image. In the next post, which will be a ‚Äúcode‚Äù version of this post, we will see more how this exactly works, but let‚Äôs introduce one more concept here:\nTo formalize the amounts of noise we will use something called a noise schedule, which maps an integer value (called the timestep \\(t\\)) to an amount of noise we will add to our image. This noise schedule is a monotonically decreasing function of \\(t\\), so large values of \\(t\\) will add a small amount of noise and small values of \\(t\\) add a large amount of noise. A typical noise schedule looks something like this:\n\n\n\n\n\nWith this noise schedule, we can pick different amounts of noise during training and add it to the images in the batch. Additionally, we will feed the noise parameter to the Unet, so that it knows how much noise was added to the image. This sould make it easier for the model to reconstruct the noise.\nTo summarize:\n\n\n\n\n\n\n\n\nComponent\nInputs\nOutputs\n\n\n\n\nCLIP text encoder\nPrompt\nEmbedding\n\n\nVAE encoder\nNoisy image\nNoisy image latents\n\n\nUnet\nNoisy image latents + Prompt embedding + Noise level\nNoise latents\n\n\nVAE decoder\nNoise latents\nNoise\n\n\n\nThat‚Äôs it for now! If you came this far, I hope you enjoyed it. For me, it helped a lot in my understanding by writing all this down. In the next blog post, we will have a look at how these concepts translate into code by making use of HuggingFace libraries such as diffusers and transformers"
  },
  {
    "objectID": "posts/05_crossentropy/index.html",
    "href": "posts/05_crossentropy/index.html",
    "title": "Cross entropy any which way",
    "section": "",
    "text": "Cross entropy is one of the most commonly used loss functions. In this post, we will have a look at how it works, and compute it in a couple of different ways.\nConsider a network that is build for image classification. During the forward pass, images are passed into the network and the network processes the data layer by layer, until evenually some final activations are being returned by the model. These final activations are called ‚Äúlogits‚Äù and represent the unnormalized predictions of our model.\nSince we generally use mini-batches during training, these logits are of shape [bs, num_classes]\nimport torch\nimport torch.nn.functional as F\n\ng = torch.manual_seed(42) # use a generator for reproducability\n\nbs = 32 # batch size of 32\nnum_classes = 3 # image classification with 3 different classes\n\nlogits = torch.randn(size=(bs, num_classes), generator=g) # size: [32,3]\n\nlogits[0:4] # show the logits for the first couple of samples\n\ntensor([[ 1.9269,  1.4873,  0.9007],\n        [-2.1055,  0.6784, -1.2345],\n        [-0.0431, -1.6047, -0.7521],\n        [ 1.6487, -0.3925, -1.4036]])\nEach row of this tensor represents the unnormalized predictions for each of our samples in the batch. We can normalize these predictions by applying a softmax. The softmax function does two things:\nThis makes sure that we can treat the output of this as probabilities, because:\nSpecifically:\n# Unnormalized predictions for our first sample (3 classes)\nlogits[0]\n\ntensor([1.9269, 1.4873, 0.9007])\n# Exponentiated predictions, making them all positive\nexp_logits = logits[0].exp()\nexp_logits\n\ntensor([6.8683, 4.4251, 2.4614])\n# Turn these values into probabilities by dividing by the sum\nprobs = exp_logits / exp_logits.sum()\n\n# verify that the sum of the probabilities sum to 1\nassert torch.allclose(probs.sum(), torch.tensor(1.))\n\nprobs\n\ntensor([0.4993, 0.3217, 0.1789])\nSo, let‚Äôs create a softmax function that does this for a whole batch:\ndef softmax(logits):\n    exp_logits = logits.exp() # shape: [32, 3]\n    exp_logits_sum = exp_logits.sum(dim=1, keepdim=True) # shape: [32, 1]\n    \n    # Note: this get's correctly broadcasted, since the exp_logits_sum will \n    # expand to [32, 3], so each value in exp_logits gets divided by the sum over its row\n    probs = exp_logits / exp_logits_sum # shape: [32, 3]\n    \n    return probs \n\nprobs = softmax(logits)\nprobs[0:4]\n\ntensor([[0.4993, 0.3217, 0.1789],\n        [0.0511, 0.8268, 0.1221],\n        [0.5876, 0.1233, 0.2891],\n        [0.8495, 0.1103, 0.0401]])\nNext, we want to compute the loss for which also need our labels. These labels represent the ground truth class for each of our samples in the batch. Since we have 3 classes they will be between 0 and 3 (e.g.¬†either 0, 1 or 2)\ng = torch.manual_seed(42) # use a generator for reproducability\n\nlabels = torch.randint(low=0, high=3, size=(32,), generator=g)\nlabels\n\ntensor([0, 2, 1, 1, 0, 2, 1, 2, 1, 2, 1, 1, 2, 0, 0, 1, 2, 1, 0, 1, 1, 2, 1, 2,\n        2, 1, 2, 0, 1, 1, 0, 0])\nFor classification we use the Negative Log Likelihood loss function, which is defined as such:\n\\[\n\\textrm{NLL} = - \\sum_{i}{q_i * \\log(p_i)}\n\\]\nwith \\(i\\) being the index that moves along the classes (3 in our example) and \\(q_i\\) being the probability that the ground truth label is class \\(i\\) (this is a somewhat strange formulation, since this probability is either 1 (for the correct class) or 0 (for all the non-correct classes)). Finally, \\(p_i\\) is the probability that the model associated to class \\(i\\).\nFor the very first row of our probs ([0.4993, 0.3217, 0.1789]) and our first label (0) we thus get:\n\\[\\begin{align}\n\\textrm{NLL} &= - ( (1 \\cdot \\log(0.4993)) + (0 \\cdot \\log(0.3217)) + (0 \\cdot \\log(0.1789)) ) \\\\\n\\textrm{NLL} &= - ( (1 \\cdot \\log(0.4993)) ) \\\\\n\\textrm{NLL} &= - \\log(0.4993)\n\\end{align}\\]\nFrom which we see that it‚Äôs just the negative log of the probability associated with the ground truth class.\nSince this computes only the NLL per sample, we also need a way to combine the NLL across the samples in our batch. We can do this either by summing or averaging, averaging has the advantage that the size of the loss remains the same when we change the batch-size, so let‚Äôs use that:\ndef nll(probs, labels):\n    # probs: shape [32, 3]\n    # labels: shape [32]\n    \n    # this plucks out the probability of the ground truth label per sample, \n    # it uses \"numpy's integer array indexing\":\n    # https://numpy.org/doc/stable/user/basics.indexing.html#integer-array-indexing\n    probs_ground_truth_class = probs[range(len(labels)), labels] # shape: [32]\n    \n    nll = -torch.log(probs_ground_truth_class).mean() # shape: []\n    return nll\nnll(probs, labels)\n\ntensor(1.3465)"
  },
  {
    "objectID": "posts/05_crossentropy/index.html#using-pytorch",
    "href": "posts/05_crossentropy/index.html#using-pytorch",
    "title": "Cross entropy any which way",
    "section": "Using PyTorch",
    "text": "Using PyTorch\nInstead of using our custom softmax, we can also use the build-in softmax function from PyTorch:\n\np = F.softmax(logits, dim=1) # dim=1 --&gt; compute the sum across the columns\nnll(p, labels)\n\ntensor(1.3465)\n\n\nInstead of using our custom nll we can also use the build-in version from PyTorch. However, nll_loss expects the log of the softmax (for numerical stability) so instead of softmax we have to use log_softmax:\n\np = F.log_softmax(logits, dim=1)\n\n# Assert that indeed the log_softmax is just the softmax followed by a log\nassert torch.allclose(p, F.softmax(logits, dim=1).log())\n\ntorch.nn.functional.nll_loss(p, labels)\n\ntensor(1.3465)\n\n\nThe combination of softmax and nll is called cross entropy, so we can also use PyTorch‚Äôs build-in version of that:\n\nF.cross_entropy(logits, labels)\n\ntensor(1.3465)\n\n\nInstead of the methods in nn.functional, we can also use classes. For that, we first create an instance of the object, and then ‚Äúcall‚Äù the instance:\n\nce = torch.nn.CrossEntropyLoss() # create a CrossEntropyLoss instance\nce(logits, labels) # calling the instance with the arguments returns the cross entropy\n\ntensor(1.3465)\n\n\nSimilarly, we can use classes for the log_softmax and nll_loss functions\n\nls = torch.nn.LogSoftmax(dim=1)\nnll = torch.nn.NLLLoss()\n\np = ls(logits)\nnll(p, labels)\n\ntensor(1.3465)\n\n\nThis is practical, if we want specify custom behavior of the loss function ahead of time of calling the actual loss function. For example, let‚Äôs say we want to compute the cross entropy loss based on ‚Äòsums‚Äô instead of ‚Äòaverages‚Äô. Then when using the method in F we would do:\n\nF.cross_entropy(logits, labels, reduction='sum')\n\ntensor(43.0866)\n\n\nSo whenever we call the loss, we have to specify the additional reduction argument.\nWhereas when using the loss classes, we can instantiate the class with that reduction argument, and then call the instance as per usual without passing anything but the logits and the labels:\n\n# instantiate \nce = torch.nn.CrossEntropyLoss(reduction='sum')\n\n# at some other point in your code, compute the loss as per default\nce(logits, labels)\n\ntensor(43.0866)\n\n\nThis is practical when the loss function is getting called by another object to which we don‚Äôt have easy access. So that we can‚Äôt easily change the arguments for that call. This is for example the case when using the FastAI Learner class, to which we pass the loss function which then get‚Äôs called by the Learner object with the default arguments (logits and labels). By using the classes, we can specify the reduction argument ahead of time and pass that instance to the Learner class."
  },
  {
    "objectID": "posts/11_nntrain_activations/index.html",
    "href": "posts/11_nntrain_activations/index.html",
    "title": "nntrain (3/n): Activations, Initialization and Normalization",
    "section": "",
    "text": "In this series, I want to discuss the creation of a small library for training neural networks: nntrain. It‚Äôs based off the excellent part 2 of Practical Deep Learning for Coders by Jeremy Howard, in which from lessons 13 to 18 (roughly) the development of the miniai library is discussed.\nThe library will build upon PyTorch. We‚Äôll try as much as possible to build from scratch to understand how it all works. Once the main functionality of components are implemented and verified, we can switch over to PyTorch‚Äôs version. This is similar to how things are done in the course. However, this is not just a ‚Äúcopy / paste‚Äù of the course: on many occasions I take a different route, and most of the code is my own. That is not to say that all of this is meant to be extremely innovative, instead I had the following goals:\nnb_dev is another great project from the fastai community, which allows python libraries to be written in jupyter notebooks. This may sound a bit weird since the mainstream paradigm is to only do experimental work in notebooks. It has the advantage though that we can create the source code for our library in the very same environment in which we want to experiment and interact with our methods, objects and structure while we are building the library. For more details on why this is a good idea and other nice features of nb_dev, see here.\nSo without further ado, let‚Äôs start with where we left off in the previous post:"
  },
  {
    "objectID": "posts/11_nntrain_activations/index.html#end-of-last-post",
    "href": "posts/11_nntrain_activations/index.html#end-of-last-post",
    "title": "nntrain (3/n): Activations, Initialization and Normalization",
    "section": "End of last post:",
    "text": "End of last post:\nIn the last two posts we build up the learner and dataloader module. In this post, we are going to use everything we have build so far to build and train a new model. From the naive MLP model we have been using so far, we will switch to a convolutional neural network (CNN). We will investigate performance and go into the fine print of making sure the networks trains well. We will do this by looking at the activations throughout the network.\n\nfrom datasets import load_dataset,load_dataset_builder\n\nfrom nntrain.dataloaders import DataLoaders, hf_ds_collate_fn\nfrom nntrain.learner import *\n\n\n #| export\nimport torchvision.transforms.functional as TF\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom operator import attrgetter\nfrom functools import partial\nimport fastcore.all as fc\nimport math\nimport torcheval.metrics as tem\nimport matplotlib.pyplot as plt\nimport random\nimport numpy as np\n\n\n #| export\ndef set_seed(seed, deterministic=False):\n    torch.use_deterministic_algorithms(deterministic)\n    torch.manual_seed(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n\nSince a CNN operates on a pixel grid instead of a flat array of pixel values, we have to load the data differently. For this we can use the Dataloaders module we have created in an earlier post. By using the hf_ds_collate_fn with flatten=False, we keep the pixel grid and end up with data have a shape of [batch, channels, height, width]:\n\nname = \"fashion_mnist\"\nds_builder = load_dataset_builder(name)\nhf_dd = load_dataset(name)\n\nbs = 1024\n\ncollate = partial(hf_ds_collate_fn, flatten=False)\n\ndls = DataLoaders.from_hf_dd(hf_dd, batch_size=bs, collate_fn=collate)\n\nxb, yb = next(iter(dls.train))\nxb.shape\n\n\n\n\n\n\n\nDownloading and preparing dataset fashion_mnist/fashion_mnist (download: 29.45 MiB, generated: 34.84 MiB, post-processed: Unknown size, total: 64.29 MiB) to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1...\nDataset fashion_mnist downloaded and prepared to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1. Subsequent calls will reuse this data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.Size([1024, 1, 28, 28])"
  },
  {
    "objectID": "posts/11_nntrain_activations/index.html#convolutional-neural-network",
    "href": "posts/11_nntrain_activations/index.html#convolutional-neural-network",
    "title": "nntrain (3/n): Activations, Initialization and Normalization",
    "section": "Convolutional Neural Network",
    "text": "Convolutional Neural Network\nNext, let‚Äôs create a simple CNN consisting of a couple of convolutions with ReLU activations in between:\n\ndef conv_block(in_c, out_c, kernel_size=3, act=True):\n    padding = kernel_size // 2    # don't lose pixels of the edges\n    stride = 2                    # reduce the image size by factor of 2 in x and y directions\n    conv = torch.nn.Conv2d(in_c, out_c, kernel_size, stride, padding)\n    if act: return nn.Sequential(conv, torch.nn.ReLU())\n    else: return conv\n\nThe model will receive images of shape 28x28 and since we use a stride of 2 in each convolution, the pixel grid will be reduced by a factor of 2 in both the x and y direction (a factor of 4 in total). At the same time we increase the number of filters by a factor of 2 so that the overall data-reduction of a single convolution is roughly a factor of 2.\nIn the very first convolution we go from 1 input channel to 8 output channels. To do that, we increase the kernel size from 3 to 5. For kernel size 3, we would have 3x3 pixels (x 1 input channel) that would map to single position in the output grid. The output grid has 8 channels, so we would go from 9 (3x3x1) to 8 (1x1x8) values. Going from 9 to 8 values is practically no reduction, which doesn‚Äôt allow the convolution to learn anything. So instead we take a kernel of size 5 so that we go from 25 (5x5x1) to 9 values and have roughly a 2x reduction.\n\ndef cnn_layers():\n    return nn.Sequential(                  # input image grid=28x28\n        conv_block(1 , 8, kernel_size=5),  # pixel grid: 14x14\n        conv_block(8 ,16),                 #             7x7\n        conv_block(16,32),                 #             4x4\n        conv_block(32,64),                 #             2x2\n        conv_block(64,10, act=False),      #             1x1\n        nn.Flatten())\ncnn_layers()\n\nSequential(\n  (0): Sequential(\n    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n    (1): ReLU()\n  )\n  (1): Sequential(\n    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n  )\n  (2): Sequential(\n    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n  )\n  (3): Sequential(\n    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n  )\n  (4): Conv2d(64, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (5): Flatten(start_dim=1, end_dim=-1)\n)\n\n\nLet‚Äôs see how this trains:\n\nset_seed(1)\n\nsubs = [ProgressS(True),\n        MetricsS(accuracy=tem.MulticlassAccuracy()),\n        DeviceS(device)]\n\nl = MomentumLearner(cnn_layers(), dls, F.cross_entropy, torch.optim.SGD, 0.1, subs)\nl.fit(5)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\naccuracy\n\n\n\n\n0\ntrain\n2.278\n0.154\n\n\n0\neval\n2.302\n0.100\n\n\n1\ntrain\n2.212\n0.192\n\n\n1\neval\n2.002\n0.288\n\n\n2\ntrain\n0.958\n0.653\n\n\n2\neval\n0.701\n0.740\n\n\n3\ntrain\n0.593\n0.783\n\n\n3\neval\n0.574\n0.797\n\n\n4\ntrain\n0.516\n0.816\n\n\n4\neval\n0.508\n0.823\n\n\n\n\n\n\n\n\nAlthough the accuracy is better then what we had with the MLP model, this doesn‚Äôt look good. The loss is going down and then spikes up to a large value. This happens twice before finally the loss is going down in a more stable manner. To understand what‚Äôs going on, we have to understand what‚Äôs happening to the activations throughout the network while we are training."
  },
  {
    "objectID": "posts/11_nntrain_activations/index.html#iterative-matrix-multiplications",
    "href": "posts/11_nntrain_activations/index.html#iterative-matrix-multiplications",
    "title": "nntrain (3/n): Activations, Initialization and Normalization",
    "section": "Iterative matrix multiplications",
    "text": "Iterative matrix multiplications\nBut before we do, it‚Äôs important to realize that in the forward pass of neural networks we iteratively multiply the inputs to the model many times with (different) weight matrices. Let‚Äôs see how that works by using some artificial data, we generate 1000 random samples of data each with 10 features taken from a unit gaussian (mean=0 and standard deviation=1)\n\nx = torch.randn(10000, 10) \nprint(f'{x.mean()=:.3f}, {x.std()=:.3f}')\n\nx.mean()=-0.003, x.std()=1.000\n\n\nNow let‚Äôs iteratively multiply these inputs with a unit gausian weight matrix mapping from 10 input features to 10 output features and record the mean and standard deviation (std) after each iteration. Theoretically the mean of the outputs should remain 0, since \\(\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y] = 0 * 0 = 0\\) if \\(X\\) and \\(Y\\) are independant. But what happens to the std?\n\nfor i in range(10):\n    print(f'\"x\" multiplied {i:02d} times: mean={x.mean().item():10.3f},   std={x.std().item():15.3f}')\n    w = torch.randn(10, 10)\n    x = x@w\n\n\"x\" multiplied 00 times: mean=    -0.003,   std=          1.000\n\"x\" multiplied 01 times: mean=     0.013,   std=          3.317\n\"x\" multiplied 02 times: mean=    -0.013,   std=          9.330\n\"x\" multiplied 03 times: mean=    -0.009,   std=         32.389\n\"x\" multiplied 04 times: mean=     0.484,   std=        103.563\n\"x\" multiplied 05 times: mean=     2.104,   std=        299.863\n\"x\" multiplied 06 times: mean=     4.431,   std=        948.056\n\"x\" multiplied 07 times: mean=    -4.135,   std=       3229.121\n\"x\" multiplied 08 times: mean=   -74.124,   std=      11527.234\n\"x\" multiplied 09 times: mean=   -38.169,   std=      35325.461\n\n\nWe observe some pretty unstable activations:\n\nthe standard deviation grows exponentially\ninitially the mean remains around 0, but eventually it starts to deviate. Probably because the standard deviation is getting larger and larger\n\nThis is a big problem for the training of neural networks as input data is passing through the network. When activations are ever increasing so are the gradients which causes the updates to the weights to explode.\nLet‚Äôs try the same with a weight matrix that has a smaller standard deviation:\n\nx = torch.randn(10000, 10) \n\nfor i in range(10):    \n    print(f'\"x\" multiplied {i:02d} times: mean={x.mean().item():10.3f},   std={x.std().item():15.3f}')\n    w = torch.randn(10, 10) * 0.1 # reduce the standard deviation to 0.1\n    x = x@w\n\n\"x\" multiplied 00 times: mean=     0.004,   std=          1.001\n\"x\" multiplied 01 times: mean=    -0.000,   std=          0.279\n\"x\" multiplied 02 times: mean=    -0.000,   std=          0.072\n\"x\" multiplied 03 times: mean=    -0.000,   std=          0.027\n\"x\" multiplied 04 times: mean=    -0.000,   std=          0.011\n\"x\" multiplied 05 times: mean=     0.000,   std=          0.003\n\"x\" multiplied 06 times: mean=    -0.000,   std=          0.001\n\"x\" multiplied 07 times: mean=     0.000,   std=          0.000\n\"x\" multiplied 08 times: mean=    -0.000,   std=          0.000\n\"x\" multiplied 09 times: mean=     0.000,   std=          0.000\n\n\nThis is not any better, all activations are about the same, and they are all zero! If this happens in a neural network, the network is not learning at all, since the activations and gradients will all be zero.\n\n\n\n\n\n\nNote\n\n\n\nYou might wonder whether this analysis still holds if we are having a network which consists of convolutional layers. Since the matrix multiplications just shown ofcourse resemble the linear layer style straight up matrix multiplications and not any convolutional arithmetic. And indeed this analysis still holds, because in fact convolutional arithmetic can be rewritten in a form in which we:\n\nflatten out the CHW dimensions into a flat array\nmultiply with a weight matrix constructed out of the weights in the kernels, configured in a special way\n\nAnd thus resembles a special form of linear layer matrix multiplications. See for example here"
  },
  {
    "objectID": "posts/11_nntrain_activations/index.html#activations",
    "href": "posts/11_nntrain_activations/index.html#activations",
    "title": "nntrain (3/n): Activations, Initialization and Normalization",
    "section": "Activations",
    "text": "Activations\nFrom the above it follows that it‚Äôs important to keep track of the activations as data flows through the network. Let‚Äôs try to build this into our framework.\nTo track the activations we can make use of PyTorch hooks: a function that you can attach to any nn.Module and which will be called after the module is being called either during the forward (register_forward_hook()) or backward pass (register_backward_hook()). Let‚Äôs create a small Hook class that wraps this logic and which can remove the hook after we are done with it. We will also store the tracked metrics as attributes on this class.\n\n #| export\nclass Hook():\n    def __init__(self, nr, layer, func):\n        wrapped_func = partial(func, self) # pass the Hook object into the function\n        self.hook = layer.register_forward_hook(wrapped_func)\n        self.layer_name = f'{nr}_{layer.__class__.__name__}'\n        \n    def remove(self):\n        self.hook.remove()\n\nAnd let‚Äôs create a Subscriber, that creates and removes the hooks and keeps track of the statistics:\n\nWe will keep track of mean and std as the main metrics we have also been looking at above\nkeep track of the histogram counts for additional visibility into the activations\nkeep track of the average firing rate per activation over the batch, but more about that later\n\n\n #| export\nclass ActivationStatsS(Subscriber):\n    \n    def __init__(self, modules):\n        self.modules = modules\n    \n    def before_fit(self, learn):\n        self.hooks = [Hook(i, module, partial(self.record_stats, learn)) for i, module in enumerate(self.modules)]\n        \n    def record_stats(self, learn, hook, layer, inp, outp):\n        if learn.model.training:\n            if not hasattr(hook, 'stats'): hook.stats = ([], [], [], [])\n            acts = outp.detach().cpu()\n            hook.stats[0].append(acts.mean())              # get the means over all activations\n            hook.stats[1].append(acts.std())               # get the stds over all activations\n            hook.stats[2].append(acts.histc(20,-10,10))    # get the histogram counts with 20 bins (-10,10)\n            \n            # computation of the not_firing_rate_per_activation\n            N = acts.shape[0]                 \n            flat = acts.view(N, -1)                        # flatten the activations: matrix of [samples, activations]\n            nf_rate_p_act = (flat == 0.0).sum(dim=0) / N   # compute not firing rate per activations (so across the samples)\n            hook.stats[3].append(nf_rate_p_act)   \n\n    def after_fit(self, learn):\n        for h in self.hooks: h.remove()\n\n\n\nCode\n# This code is folded by default to not clutter the blog\n@fc.patch()\ndef plot(self:ActivationStatsS, figsize=(15,4), average_firing_rate=False):\n    plots = 3 if average_firing_rate else 2\n    fig,axs = plt.subplots(1,plots, figsize=figsize)\n    legend = []\n    for h in self.hooks:\n        axs[0].plot(h.stats[0])\n        axs[0].set_title('mean')\n        axs[1].plot(h.stats[1])\n        axs[1].set_title('std')\n        if average_firing_rate:\n            axs[2].plot(1-torch.stack(h.stats[3]).T.mean(dim=0))\n            axs[2].set_title('average firing rate')\n            axs[2].set_ylim(0,1)\n        legend.append(h.layer_name)\n    plt.legend(legend);\n\n@fc.patch()\ndef plot_hist(self:ActivationStatsS, figsize=None, log=True):\n    if figsize is None: figsize = (15, len(self.hooks))\n    fig,axs = plt.subplots(math.ceil(len(self.hooks)/2), 2, figsize=figsize)\n    axs = axs.flat\n    for i, hook in enumerate(self.hooks):\n        d = torch.stack(hook.stats[2]).T\n        if log: d = d.log1p()\n        axs[i].imshow(d, cmap='Blues', origin='lower', aspect='auto')\n        axs[i].set_title(hook.layer_name)\n        axs[i].set_yticks(np.arange(0, 20, 2), np.arange(-10, 10, 2))\n\n@fc.patch()\ndef plot_dead(self:ActivationStatsS, binary=False, figsize=None):\n    if figsize is None: figsize = (15, len(self.hooks))\n    fig,axs = plt.subplots(math.ceil(len(self.hooks)/2), 2, figsize=figsize)\n    axs = axs.flat\n    for i, hook in enumerate(self.hooks):\n        d = torch.stack(hook.stats[3]).T\n        if binary: d = d == 1.0\n        axs[i].imshow(d, cmap='Greys', origin='lower', aspect='auto')\n        axs[i].set_title(hook.layer_name)\n\n\n\nset_seed(1)\n\nmodel = cnn_layers()\n\n# show activation stats on all ReLU layers\nmodules = [module for module in model.modules() if isinstance(module, nn.ReLU)]\nact_stats = ActivationStatsS(modules)\n\nsubs = [act_stats,\n        DeviceS(device)]\n\nl = MomentumLearner(model, dls, F.cross_entropy, torch.optim.SGD, 0.1, subs)\nl.fit(5)\n\nLet‚Äôs have a look at the mean and std‚Äôs of the layers as we progressively train the model. On the horizontal axis the number of batches are displayed and the coloured lines depict the mean and std respectively of the activations in the layers we are tracking (all the ReLU layers in our model):\n\nact_stats.plot()\n\n\n\n\nThese plots show a similar problem to what we saw in the loss plots: two large spikes during training. In the beginning the means are nice and small (around 0), but the std‚Äôs are way too small (also around 0). There is thus very little variation in our activations. The std then increases exponentially (exactly as we have seen in the iterative matrix multiplication example above!) and crashes back to zero. This patterns repeats once again, and then finally the std‚Äôs stabilize around a value somewhat in the range of 1.\nLet‚Äôs also have a look at the histogram plots, these plots show a single histogram vertically. On the horizontal axis we have again number of batches. Vertically we display a histogram as a heatmap (to help with the colorscale, we actually display the log of the histogram counts): high counts in a bin correspond to a dark blue color. The histogram records values on the vertical axis from -10 to 10. Since we are tracking the stats of ReLU layers there are no counts below zero.\n\nact_stats.plot_hist()\n\n\n\n\nFinally, we can also have a look at the percentage of ‚Äúdead activations‚Äù of our ReLU neurons. Remember that a ReLU neuron passes the data directly through if the data is larger than 0, and outputs 0 whenever the data is below 0. Whenever the data is clipped at zero, it‚Äôs gradient will also be zero (since the derivative of a horizontal line is zero). For backpropagation this means that all the upstream gradient components that flow through this neuron will all be zero, which translates to no updates.\nIn principle it‚Äôs not a problem when for some samples the ReLU output is zero, this is actually totally normal and part of what the ReLY should do. Wowever when this happens for all batches in a the (training) epoch, we have a neuron which never activates for any of our data-points, and thus never passes any gradient to upstream components. This is what Andrej Karpathy calls a ‚Äúdead neuron‚Äù, and signals some kind of ‚Äúpermanent brain damage of a neural net‚Äù.\nWe are tracking this by computing the ‚Äúnone-firing-rate‚Äù per ReLU activation over the batch: if none of the samples in a batch have a positive ReLU output we record a value of 1.0, if for 50% of the samples the ReLU output is positive we record a value of 0.5.\nWith the following plots, we display those neurons that don‚Äôt fire a single time in a minibatch (black) vs those neurons that fire at least for one sample in the batch (white). Horizontally the batches, vertically the activations per layer (CxHxW)\n\nact_stats.plot_dead(binary=True)\n\n\n\n\nAs expected, we are seeing a very large number of black clusters. Especially starting from the two spikes we identified above, we see many neurons being totally thrown of and never recover anymore from it. This is a sign of severe training problems."
  },
  {
    "objectID": "posts/11_nntrain_activations/index.html#initialization",
    "href": "posts/11_nntrain_activations/index.html#initialization",
    "title": "nntrain (3/n): Activations, Initialization and Normalization",
    "section": "Initialization",
    "text": "Initialization\nWe have seen the importance of keeping the activations stable throughout training, and we have seen how we can monitor these activations.\nA first effort at stabilizing the activations, is by taking care of the initialization of the network, which is the process of setting the weights before they are trained. Above we showed that if the std of the weights is too large, our activations explode over time and if the std is too small the activations vanish. Can we set the std to a value that is just right, and makes sure the std of the activations stays roughly 1?\nWith Xavier initialization, the weight matrices are initialized in such a way that activations taken from a unit gaussian don‚Äôt explode or vanish as we have seen above. It turns out, that we have to scale the standard deviation by \\(1/\\sqrt{n_{in}}\\)\n\nx = torch.randn(10000, 10) \n\n# reduce the standard deviation by a factor of 1/sqrt(10)\nw3 = torch.randn(10, 10) * (1/math.sqrt(10))\n\nfor i in range(10):\n    x = x@w3\nprint(f'\"x\" multiplied {i:02d} times: mean={x.mean().item():10.3f},   std={x.std().item():15.3f}')\n\n\"x\" multiplied 09 times: mean=    -0.002,   std=          0.882\n\n\nHowever, this doesn‚Äôt cover the full story. During training of a neural network we also have activation functions sitting in between our matrix mulitplies. And activations typically squash the activations coming out of the (linear, convolutional..) layer. See for example what happens to our activations, after multiplying with Xavier initialized weights and adding a ReLU non-linearity:\n\nx = torch.randn(10000, 10) \n\n# reduce the standard deviation by a factor of 1/sqrt(10)\nw4 = torch.randn(10, 10) * (1/math.sqrt(10))\n\nfor i in range(10):\n    x = (x@w3).relu()\nprint(f'\"x\" multiplied {i:02d} times: mean={x.mean().item():10.3f},   std={x.std().item():15.3f}')\n\n\"x\" multiplied 09 times: mean=     0.004,   std=          0.009\n\n\nAnd as expected, everyhing has imploded to zero. It turns out that we can correct for this squashing by adding a gain. This is called Kaiming initialization. For example the gain for ReLU is \\(\\sqrt{2}\\):\n\nx = torch.randn(10000, 10)\n\n# add a gain of sqrt(2)\nw4 = torch.randn(10, 10) * math.sqrt(2/10)\n\nfor i in range(10):\n    x = torch.nn.functional.relu((x@w4))\nprint(f'\"x\" multiplied {i:02d} times: mean={x.mean().item():10.3f},   std={x.std().item():15.3f}')\n\n\"x\" multiplied 09 times: mean=     0.332,   std=          0.688\n\n\nSo let‚Äôs apply Kaiming initialization to our model, and see how it performs:\n\n #| export\ndef init_weights(m):\n    if isinstance(m, nn.Conv2d): torch.nn.init.kaiming_normal_(m.weight)\n\n\nset_seed(1)\n\nmodel = cnn_layers().apply(init_weights)\n\nmodules = [module for module in model.modules() if isinstance(module, nn.ReLU)]\nact_stats = ActivationStatsS(modules)\n\nsubs = [act_stats,\n        ProgressS(True),\n        MetricsS(accuracy=tem.MulticlassAccuracy()),\n        DeviceS(device)]\n\nl = MomentumLearner(model, dls, F.cross_entropy, torch.optim.SGD, 0.1, subs)\nl.fit(5)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\naccuracy\n\n\n\n\n0\ntrain\n1.400\n0.563\n\n\n0\neval\n0.622\n0.768\n\n\n1\ntrain\n0.507\n0.815\n\n\n1\neval\n0.457\n0.834\n\n\n2\ntrain\n0.413\n0.851\n\n\n2\neval\n0.426\n0.845\n\n\n3\ntrain\n0.376\n0.864\n\n\n3\neval\n0.389\n0.861\n\n\n4\ntrain\n0.353\n0.872\n\n\n4\neval\n0.379\n0.863\n\n\n\n\n\n\n\n\nPropper initialization increases the performance from 82% to around 87%, also the loss graph looks a bit better. Let‚Äôs have a look at our activation plots:\n\nact_stats.plot()\n\n\n\n\n\nact_stats.plot_hist()\n\n\n\n\n\nact_stats.plot_dead(binary=True)\n\n\n\n\nA lot better indeed, but we still see spikes and ‚Äúbrain damage‚Äù occuring after the spikes."
  },
  {
    "objectID": "posts/11_nntrain_activations/index.html#normalisation",
    "href": "posts/11_nntrain_activations/index.html#normalisation",
    "title": "nntrain (3/n): Activations, Initialization and Normalization",
    "section": "Normalisation",
    "text": "Normalisation\nBesides initializing the weight matrices properly, we can also normalize the data itself. Since the batch size is quite large (1024) let‚Äôs do so by taking the statistics of the first batch:\n\nxb_mean = xb.mean()\nxb_std = xb.std()\n\nCreate a small Subscriber that normalizes our inputs before a batch:\n\n #| export\nclass NormalizationS(Subscriber):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n        \n    def before_batch(self, learn):\n        learn.batch = [(learn.batch[0] - self.mean) / self.std, learn.batch[1]]\n\n\nset_seed(1)\n\nmodel = cnn_layers().apply(init_weights)\n\nmodules = [module for module in model.modules() if isinstance(module, nn.ReLU)]\nact_stats = ActivationStatsS(modules)\n\nnorm = NormalizationS(xb_mean, xb_std)\n\nsubs = [norm, \n        act_stats,\n        ProgressS(True),\n        MetricsS(accuracy=tem.MulticlassAccuracy()),\n        DeviceS(device)]\n\nl = MomentumLearner(model, dls, F.cross_entropy, torch.optim.SGD, 0.1, subs)\nl.fit(5)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\naccuracy\n\n\n\n\n0\ntrain\n0.905\n0.692\n\n\n0\neval\n0.507\n0.813\n\n\n1\ntrain\n0.429\n0.844\n\n\n1\neval\n0.407\n0.852\n\n\n2\ntrain\n0.377\n0.863\n\n\n2\neval\n0.391\n0.860\n\n\n3\ntrain\n0.341\n0.876\n\n\n3\neval\n0.364\n0.868\n\n\n4\ntrain\n0.325\n0.881\n\n\n4\neval\n0.366\n0.866\n\n\n\n\n\n\n\n\n\n# act_stats.plot(dead=True)\nact_stats.plot()\n\n\n\n\n\nact_stats.plot_hist()\n\n\n\n\n\nact_stats.plot_dead(binary=True)\n\n\n\n\nThese are all good improvements. The histogram plots start to look a lot better and the amount of dead neurons is greatly reduced. We still have problems though in the beginning of training."
  },
  {
    "objectID": "posts/11_nntrain_activations/index.html#batch-normalization",
    "href": "posts/11_nntrain_activations/index.html#batch-normalization",
    "title": "nntrain (3/n): Activations, Initialization and Normalization",
    "section": "Batch Normalization",
    "text": "Batch Normalization\nSo far we have tried to tackle the problems around activations from two sides: weight initialization and normalization of the input data. These are effective measures, but don‚Äôt cover the full problem: the weights are updated during training after which they are no longer normalized. Additionally, after we normalize our data we just ‚Äúhave to send it through the network‚Äù and see what happens with the activations.\nThe idea behind batch normalization is remarkably simple: if we know that we need unit gaussian activations throughout the network, let‚Äôs just make them unit gaussianü§ì. This might sound a bit weird, but in fact the normalization operation is perfectly differentiable, and thus the gradients can be backpropagated through a normalization operation. Batch normalization takes the form of a layer and normalizes each batch during training.\nLet‚Äôs start with the basic functionality, a layer that normalizes a batch of data. Note that Batchnorm normalizes the batch across the batch, height and width but not across the channels. So when passing RGB images through your network, the mean and std would have 3 values each (for a Batchnorm layer that would act directly upon the inputs)\n\nclass BatchNorm(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n    def forward(self, xb):\n        mean = xb.mean(dim=(0,2,3), keepdim=True)\n        std = xb.std(dim=(0,2,3), keepdim=True)\n        return (xb - mean) / std\n\n\nxb.mean(dim=(0,2,3)), xb.std(dim=(0,2,3))\n\n(tensor([0.2859]), tensor([0.3539]))\n\n\n\nout = BatchNorm().forward(xb)\nout.mean(dim=(0,2,3)), out.std(dim=(0,2,3))\n\n(tensor([8.2565e-08]), tensor([1.]))\n\n\nAn interesting and somewhat remarkable consequence of this layer, is that the activations of a single sample in the batch are getting coupled to the activations of the other samples in the batch, since the mean and std of the batch are computed across all the samples. This leads to all sort of strange behavior and after batch normalization other normalization layers have been developed which don‚Äôt have this property: such as layer, group or instance normalization. But as it turns out, this coupling also has a regularizing effect. Since the coupling of samples acts somewhat similarly to data augmentation.\nAdditionally, batchnorm defines two parameters mult and add by which the outputs are multiplied by and added to. These parameters are initialized at 1 and 0, meaning that at the very start of training these layers are in fact normalizing the data. However, they are learnable parameters, so during training these values can be changed if the model sees fit. This means that a Batchnorm layer can in fact do something totally different then normalizing the data!\n\nclass BatchNorm(nn.Module):\n    def __init__(self, nf):\n        super().__init__()\n        self.mult = torch.nn.Parameter(torch.ones(nf, 1, 1)) # also computed per channel\n        self.add = torch.nn.Parameter(torch.zeros(nf, 1, 1)) # also computed per channel\n        \n    def forward(self, xb):\n        mean = xb.mean(dim=(0,2,3), keepdim=True)\n        std = xb.std(dim=(0,2,3), keepdim=True)\n        return self.mult * (xb - mean) / std + self.add\n\nOne problem with this, is that during inference we would like to be able to pass in just a single sample. But because of the batchnorm layer which is expecting a full batch of data, it‚Äôs no longer clear how to get sensible predictions out of the model. One way to solve this, is to keep running statistics of the mean and std during training and just use these when performing inference. Let‚Äôs add that as well:\n\nclass BatchNorm(nn.Module):\n    def __init__(self, nf, mom=0.1):\n        super().__init__()\n        self.mom = mom\n        self.mult = torch.nn.Parameter(torch.ones(nf, 1, 1))\n        self.add = torch.nn.Parameter(torch.zeros(nf, 1, 1))\n        self.register_buffer('var',  torch.ones(1,nf,1,1))    # make sure they are stored during export\n        self.register_buffer('mean', torch.zeros(1,nf,1,1))   # make sure they are stored during export\n        \n    def update_stats(self, xb):\n        mean = xb.mean(dim=(0,2,3), keepdim=True)\n        var = xb.var(dim=(0,2,3), keepdim=True)\n        self.mean.lerp_(mean, self.mom)                        # take a weighted average (in place) between self.mean and mean\n        self.var.lerp_(var, self.mom)                          # dito with variance\n        \n    def forward(self, xb):\n        if self.training:\n            with torch.no_grad(): self.update_stats(xb)\n        return self.mult * ((xb - self.mean) / (self.var + 1e-5).sqrt()) + self.add\n\nTo add this to our model we have to redefine some functions:\n\n #| export\ndef conv_block(in_c, out_c, kernel_size=3, act=True, norm=True):\n    padding = kernel_size // 2\n    stride = 2\n    layers = [torch.nn.Conv2d(in_c, out_c, kernel_size, stride, padding, bias=not norm)]\n    if norm: layers.append(torch.nn.BatchNorm2d(out_c))\n    if act: layers.append(torch.nn.ReLU())\n    return nn.Sequential(*layers) if len(layers)&gt;1 else layers[0]\n\n\n #| export\ndef cnn_layers(act=True):\n    return nn.Sequential(                  \n        conv_block(1 , 8, kernel_size=5),\n        conv_block(8 ,16),\n        conv_block(16,32),\n        conv_block(32,64),\n        conv_block(64,10, norm=False, act=False),\n        nn.Flatten())\n\n\nset_seed(1)\n\nmodel = cnn_layers().apply(init_weights)\n\nmodules = [module for module in model.modules() if isinstance(module, nn.ReLU)]\nact_stats = ActivationStatsS(modules)\n\nnorm = NormalizationS(xb_mean, xb_std)\n\nsubs = [norm, \n        act_stats,\n        ProgressS(True),\n        MetricsS(accuracy=tem.MulticlassAccuracy()),\n        DeviceS(device)]\n\nl = MomentumLearner(model, dls, F.cross_entropy, torch.optim.SGD, 0.4, subs)\nl.fit(5)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\naccuracy\n\n\n\n\n0\ntrain\n0.572\n0.795\n\n\n0\neval\n0.413\n0.845\n\n\n1\ntrain\n0.339\n0.877\n\n\n1\neval\n0.352\n0.871\n\n\n2\ntrain\n0.305\n0.888\n\n\n2\neval\n0.345\n0.871\n\n\n3\ntrain\n0.277\n0.898\n\n\n3\neval\n0.340\n0.875\n\n\n4\ntrain\n0.262\n0.903\n\n\n4\neval\n0.300\n0.890\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nI must be missing something in the Batchnorm layer defined above, because it doesn‚Äôt train as well as the Batchnorm2d layer from PyTorch. Let me know if anybody knows what I‚Äôm missing\n\n\n\nact_stats.plot()\n\n\n\n\n\nact_stats.plot_hist()\n\n\n\n\n\nact_stats.plot_dead(binary=True)\n\n\n\n\nAnd this is all looking very good, no more spikes in the loss, histograms are looking good and no or very little permanently dead neurons.\nLet‚Äôs have a final look at the ‚Äúdead plot‚Äù. Without the binary=True it displays the average dead rate across the samples in in the minibatch.\nAdditionally we can plot the average firing rate across all neurons:\n\nact_stats.plot_dead()\n\n\n\n\n\nact_stats.plot(average_firing_rate=True)\n\n\n\n\nFrom which we see that on average all ReLU neurons fire for around 50% of the samples in a minibatch. Some fire a bit less (darker) some fire a bit more (brighter), but we have very little neurons that never fire (black)."
  },
  {
    "objectID": "posts/11_nntrain_activations/index.html#final-remarks",
    "href": "posts/11_nntrain_activations/index.html#final-remarks",
    "title": "nntrain (3/n): Activations, Initialization and Normalization",
    "section": "Final remarks",
    "text": "Final remarks\nWe have again come a long way, and have seen how we can make sure to train a neural network properly. We have learned to look at activations from many angles, and improved our model up to around 89% accuracy by careful initialization and normalization."
  },
  {
    "objectID": "posts/01_blog_setup/index.html",
    "href": "posts/01_blog_setup/index.html",
    "title": "Blog setup",
    "section": "",
    "text": "In this blog post I‚Äôll explain how I created this blog, using Quarto and GitHub. In step 4 I‚Äôll show how to setup GitHub Actions, this has advantages over the other ways to publish our blog:\nI‚Äôm working on a Macbook, and using VS Code for code editing. If you are on a Linux or Windows machine, be aware that things might be a bit different from what I describe here.\nI am assuming you already have a GitHub account, that VS Code is installed and configured to run Python and Jupyter Notebooks."
  },
  {
    "objectID": "posts/01_blog_setup/index.html#step-1-install-quarto",
    "href": "posts/01_blog_setup/index.html#step-1-install-quarto",
    "title": "Blog setup",
    "section": "Step 1: install Quarto",
    "text": "Step 1: install Quarto\nFirst of all you need to install Quarto, go here, download and install the software. You should do this on the machine that you want to use for writing your blog, in my case my Macbook laptop.\nOnce installed you will have access to the quarto Command Line Interface (CLI). To make sure everything works as expected, open a terminal and execute:\n\n\nTerminal\n\nquarto --help\n\nThis should render some outputs describing the different commands and options that are part of the Quarto CLI and shows that Quarto is installed successfully."
  },
  {
    "objectID": "posts/01_blog_setup/index.html#step-2-create-a-github-repo",
    "href": "posts/01_blog_setup/index.html#step-2-create-a-github-repo",
    "title": "Blog setup",
    "section": "Step 2: create a GitHub repo",
    "text": "Step 2: create a GitHub repo\nTo host our blog we will use GitHub Pages, which is a service to host a website from a GitHub repository. Based on the name you pick for your repository you will create a so-called project-website or your unique user-website. For any general repo named my-awesome-repo, the website will be hosted on https://&lt;github-username&gt;.github.io/my-awesome-repo. This is a project-websites and you can create as many as you like.\nTo create your user-website, you have to name the repo exactly like this: &lt;github-username&gt;.github.io, the user-website will be hosted at https://&lt;github-username&gt;.github.io.\nThis is exactly what I want, so I create a new repo with the name: lucasvw.github.io.\nI find it helpful to add a .gitignore file with a Python template, to which we can later add some more entries to facilitate storing the right files on GitHub. Also make sure that the repo is Public (and not set to Private). Additionally, I added a README file and choose the Apache2 License.\nNext, I clone this repo to my machine by running:\n\n\nTerminal\n\ngit clone git@github.com:lucasvw/lucasvw.github.io.git"
  },
  {
    "objectID": "posts/01_blog_setup/index.html#step-3-add-a-quarto-project-to-the-repo",
    "href": "posts/01_blog_setup/index.html#step-3-add-a-quarto-project-to-the-repo",
    "title": "Blog setup",
    "section": "Step 3: add a Quarto project to the repo",
    "text": "Step 3: add a Quarto project to the repo\nNext, open VS Code and open the cloned repo. Then access the VS Code terminal and run:\n\n\nTerminal\n\nquarto create-project --type website:blog\n\nThis will add a number of files to our repo, which represent the basic structure of our blog. Most importantly:\n\nposts: here we will create our blog entries (one subfolder per blog entry)\n_quarto.yml: configuration file for our blog such as the theme, name, GitHub and Twitter links\nabout.qmd: source code for the ‚Äúabout‚Äù page.\nindex.qmd: source code for the landing page.\n\n\n\n\n\n\n\nNote\n\n\n\n.qmd files are like markdown files, but with lots of additional functionality from Quarto. Go here for more information on Markdown syntax and here for Quarto Markdown\n\n\nTo see what we currently have, let‚Äôs render our blog locally:\n\n\nTerminal\n\nquarto preview\n\nAlternatively, we can install the Quarto extension in VS Code, which will show a render button in the top right corner on any opened qmd file.\nTo publish the current contents to GitHub pages, we can run:\n\n\nTerminal\n\nquarto publish gh-pages\n\nWhen doing so, we get a message that we have to change the branch from which GitHub Pages builds the site. To do this, I go to https://github.com/lucasvw/lucasvw.github.io/settings/pages and select gh-pages instead of the main branch.\nAnd voila, in a few moments our blog will be running live at https://lucasvw.github.io/"
  },
  {
    "objectID": "posts/01_blog_setup/index.html#step-4-finalize-set-up-github-actions",
    "href": "posts/01_blog_setup/index.html#step-4-finalize-set-up-github-actions",
    "title": "Blog setup",
    "section": "Step 4: Finalize set-up: GitHub Actions",
    "text": "Step 4: Finalize set-up: GitHub Actions\nWhen we run the quarto publish gh-pages command, Quarto processes our files and turns them into web readable files (HTML, JS, CSS etc). It stores these files in our gh-pages branch and pushes them to our remote GitHub repo. This is great, but it means that this doesn‚Äôt store our source files.\nTo do so, let‚Äôs first open our .gitignore file and make sure that it contains the following entries so that we don‚Äôt check in any files we don‚Äôt need.\n\n\n.gitignore\n\n# Quarto\n/.quarto/\n_site/\n\n# Mac files\n.DS_Store\n\nNext, we can commit all the remaining files to Git and push them to our remote repo. If we ever lose access to our local machine, we can restore everything we need from GitHub.\nHowever, now we have 2 things we need to do whenever we finish our work:\n\nstore our source files on the main branch and push to GitHub\nrun the publish command to update the blog\n\nThis is a bit annoying and it would be much better if we can just push to the main branch and GitHub would take care of building our website and updating it. This also allows us to create blog entries on any machine that has access to git, we don‚Äôt need to have quarto installed. This is particularly practical if we want to write blog entries from our deep learning server. So let‚Äôs use GitHub actions for this.\n\n\n\n\n\n\nNote\n\n\n\nBefore you continue make sure you have at least once run a quarto publish gh-pages command, this is necessary for the things below to work\n\n\nFirst we need to add the following snippet to _quarto.yml\n\n\n_quarto.yml\n\nexecute:\n  freeze: auto\n\nThis will make sure that GitHub actions doesn‚Äôt execute any executable code, but will show the pre-rendered outputs it finds in the _freeze folder.\nFinally, create the file .github/workflows/publish.yml and populate it with the following code:\n\n\n.github/workflows/publish.yml\n\non:\n  workflow_dispatch:\n  push:\n    branches: main\n\nname: Quarto Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v3\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n\n      - name: Render and Publish\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\nOnce we push these things to GitHub, we are good to go. Whenever we push anything to the main branch, this workflow will execute and take care of updating the gh-pages branch and updating the blog."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Another Machine Learning Blog",
    "section": "",
    "text": "nntrain (3/n): Activations, Initialization and Normalization\n\n\n\n\n\n\n\ncode\n\n\nneural network\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nAug 20, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nnntrain (2/n): Learner\n\n\n\n\n\n\n\ncode\n\n\nneural network\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nnntrain (1/n): Datasets and Dataloaders\n\n\n\n\n\n\n\ncode\n\n\nneural network\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nnntrain (0/n): Preliminaries\n\n\n\n\n\n\n\ncode\n\n\nneural network\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nAug 9, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Stable Diffusion - Code\n\n\n\n\n\n\n\ngenerative\n\n\nstable diffusion\n\n\ndiffusers\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\n\nJul 3, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Stable Diffusion - Concepts\n\n\n\n\n\n\n\ngenerative\n\n\nstable diffusion\n\n\ndiffusers\n\n\nconcepts\n\n\n\n\n\n\n\n\n\n\n\n\nMar 18, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nCross entropy any which way\n\n\n\n\n\n\n\nloss functions\n\n\nsoftmax\n\n\nnll\n\n\ncross entropy\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nFast matrix multiplications\n\n\n\n\n\n\n\nfoundations\n\n\nmaths\n\n\nvectorization\n\n\nlinear algebra\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nFirst competitionüèÖ\n\n\n\n\n\n\n\ndeep learning\n\n\nimage classification\n\n\ncompetition\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nPaperspace setup\n\n\n\n\n\n\n\nsetup\n\n\npaperspace\n\n\ngpu\n\n\nhow-to\n\n\nmlops\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nBlog setup\n\n\n\n\n\n\n\nblogging\n\n\nsetup\n\n\nhow-to\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/02_paperspace_setup/index.html",
    "href": "posts/02_paperspace_setup/index.html",
    "title": "Paperspace setup",
    "section": "",
    "text": "Most people don‚Äôt have a GPU installed in their working machine that is suited for Deep Learning, and in fact you don‚Äôt need to. It‚Äôs quite easy to setup a remote GPU server nowadays, and in this blog I will explain how to do so with Paperspace Gradient.\nI started using Paperspace because of a recommendation from Jeremy Howard in his Live Coding Videos. If you haven‚Äôt seen these lectures, I can highly recommend them. They are a great resource on many things related to getting started with Deep Learning. Jeremy shows a lot of productivity hacks and practical tips on getting a good setup.\nHowever, the Paperspace setup explanations are a bit out-dated which can lead to confusion when following along with the video‚Äôs. Also, after the recording of the videos Jeremy created some nice scripts which simplify the setup. This blog will hopefully help others to navigate this and quickly set-up a remote GPU server. I would advice anybody who wants to try Paperspace, to first watch the videos from Jeremy to have a general idea of how it works, and then follow these steps to quickly get set-up.\nOnce you have signed up to Paperspace, go to their Gradient service and create a new project. Paperspace has a free tier, as well as a pro- ($8/month) and growth-plan ($39/month). I personally signed up for the pro-plan, which has a very good value for money. You get 15Gb persistent storage and free Mid instance types. If available, I use the A4000, which is the fastest and comes with 16GB of GPU memory.\nWith the pro-plan you can create up to 3 servers, or ‚ÄúNotebooks‚Äù as they are called by Paperspace (throughout this blog I‚Äôll refer to them as Notebook Servers). So let‚Äôs create one:"
  },
  {
    "objectID": "posts/02_paperspace_setup/index.html#first-look-at-our-notebook-server",
    "href": "posts/02_paperspace_setup/index.html#first-look-at-our-notebook-server",
    "title": "Paperspace setup",
    "section": "First look at our Notebook Server",
    "text": "First look at our Notebook Server\nNext, let‚Äôs open a terminal and get familiar with our Server\n\n\nTerminal\n\n&gt; which python\n/usr/local/bin/python\n\n&gt; python --version\nPython 3.9.13\n\nAnd let‚Äôs also check the PATH variable:\n\n\nTerminal\n\n&gt; echo $PATH\n/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin: /usr/sbin:/usr/bin:/sbin:/bin:/root/mambaforge/bin\n\nThe python command is thus pointing to the system Python installation. However, on the PATH variable we are also seeing an entry at the end mentioning mambaforge.\nAnd indeed we can execute:\n\n\nTerminal\n\n&gt; mamba list | grep python\n\nipython                   8.5.0              pyh41d4057_1    conda-forge\nipython_genutils          0.2.0                      py_1    conda-forge\npython                    3.10.6          h582c2e5_0_cpython    conda-forge\npython-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge\npython-fastjsonschema     2.16.2             pyhd8ed1ab_0    conda-forge\npython_abi                3.10                    2_cp310    conda-forge\n\nSo we are having both a mamba based Python 3.10.6 and a system installation of Python 3.9.13.\nLet‚Äôs open a Jupyter Notebook and see which Python version is running:\n\n\nUntitled.ipynb\n\nimport sys\nsys.version\n\nWhich returns: '3.9.13 (main, May 23 2022, 22:01:06) \\n[GCC 9.4.0]'. Jupyter is thus running the system Python installation.\n\n\n\n\n\n\nNote\n\n\n\nIn the videos Jeremy mentions that we should never use the system Python but instead always create a Mamba installation. However, since we are working here on a virtual machine that is only used for running Python, this shouldn‚Äôt be a problem. Just be aware that we are using the system Python which is totally separate from the Mamba setup.\n\n\nSince we are running the system Python version, we can inspect all the packages that are installed:\n\n\nTerminal\n\n&gt; pip list\n\n...\nfastai                            2.7.10\nfastapi                           0.92.0\nfastbook                          0.0.28\nfastcore                          1.5.27\nfastdownload                      0.0.7\nfastjsonschema                    2.15.3\nfastprogress                      1.0.3\n...\ntorch                             1.12.0+cu116\ntorchaudio                        0.12.0+cu116\ntorchvision                       0.13.0+cu116\n..."
  },
  {
    "objectID": "posts/02_paperspace_setup/index.html#persisted-storage-at-paperspace",
    "href": "posts/02_paperspace_setup/index.html#persisted-storage-at-paperspace",
    "title": "Paperspace setup",
    "section": "Persisted Storage at Paperspace",
    "text": "Persisted Storage at Paperspace\nIn general, things are not persisted on Paperspace. So anything we store during a session, will be gone when we restart our Notebook Server. However, Paperspace comes with two special folders that are persisted. It‚Äôs important to understand how these folder works since we obviously need to persist our work. Not only that, but we also need to persist our configuration files from services lik GitHub, Kaggle and HuggingFace and potentially any other config files for tools or services we are using.\nThe persisted folders are called /storage and /notebooks. Anything in our /storage is shared among all the Notebook Servers we are running, whereas anything that is stored in the /notebooks folder is only persisted on that specific Notebook Server."
  },
  {
    "objectID": "posts/02_paperspace_setup/index.html#set-up",
    "href": "posts/02_paperspace_setup/index.html#set-up",
    "title": "Paperspace setup",
    "section": "Set up",
    "text": "Set up\nIn the first few videos, Jeremy shows a lot of tricks on how to install new packages and set up things like Git and GitHub. After the recording of these videos, he made a GitHub repo which facilitates this setup greatly and makes most of the steps from the videos unnecessary. So let‚Äôs use that:\n\n\nTerminal\n\n&gt; git clone https://github.com/fastai/paperspace-setup.git\n&gt; cd paperspace-setup\n&gt; ./setup.sh\n\nTo understand what this does, let‚Äôs have a look at setup.sh:\n\n\nsetup.py\n\n#!/usr/bin/env bash\n\nmkdir /storage/cfg\ncp pre-run.sh /storage/\ncp .bash.local /storage/\necho install complete. please start a new instance\n\nFirst it‚Äôs creating a new directory inside of our /storage folder called cfg. As we will see, this is where we will store all our configuration files and folders.\nNext, the script copies 2 files to our storage folder. Let‚Äôs have a closer look at those\n\npre-run.sh\nDuring startup of a Notebook Server (upon creation or restart), Paperspace automatically executes the script it finds at /storage/pre-run.sh. This is really neat, since we can create a script at this location to automate our setup!\nFor the full script, click here, and let‚Äôs have a closer look at this first snippet:\n\n\npre-run.sh (snippet)\n\nfor p in .local .ssh .config .ipython .fastai .jupyter .conda .kaggle\ndo\n        if [ ! -e /storage/cfg/$p ]; then\n                mkdir /storage/cfg/$p\n        fi\n        rm -rf ~/$p\n        ln -s /storage/cfg/$p ~/\ndone\n\nSo we are iterating through a list of folder names (.local .ssh ...) on line 1, and for each one we create a directory inside of /storage/cfg on line 4. We only do this if the directory doesn‚Äôt already exist on line 3. Next, each of these folders is symlinked to the home directory (~/) on line 7.\nThis means that:\n\nWhen we store something in any of these symlinked folders (e.g.¬†~/.local), it‚Äôs actually being written to the associated storage folder (e.g.¬†/storage/cfg/.local) because of the symlink.\nWhenever we restart our Notebook Server, all the stuff that has previously been persisted (e.g.¬†in /storage/cfg/.local) are made available again in the home directory (e.g.¬†~/.local).\n\nThis is very nice, because as it turns out: many tools keep their configuration files in this home folder. So by persisting this data, they will keep working across restarts of our Notebook servers.\nLet‚Äôs a closer look at the folders we are persisting:\n\n.local\nWe saw before that the FastAI runtime comes with a number of installed Python packages. If we want to install additional packages, we could do: pip install &lt;package&gt;. However, pip installs the packages in /usr/local/lib, and are thus not persisted. To make sure our packages are persisted, we can instead install with pip install --user &lt;package&gt;. This --user flag, tells pip to install the package only for the current user, and so it installs into the ~/.local directory. So by persisting this folder, we make sure that we our custom installed python packages are persisted, awesome!\n\n\n.ssh\nTo authenticate with GitHub without using passwords, we use ssh keys. To create a pair of keys, we run: ssh-keygen. This creates the private key (id_rsa) and the public key (id_rsa.pub) to the ~/.ssh folder. Once we upload the public key to GitHub we can authenticate with GitHub, and by persisting this folder we can authenticate upon restart!\nBy now you probably get the idea, any of these folders represent a certain configuration we want to persist:\n\n.conda: contains conda/mamba installed packages\n.kaggle: contains a kaggle.json authentication file\n.fastai: contains downloaded datasets and some other configuration\n.config, .ipython and .jupyter: contain config files for various pieces of software such as matplotlib, ipython and jupyter.\n\nI personally also added .huggingface to this list, to make sure my HuggingFace credentials are also persisted. See here for the PR back into the main repo.\nIn the second part of the script we do exactly the same thing, but for a number of files instead of directories.\n\n\npre-run.sh (snippet)\n\nfor p in .git-credentials .gitconfig .bash_history\ndo\n        if [ ! -e /storage/cfg/$p ]; then\n                touch /storage/cfg/$p\n        fi\n        rm -rf ~/$p\n        ln -s /storage/cfg/$p ~/\ndone\n\nNow that we understand pre-run.sh, let‚Äôs have a look at the second file we store in our /storage folder:\n\n\n\n.bash.local\n\n\n.bash.local\n\n#!/usr/bin/env bash\n\nalias mambai='mamba install -p ~/.conda '\nalias pipi='pip install --user '\n\nexport PATH=~/.local/bin:~/.conda/bin/:$PATH\n\nPaperspace runs this script whenever we open a terminal. As you can see it defines two aliases to easily install things persistently with either mamba (mambai) or pip (pipi).\nAny binaries that are installed this way, are installed in ~/.local/bin (through pip) and to ~/.conda/bin/ (through mamba). We need to add these paths to the PATH variable, to make sure we can call them from the command line.\n\n\nNote on Mamba\nAt this point you might wonder why we have the Mamba installation at all, since we have seen that the system Python is used. In fact, our Mamba environment is totally decoupled from what we are using in our Jupyter notebook, and installing packages through mamba will not make them available in Jupyter. Instead, we should install Python packages through pip.\nSo what do we need Mamba for? I guess Jeremy has done this to be able to install binaries that he wants to use from the Terminal. For example, in the videos he talks about ctags which he installs through mamba. Since installing none-Python specific binaries through pip can be complicated, we can use Mamba instead. In other words, we can use it as a general package manager, somewhat similar to apt-get.\n\n\nFinal words\nIn my opinion Paperspace offers a great product for very fair money, especially if combined with the setup described in this blog!"
  },
  {
    "objectID": "posts/09_nntrain_ds/index.html",
    "href": "posts/09_nntrain_ds/index.html",
    "title": "nntrain (1/n): Datasets and Dataloaders",
    "section": "",
    "text": "In this series, I want to discuss the creation of a small library for training neural networks: nntrain. It‚Äôs based off the excellent part 2 of Practical Deep Learning for Coders by Jeremy Howard, in which from lessons 13 to 18 (roughly) the development of the miniai library is discussed.\nThe library will build upon PyTorch. We‚Äôll try as much as possible to build from scratch to understand how it all works. Once the main functionality of components are implemented and verified, we can switch over to PyTorch‚Äôs version. This is similar to how things are done in the course. However, this is not just a ‚Äúcopy / paste‚Äù of the course: on many occasions I take a different route, and most of the code is my own. That is not to say that all of this is meant to be extremely innovative, instead I had the following goals:\nnb_dev is another great project from the fastai community, which allows python libraries to be written in jupyter notebooks. This may sound a bit weird since the mainstream paradigm is to only do experimental work in notebooks. It has the advantage though that we can create the source code for our library in the very same environment in which we want to experiment and interact with our methods, objects and structure while we are building the library. For more details on why this is a good idea and other nice features of nb_dev, see here.\nSo without further ado, let‚Äôs start with where we left off in the previous post:"
  },
  {
    "objectID": "posts/09_nntrain_ds/index.html#end-of-last-post",
    "href": "posts/09_nntrain_ds/index.html#end-of-last-post",
    "title": "nntrain (1/n): Datasets and Dataloaders",
    "section": "End of last post:",
    "text": "End of last post:\nfrom datasets import load_dataset,load_dataset_builder\nimport torchvision.transforms.functional as TF\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nname = \"fashion_mnist\"\nds_builder = load_dataset_builder(name)\nds_hf = load_dataset(name, split='train')\n\nx_train = torch.stack([TF.to_tensor(i).view(-1) for i in ds_hf['image']])\ny_train = torch.stack([torch.tensor(i) for i in ds_hf['label']])\n\ndef fit(epochs):\n    for epoch in range(epochs):\n        for i in range(0,len(x_train), bs):\n            xb = x_train[i:i+bs]\n            yb = y_train[i:i+bs]\n\n            preds = model(xb)\n            acc = accuracy(preds, yb)\n            loss = loss_func(preds, yb)\n            loss.backward()\n\n            opt.step()\n            opt.zero_grad()\n        print(f'{epoch=} | {loss=:.3f} | {acc=:.3f}')\n\ndef accuracy(preds, targs):\n    return (preds.argmax(dim=1) == targs).float().mean()        \n\ndef get_model_opt():\n    layers = [nn.Linear(n_in, n_h), nn.ReLU(), nn.Linear(n_h, n_out)]\n    model = nn.Sequential(*layers)\n    \n    opt = torch.optim.SGD(model.parameters(), lr)\n    \n    return model, opt\n\nn_in  = 28*28\nn_h   = 50\nn_out = 10\nlr    = 0.01\nbs    = 1024\nloss_func = F.cross_entropy\n\nmodel, opt = get_model_opt()\nfit(5)"
  },
  {
    "objectID": "posts/09_nntrain_ds/index.html#datasets",
    "href": "posts/09_nntrain_ds/index.html#datasets",
    "title": "nntrain (1/n): Datasets and Dataloaders",
    "section": "Datasets:",
    "text": "Datasets:\nAll the stuff in this post will be based on tackling the minibatch construct we currently have in the training loop on lines 16-18:\n...\nfor i in range(0,len(x_train), bs):\n    xb = x_train[i:i+bs]\n    yb = y_train[i:i+bs]\n...\nAnd the first refactor will be to create a Dataset object, which allows us to simplify:\n...\nfor i in range(0,len(x_train), bs):\n    xb, yb = dataset[i:i+bs]\n...\nThis is pretty straight-forward, a Dataset is something that holds our data and upon ‚Äúindexing into‚Äù it returns a sample of the data:\n\nclass Dataset():\n    \n    def __init__(self, x_train, y_train):\n        self.x_train = x_train\n        self.y_train = y_train\n        \n    def __getitem__(self, i):\n        return self.x_train[i], self.y_train[i]\n    \n    def __len__(self):\n        return len(self.x_train)\n\n\nds = Dataset(x_train, y_train)\nprint([i.shape for i in ds[0]])\n\n[torch.Size([784]), torch.Size([])]\n\n\nNext, we want to further improve the training loop and get to this behavior:\n...\nfor xb, yb in dataloader:\n...\nSo our dataloader needs to wrap the dataset, and provide some kind of an iterator returning batches of data, based on the specified batch size. Let‚Äôs create one:\n\nclass DataLoader():\n    \n    def __init__(self, dataset, batch_size):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        \n    def __iter__(self):\n        for i in range(0,len(self.dataset),self.batch_size):\n            yield self.dataset[i:i+self.batch_size]\n\nNow the training loop is simplified to:\n\ndef fit(epochs):\n    for epoch in range(epochs):\n        for xb, yb in dl:\n            preds = model(xb)\n            acc = accuracy(preds, yb)\n            loss = loss_func(preds, yb)\n            loss.backward()\n\n            opt.step()\n            opt.zero_grad()\n        print(f'{epoch=} | {loss=:.3f} | {acc=:.3f}')\n\n\ndl = DataLoader(ds, bs)\n\nmodel, opt = get_model_opt()\nfit(5)\n\nepoch=0 | loss=2.062 | acc=0.441\nepoch=1 | loss=1.785 | acc=0.597\nepoch=2 | loss=1.531 | acc=0.637\nepoch=3 | loss=1.334 | acc=0.645\nepoch=4 | loss=1.190 | acc=0.660"
  },
  {
    "objectID": "posts/09_nntrain_ds/index.html#next-up-shuffling-the-data",
    "href": "posts/09_nntrain_ds/index.html#next-up-shuffling-the-data",
    "title": "nntrain (1/n): Datasets and Dataloaders",
    "section": "Next up: shuffling the data",
    "text": "Next up: shuffling the data\nThe above training loop already looks pretty good, it‚Äôs small and concise, and fairly generic. The next improvement we are going to make is something that doesn‚Äôt improve the code of the training loop, but improves training of the model. So far during training, we cycle each epoch through the data in the exact same order. This means that all training samples are always batched together with the exact same other samples. This is not good for training our model, instead we want to shuffle the data up. So that each epoch, we have batches of data that have not yet been batched up together. This additional variation helps the model to generalize as we will see.\nThe simplest implementation would be to create a list of indices, which we put in between the dataset and the sampling of the mini-batches. In case we don‚Äôt need to shuffle, this list will just be [0, 1, ... len(dataset)].\n\nimport random\n\nclass DataLoader():\n    \n    def __init__(self, dataset, batch_size, shuffle):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        \n    def __iter__(self):\n        self.indices = list(range(0, len(self.dataset)))\n        if self.shuffle: \n            random.shuffle(self.indices)\n            \n        for i in range(0,len(self.dataset),self.batch_size):\n            yield self.dataset[self.indices[i:i+self.batch_size]]\n\n\nmodel, opt = get_model_opt()\ndl = DataLoader(ds, bs, shuffle=True)\nfit(5)\n\nepoch=0 | loss=2.067 | acc=0.429\nepoch=1 | loss=1.800 | acc=0.515\nepoch=2 | loss=1.539 | acc=0.592\nepoch=3 | loss=1.358 | acc=0.618\nepoch=4 | loss=1.187 | acc=0.692\n\n\nThis works just fine, but let‚Äôs see if we can encapsulate this logic in a separate class. We start with a simple Sampler class that we can iterate through and either gives indices in order, or shuffled:\n\nclass Sampler():\n    def __init__(self, ds, shuffle=False):\n        self.range = list(range(0, len(ds)))\n        self.shuffle = shuffle\n        \n    def __iter__(self):\n        if self.shuffle: random.shuffle(self.range)\n        for i in self.range:\n            yield i\n\n\ns = Sampler(ds, False)           # shuffle = False\nfor i, sample in enumerate(s): \n    print(sample, end=', ')\n    if i == 5: break\n\n0, 1, 2, 3, 4, 5, \n\n\n\ns = Sampler(ds, True)            # shuffle = TRUE\nfor i, sample in enumerate(s): \n    print(sample, end=', ')\n    if i == 5: break\n\n58844, 19394, 36509, 38262, 51037, 46835, \n\n\nNext, let‚Äôs create a BatchSampler that does the same, but returns the indexes in batches. For that we can use the islice() function from the itertools module:\n\nfrom itertools import islice\n\ndef printlist(this): print(list(this))\n\nlst = list(range(0, 10))         # create a list of 10 numbers\n\nprintlist(islice(lst, 0, 3))     # with islice we can get a slice out of the list\nprintlist(islice(lst, 5, 10))\n\n[0, 1, 2]\n[5, 6, 7, 8, 9]\n\n\n\nprintlist(islice(lst, 4))        # we can also get the \"next\" 4 elements\nprintlist(islice(lst, 4))        # doing that twice gives the same first 4 elements\n\n[0, 1, 2, 3]\n[0, 1, 2, 3]\n\n\n\nlst = iter(lst)                  # however if we put an iterator on the list:\n\nprintlist(islice(lst, 4))        # first 4 elements\nprintlist(islice(lst, 4))        # second 4 elements\nprintlist(islice(lst, 4))        # remaining 2 elements\nprintlist(islice(lst, 4))        # iterator has finished..\n\n[0, 1, 2, 3]\n[4, 5, 6, 7]\n[8, 9]\n[]\n\n\nAnd thus we create our BatchSampler:\n\nclass BatchSampler():\n    def __init__(self, sampler, batch_size):\n        self.sampler = sampler\n        self.batch_size = batch_size\n        \n    def __iter__(self):\n        it = iter(self.sampler)\n        while True:\n            res = list(islice(it, self.batch_size))\n            if len(res) == 0:    # return when the iterator has finished          \n                return           \n            yield res\n\nLet‚Äôs see the BatchSamepler in action:\n\ns = Sampler(list(range(0,10)), shuffle=False)\nbatchs = BatchSampler(s, 4)\nfor i in batchs:\n    printlist(i)\n\n[0, 1, 2, 3]\n[4, 5, 6, 7]\n[8, 9]\n\n\nAnd let‚Äôs incorporate it into the DataLoader:\n\nclass DataLoader():\n    \n    def __init__(self, dataset, batch_sampler):\n        self.dataset = dataset\n        self.batch_sampler = batch_sampler\n        \n    def __iter__(self):\n        for batch in self.batch_sampler:\n            yield self.dataset[batch]\n\n\ns = Sampler(ds, shuffle=True)\ndl = DataLoader(ds, BatchSampler(s, bs))\n\nmodel, opt = get_model_opt()\nfit(5)\n\nepoch=0 | loss=1.981 | acc=0.462\nepoch=1 | loss=1.698 | acc=0.567\nepoch=2 | loss=1.468 | acc=0.620\nepoch=3 | loss=1.346 | acc=0.613\nepoch=4 | loss=1.202 | acc=0.656"
  },
  {
    "objectID": "posts/09_nntrain_ds/index.html#collation",
    "href": "posts/09_nntrain_ds/index.html#collation",
    "title": "nntrain (1/n): Datasets and Dataloaders",
    "section": "Collation",
    "text": "Collation\nAnd this works pretty good. However, there is one caveat. In the very beginning of this post we did:\nx_train = torch.stack([TF.to_tensor(i).view(-1) for i in ds_hf['image']])\ny_train = torch.stack([torch.tensor(i) for i in ds_hf['label']])\nAnd we ideally would like these transformations to be part of the Dataloaders / Dataset paradigm. So instead of first transforming the Huggingface Dataset into x_train and y_train, we want to directly use the dataset. We can do so by adding a collate function. This wraps around a list of individual samples into the datasets, and receives a list of individual x,y tuples ([(x1,y1), (x2,y2), ..]) as argument. In that function, we can determine how to treat these items and parse it in a way that is suitable to our needs. i.e.:\n\nbatch the x and y, so that we transform from [(x1,y1), (x2,y2), ..] to [(x_1,x_2, ..), (y_1,y_2, ..)]\nmove individual items x_i and y_i to tensors\nstack the x tensors and y tensors respectively into one big tensor\n\nSo let‚Äôs update our DataLoader with a collate_func that wraps around individual samples:\n\nclass DataLoader():\n    \n    def __init__(self, dataset, batch_sampler, collate_func):\n        self.dataset = dataset\n        self.batch_sampler = batch_sampler\n        self.collate_func = collate_func\n        \n    def __iter__(self):\n        for batch in self.batch_sampler:\n            yield self.collate_func(self.dataset[sample] for sample in batch)\n\nAnd now let‚Äôs create a custom collate function to deal with our data. Specifically, remember that a sample of our huggingface dataset is a dictionary (and not a tuple) with keys image and label holding a PIL.Image.image object and a number (representing any out of 10 classes) respectively.\nSo our collate_func should:\n\ntransform the dictionary into a tuple\nmove everything to a tensor\nzip the results so that x and y are batched\nand combine the list of tensors for x and y respectively into one big tensor\n\n\ndef collate_func(data):\n    data = [(TF.to_tensor(sample['image']).view(-1), torch.tensor(sample['label'])) for sample in data]\n    x, y = zip(*data)\n    return torch.stack(x), torch.stack(y)\n\nAnd let‚Äôs see it in action, now using the huggingface dataset ds_hf:\n\ns = Sampler(ds_hf, shuffle=True)\ndl = DataLoader(ds_hf, BatchSampler(s, bs), collate_func)\n\nmodel, opt = get_model_opt()\nfit(5)\n\nepoch=0 | loss=2.125 | acc=0.345\nepoch=1 | loss=1.899 | acc=0.497\nepoch=2 | loss=1.635 | acc=0.609\nepoch=3 | loss=1.389 | acc=0.640\nepoch=4 | loss=1.260 | acc=0.641\n\n\nNot bad, we have replicated the main logic of PyTorch‚Äôs DataLoader. The version from PyTorch has a slightly different API as we don‚Äôt have to specify the BatchSampler, instead we can just pass shuffle=True:\n\nfrom torch.utils.data import DataLoader\n\ns = Sampler(ds_hf, shuffle=True)\ndl = DataLoader(ds_hf, batch_size=bs, shuffle=True, collate_fn=collate_func)\n\nmodel, opt = get_model_opt()\nfit(5)\n\nepoch=0 | loss=2.107 | acc=0.434\nepoch=1 | loss=1.840 | acc=0.620\nepoch=2 | loss=1.605 | acc=0.641\nepoch=3 | loss=1.354 | acc=0.641\nepoch=4 | loss=1.258 | acc=0.618"
  },
  {
    "objectID": "posts/09_nntrain_ds/index.html#validation-set",
    "href": "posts/09_nntrain_ds/index.html#validation-set",
    "title": "nntrain (1/n): Datasets and Dataloaders",
    "section": "Validation set",
    "text": "Validation set\nLet‚Äôs add a validation set to make sure we validate on data we are not training on. For that we are going to pull the data from the datasets library without the splits argument, which will give us a dataset dictionary containing both a training and a test dataset:\n\nhf_dd = load_dataset(name)\nhf_dd\n\nReusing dataset fashion_mnist (/root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1)\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 60000\n    })\n    test: Dataset({\n        features: ['image', 'label'],\n        num_rows: 10000\n    })\n})\n\n\nAnd let‚Äôs create two dataloaders, one for the train and one for the validation set. For the validation loader we can double the batch size since we won‚Äôt be computing gradients for the forward pass:\n\ntrain_loader = DataLoader(hf_dd['train'], batch_size=bs, shuffle=True, collate_fn=collate_func)\nvalid_loader = DataLoader(hf_dd['test'], batch_size=2*bs, shuffle=False, collate_fn=collate_func)\n\nWe change the training loop in a couple of ways:\n\ncompute loss and metrics more correctly, by taking care of the batch-size and taking the average over all data\nadd a seperate forward pass for the validation set\n\n\ndef fit(epochs):\n    for epoch in range(epochs):\n        model.train()                                       # put the model in \"train\" mode\n        n_t = train_loss_s = 0                              # initialize variables for computing averages\n        for xb, yb in train_loader:\n            preds = model(xb)\n            train_loss = loss_func(preds, yb)\n            train_loss.backward()\n            \n            n_t += len(xb)\n            train_loss_s += train_loss.item() * len(xb)\n            \n            opt.step()\n            opt.zero_grad()\n        \n        model.eval()                                        # put the model in \"eval\" mode\n        n_v = valid_loss_s = acc_s = 0                      # initialize variables for computing averages\n        for xb, yb in valid_loader:\n            with torch.no_grad():                           # no need to compute gradients on validation set\n                preds = model(xb)\n                valid_loss = loss_func(preds, yb)\n                \n                n_v += len(xb)\n                valid_loss_s += valid_loss.item() * len(xb)\n                acc_s += accuracy(preds, yb) * len(xb)\n        \n        train_loss = train_loss_s / n_t                     # compute averages of loss and metrics\n        valid_loss = valid_loss_s / n_v\n        acc = acc_s / n_v\n        print(f'{epoch=} | {train_loss=:.3f} | {valid_loss=:.3f} | {acc=:.3f}')\n\n\nmodel, opt = get_model_opt()\n\nfit(5)\n\nepoch=0 | train_loss=2.198 | valid_loss=2.095 | acc=0.276\nepoch=1 | train_loss=1.980 | valid_loss=1.852 | acc=0.539\nepoch=2 | train_loss=1.718 | valid_loss=1.591 | acc=0.617\nepoch=3 | train_loss=1.481 | valid_loss=1.387 | acc=0.624\nepoch=4 | train_loss=1.305 | valid_loss=1.241 | acc=0.637\n\n\nAnd that‚Äôs it for this post (almost)! We have seen a lot of details on Datasets, Dataloaders and the transformation of data. We have used these concepts to improve our training loop: shuffling the training data on each epoch, and the computation of the metrics on the validation set. But before we close off, let‚Äôs make our very first exports into the library, so that next time we can continue where we finished off."
  },
  {
    "objectID": "posts/09_nntrain_ds/index.html#first-exports",
    "href": "posts/09_nntrain_ds/index.html#first-exports",
    "title": "nntrain (1/n): Datasets and Dataloaders",
    "section": "First exports",
    "text": "First exports\nWhen exporting code to a module with nbdev the first thing we need to do is declare the default_exp directive. This makes sure that when we run the export, the module will be exported to dataloaders.py\n\n #| default_exp dataloaders\n\nNext, we can export any code into the module by adding #|export on top of the cell we want to export. For example:\n\n #| export\n\ndef print_hello():\n    print('hello')\n\nTo export, we simply execute:\n\nimport nbdev; nbdev.nbdev_export()\n\nThis will create a file called dataloaders.py in the library folder (in my case nntrain) with the contents:\n# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_dataloaders.ipynb.\n\n# %% auto 0\n__all__ = ['func']\n\n# %% ../nbs/01_dataloaders.ipynb 59\ndef print_hello():\n    print('hello')\nSo what do we want to export here? Let‚Äôs see if we can create some generic code for loading data from the Huggingface datasets library into a PyTorch Dataloader:\n\n #|export\nimport torchvision.transforms.functional as TF\nfrom torch.utils.data import DataLoader\nimport torch\nimport PIL\n\n\n #|export\n\ndef hf_ds_collate_fn(data, flatten=True):\n    '''\n    Collation function for building a PyTorch DataLoader from a a huggingface dataset.\n    Tries to put all items from an entry into the dataset to tensor.\n    PIL images are converted to tensor, either flattened or not \n    '''\n\n    def to_tensor(i, flatten):\n        if isinstance(i, PIL.Image.Image):\n            if flatten:\n                return torch.flatten(TF.to_tensor(i))\n            return TF.to_tensor(i)\n        else:\n            return torch.tensor(i)\n    \n    to_tensor = partial(to_tensor, flatten=flatten)      # partially apply to_tensor() with flatten arg\n    data = [map(to_tensor, el.values()) for el in data]  # map each item from a dataset entry through to_tensor()\n    data = zip(*data)                                    # zip data of any length not just (x,y) but also (x,y,z)\n    return (torch.stack(i) for i in data)\n\n\n #|export\nclass DataLoaders:\n    def __init__(self, train, valid):\n        '''Class that exposes two PyTorch dataloaders as train and valid arguments'''\n        self.train = train\n        self.valid = valid\n    \n    @classmethod\n    def _get_dls(cls, train_ds, valid_ds, bs, collate_fn, **kwargs):\n        '''Helper function returning 2 PyTorch Dataloaders as a tuple for 2 Datasets. **kwargs are passed to the DataLoader'''\n        return (DataLoader(train_ds, batch_size=bs, shuffle=True, collate_fn=collate_fn, **kwargs),\n                DataLoader(valid_ds, batch_size=bs*2, collate_fn=collate_fn, **kwargs))\n        \n    @classmethod\n    def from_hf_dd(cls, dd, batch_size, collate_fn=hf_ds_collate_fn, **kwargs):\n        '''Factory method to create a Dataloaders object for a Huggingface Dataset dict,\n        uses the `hf_ds_collate_func` collation function by default, **kwargs are passes to the DataLoaders'''\n        return cls(*cls._get_dls(*dd.values(), batch_size, collate_fn, **kwargs))\n\nWith show_doc() we can include the documentations of class methods:\n\n #|hide\nfrom nbdev.showdoc import *\n\n\nshow_doc(DataLoaders.from_hf_dd)\n\n\n\nDataLoaders.from_hf_dd\n\n DataLoaders.from_hf_dd (dd, batch_size, collate_fn=&lt;function\n                         hf_ds_collate_fn&gt;, **kwargs)\n\nFactory method to create a Dataloaders object for a Huggingface Dataset dict, uses the hf_ds_collate_func collation function by default, **kwargs are passes to the DataLoaders\n\n\n\nExample usage:\n\ndef fit(epochs):\n    for epoch in range(epochs):\n        model.train()                                       \n        n_t = train_loss_s = 0                              \n        for xb, yb in dls.train:\n            preds = model(xb)\n            train_loss = loss_func(preds, yb)\n            train_loss.backward()\n            \n            n_t += len(xb)\n            train_loss_s += train_loss.item() * len(xb)\n            \n            opt.step()\n            opt.zero_grad()\n        \n        model.eval()                                        \n        n_v = valid_loss_s = acc_s = 0                      \n        for xb, yb in dls.valid: \n            with torch.no_grad():                           \n                preds = model(xb)\n                valid_loss = loss_func(preds, yb)\n                \n                n_v += len(xb)\n                valid_loss_s += valid_loss.item() * len(xb)\n                acc_s += accuracy(preds, yb) * len(xb)\n        \n        train_loss = train_loss_s / n_t                     \n        valid_loss = valid_loss_s / n_v\n        acc = acc_s / n_v\n        print(f'{epoch=} | {train_loss=:.3f} | {valid_loss=:.3f} | {acc=:.3f}')\n\n\nhf_dd = load_dataset('fashion_mnist')\nbs    = 1024\ndls = DataLoaders.from_hf_dd(hf_dd, bs)\n\nReusing dataset fashion_mnist (/root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1)\n\n\n\n\n\n\nmodel, opt = get_model_opt()\n\nfit(1)\n\nepoch=0 | loss=2.094 | acc=0.431\n\n\n\n #|hide\nimport nbdev; nbdev.nbdev_export()\n\nAnd that‚Äôs it. We have created our first module of the nntrain libraryüï∫. Links:\n\nDataloaders Notebook: the ‚Äúsource‚Äù of the source code\nDataloaders module: the .py source code exported from the notebook\nDocumentation: automatically created from the notebook and hosted on Github"
  },
  {
    "objectID": "posts/03_aiornot/index.html",
    "href": "posts/03_aiornot/index.html",
    "title": "First competitionüèÖ",
    "section": "",
    "text": "In the past couple of weeks I have participated in the first ever Hugging Face competition: aiornot. And as a matter of fact, it was also my first competition to participate in! The competition consisted of 62060 images (18618 train and 43442 test images) which were either created by an AI or not (binary image classification).\nToday, the competition has finished and the private leaderboard has been made public. I‚Äôm super happy (and proud üòá) that I finished in 15th place (98 participants):"
  },
  {
    "objectID": "posts/03_aiornot/index.html#credit-where-credit-is-due",
    "href": "posts/03_aiornot/index.html#credit-where-credit-is-due",
    "title": "First competitionüèÖ",
    "section": "Credit where credit is due:",
    "text": "Credit where credit is due:\n\nü§ó Hugging Face\nI would like to thank Hugging Face and in particular Abhishek Thakur for organizing this competition. I started looking for a first competition at Kaggle a few weeks back, and was very interested in the RSNA competition but quickly found that it was probably a bit too complicated for my first competition. I then saw a tweet from Abhishek announcing this competition and found it a perfect competition to get started.\n\n\nfastai\nIn the past month I have been following the fastai course and I am extremely grateful to Jeremy Howard and Sylvain Gugger for creating fastai. The book, the course, the videos and the great community they have built is really something special and is perfectly tailored for anybody who wants to get started with Deep Learning. Without fastai I could never have pulled this off üôè."
  },
  {
    "objectID": "posts/03_aiornot/index.html#learnings-and-notes",
    "href": "posts/03_aiornot/index.html#learnings-and-notes",
    "title": "First competitionüèÖ",
    "section": "Learnings and notes",
    "text": "Learnings and notes\n\nI quickly learned that data augmentation didn‚Äôt work well on this data. Initially I was a bit surprised by this, but upon inspection of the images I arrived at the following intuition. Normally we want to classify images by what‚Äôs being displayed in the image. So 2 images of a bike should both be classified as such. However, in this dataset we can have images of the same object but if one is created by an AI, and the other is not then they should be classified differently. So instead of looking at what‚Äôs being displayed, it probably has to learn more about the style or the way the image is built up. I can imagine that data augmentation makes this more difficult, especially warping, affine transformations and brightness, contrast augmentations. I was happily surprised to find that the 2nd and 4th place solutions also didn‚Äôt use these data augmentation!\nTraining on larger images works very well. I got a large performance boost for switching to sizes of 416. Jeremy Howard mentioned that this generally works well, and I think because of the nature of these images it worked especially well. To train large models on large images, I heavily relied on Gradient Accumulation to not have to reduce the batchsize.\nTransformer based models such as SWIN and VIT performed not as good as models based on convolutions, I used the convnext models.\nProgressive resizing didn‚Äôt work for me.\nI tried training on 5 epochs and 10 epochs. 10 epochs never gave me better results.\n\nLast but not least:\nParticipating in competitions is very motivating and rewarding. Working individually through courses, exercises and lecture notes is very interesting, but you don‚Äôt get a lot of feedback to how you are doing. Am I doing well? Should I spend more time on investigations into certain areas? When participating in a real-world competition you have a very clear goal, and you get immediate feedback on how you are doing. This type of project based learning has the advantage that it‚Äôs very clear what you need to focus on: anything that you encounter during the project.\nIt‚Äôs also great that it has a finite timeline, so that afterwards you can have a sense of achievement which motivates a lot. The Germans have a very nice word for this: Erfolgserlebnis.\n\n\n\nImage for Stable Diffusion prompt: ‚ÄúSense of achievement when finishing my first ever machine learning competition‚Äù"
  },
  {
    "objectID": "posts/07_stable_diffusion_code/index.html",
    "href": "posts/07_stable_diffusion_code/index.html",
    "title": "Introduction to Stable Diffusion - Code",
    "section": "",
    "text": "In the previous blog post, the main components and some intuition behind Stable Diffusion were introduced. Now, let‚Äôs see how we can use the HuggingFace diffusers library to generate images. The content of this blog post is based on Lesson 9 and Lesson 10 of Deep Learning for Coders. The end-to-end pipeline is very practical and easy to use, it‚Äôs basically a one-liner. We create a diffusion pipeline by downloading pre-trained models from a repo in the HuggingFace hub. Then, we can call this pipe object with a certain prompt:\n# pip install diffusers==0.12.1\n# pip install accelerate\n# pip install transformers==4.25.1\n\nfrom torchvision import transforms as tfms\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom PIL import Image\n\nnum_inference_steps = 50\nbatch_size = 1\n\npipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\").to(\"cuda\")\n\nprompt = \"Homer from the Simpsons on his roadbike climbing a mountain in the Pyrenees\"\n\n\n\n\nThe config attributes {'scaling_factor': 0.18215} were passed to AutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\ntorch.manual_seed(114)\npipe(prompt, num_inference_steps=num_inference_steps, guidance_scale=7.5).images[0]\nNot bad, but not great either. Let‚Äôs dive one layer deeper, and create the components described in the previous post: the Unet, the autoencoder, text encoder and noise scheduler:\nfrom diffusers import AutoencoderKL, LMSDiscreteScheduler, UNet2DConditionModel\nfrom transformers import CLIPTextModel, CLIPTokenizer, logging\n\nlogging.set_verbosity_error()\n\n# Autoencoder, to go from image -&gt; latents (encoder) and back (decoder)\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(\"cuda\")\n\n# UNet, to predict the noise (latents) from noisy image (latents)\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\").to(\"cuda\")\n\n# Tokenizer and Text encoder to create prompt embeddings\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(\"cuda\")\n\n# The noise scheduler\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\nscheduler.set_timesteps(num_inference_steps)\n\nThe config attributes {'scaling_factor': 0.18215} were passed to AutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\nTo use these components, we have to first tokenize the prompt. Tokenization is nothing more then transforming each word of the prompt into it‚Äôs associated integer according to a ‚Äúvocabulary‚Äù. The ‚Äúvocabulary‚Äù is the mapping of words to integers and is thus generally quite large.\ntext_input = tokenizer(prompt,               # the prompt we want to tokenize\n                       padding=\"max_length\", # pad the tokenized input to the max length\n                       return_tensors=\"pt\")  # return PyTorch tensors\ntext_input.input_ids\n\ntensor([[49406, 16931,   633,   518, 21092,   525,   787,  4370,  3701,  9877,\n           320,  3965,   530,   518, 39744, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407]])\nAbove we see the integers that are associated with each word in our prompt. We can decode the integers back into words and see if it matches our prompt. Let‚Äôs have a look at the first 5 tokens:\n[tokenizer.decode(token) for token in text_input.input_ids[0]][:5]\n\n['&lt;|startoftext|&gt;', 'homer', 'from', 'the', 'simpsons']\nWe see that all capital letters have been removed by the tokenization, and a special token is inserted at the beginning of the prompt. Also, we see the integer 49407 is being used to pad our input to the maximum length:\ntokenizer.decode(49407)\n\n'&lt;|endoftext|&gt;'\nNext, we will pass these tokens through the text-encoder to turn each token into an embedding vector. Since we have 77 tokens and the embeddings are of size 768, this will be a tensor of shape [77, 768].\ntext_embeddings = text_encoder(text_input.input_ids.to(\"cuda\"))[0]\ntext_embeddings.shape\n\ntorch.Size([1, 77, 768])\nWhen generating a completely new image, we start with a fully random noisy latent, so let‚Äôs create one:\ntorch.manual_seed(1024)\nlatents = torch.randn((batch_size,              # batch size: 1\n                       unet.config.in_channels, # input channels of the unet: 4\n                       unet.config.sample_size, # height dimension of the unet: 64\n                       unet.config.sample_size) # width dimension of the unet: 64\n                     ).to(\"cuda\")               # put the tensor on the GPU\n\nlatents = latents * scheduler.init_noise_sigma  # scale the noise\n\nlatents.shape\n\ntorch.Size([1, 4, 64, 64])\nThe latents thus carry 4 channels and are of size 64 by 64. Let‚Äôs pass this latent iteratively through the Unet, each time subtracting partly the amount of predicted noise (the output of the Unet)\nfor i, t in enumerate(scheduler.timesteps):\n    inputs = latents\n    inputs = scheduler.scale_model_input(inputs, t)\n\n    # predict the noise \n    with torch.no_grad(): \n        pred = unet(inputs, t, encoder_hidden_states=text_embeddings).sample\n\n    # update the latents by removing the predicted noise according to the noise schedule\n    latents = scheduler.step(pred, t, latents).prev_sample\nLet‚Äôs visualize the four channels of this latent representation in grey-scale:\nfig, axs = plt.subplots(1, 4, figsize=(16, 4))\nfor c in range(4):\n    axs[c].imshow(latents[0][c].cpu(), cmap='Greys')\nTo transform the latent representation to full-size images, we can use the decoder of the VAE. Note that when we do that, we move from a tensor of shape [4, 64, 64] to [3, 512, 512]:\nprint(latents.shape, vae.decode(latents).sample.shape)\n\ntorch.Size([1, 4, 64, 64]) torch.Size([1, 3, 512, 512])\nAnd let‚Äôs visualize the result:\n#scale back according to the VAE paper\nwith torch.no_grad(): image = vae.decode(1 / 0.18215 * latents).sample\n\n# move tensor to numpy\nimage = image[0].detach().cpu().permute(1, 2, 0).numpy()\n# scale the values to 0-255\nimage = ((image / 2 + 0.5).clip(0, 1) * 255).round().astype(\"uint8\")\nImage.fromarray(image)\nUnfortunately, the result looks very bad and especially much worse then our one-liner. The main reason for this, is that the StableDiffusionPipeline is using something called Classifier Free Diffusion Guidance. So let‚Äôs have a look at that. But before we do, let‚Äôs add two code snippets to transfrom from the latent representation to the full size image representation and back. We will do this a couple of times, so it helps to keep the code a bit cleaner:\ndef latents_to_image(latent):\n    with torch.no_grad(): \n        image = vae.decode(1 / 0.18215 * latent).sample\n\n    image = image[0].detach().cpu().permute(1, 2, 0).numpy()\n    image = ((image / 2 + 0.5).clip(0, 1) * 255).round().astype(\"uint8\")\n    return Image.fromarray(image)\n    \ndef image_to_latent(input_im):\n    with torch.no_grad():\n        latent = vae.encode(torch.Tensor(np.transpose(np.array(input_im) / 255., (2, 0, 1))).unsqueeze(0).to('cuda')*2-1)\n    return 0.18215 * (latent).latent_dist.sample()"
  },
  {
    "objectID": "posts/07_stable_diffusion_code/index.html#classifier-free-diffusion-guidance",
    "href": "posts/07_stable_diffusion_code/index.html#classifier-free-diffusion-guidance",
    "title": "Introduction to Stable Diffusion - Code",
    "section": "Classifier Free Diffusion Guidance",
    "text": "Classifier Free Diffusion Guidance\nClassifier Free Guidance refers to a technique in which two images are being constructed at the same time from the same latent. One of the images is being reconstructed based on the specified prompt (conditional generation), the other image is being generated by an empty prompt (unconditional generation). By mixing the two images in the process according to a parameter (called the guidance-scale) the generated image for the prompt is going to look much better:\n\ntorch.manual_seed(3016)\n\ncond_input = tokenizer(\"Homer from the Simpsons on his roadbike climbing a mountain in the Pyrenees\", padding=\"max_length\", return_tensors=\"pt\") \ncond_embeddings = text_encoder(cond_input.input_ids.to(\"cuda\"))[0]\n\n# Create embeddings for the unconditioned process\nuncond_input = tokenizer(\"\", padding=\"max_length\", return_tensors=\"pt\") \nuncond_embeddings = text_encoder(uncond_input.input_ids.to(\"cuda\"))[0]\n\n# Concatenate the embeddings\nembeddings = torch.cat([cond_embeddings, uncond_embeddings])\n\nguidance_scale = 7.5\n\n# Create a \"fresh\" random latent to start with\nlatents = torch.randn((batch_size, unet.config.in_channels, unet.config.sample_size, unet.config.sample_size)).to(\"cuda\")               \nlatents = latents * scheduler.init_noise_sigma\n\nfor i, t in enumerate(scheduler.timesteps):\n    inputs = torch.cat([latents, latents]) # concatenate the latents\n    inputs = scheduler.scale_model_input(inputs, t)\n\n    # predict the noise \n    with torch.no_grad(): \n        pred = unet(inputs, t, encoder_hidden_states=embeddings).sample\n    \n    # pull both images apart again\n    pred_cond, pred_uncond = pred.chunk(2)\n    \n    # mix the results according to the guidance scale parameter\n    pred = pred_uncond + guidance_scale * (pred_cond - pred_uncond)\n\n    # update the latents by removing the predicted noise according to the noise schedule\n    latents = scheduler.step(pred, t, latents).prev_sample\n    \nlatents_to_image(latents)\n\n\n\n\nMuch better! As you can see, Classifier Free Guidance is a simple technique but it works very well. This morning (03-07-2023) I saw a tweet that introduced the same concept to the world of Large Language Models (LLMs):"
  },
  {
    "objectID": "posts/07_stable_diffusion_code/index.html#negative-prompt",
    "href": "posts/07_stable_diffusion_code/index.html#negative-prompt",
    "title": "Introduction to Stable Diffusion - Code",
    "section": "Negative Prompt",
    "text": "Negative Prompt\nAs mentioned, the unconditional image with Classifier Free Guidance is created from an empty prompt. It turns out that we can use the prompt of this second image as a so-called negative prompt. If there are certain elements we don‚Äôt want to see in our image, we can specify it in this prompt.\nWe can see this by rewriting the Classifier Free Guidance equation:\n\\[\\begin{align}\np &= p_{uc} + g (p_{c} - p_{uc}) \\\\\np &= g p_{c} + (1 - g) p_{uc} \\\\\n\\end{align}\\]\nSo with a guidance scale value larger than 1, the unconditional prediction \\(p_{uc}\\) is being subtracted from the conditional prediction \\(p_c\\), which has the effect of removing the concept from the conditional image.\nAn example of Homer Simpson eating lunch:\n\ncond_input = tokenizer(\"Homer Simpson eating lunch\", padding=\"max_length\", return_tensors=\"pt\") \ncond_embeddings = text_encoder(cond_input.input_ids.to(\"cuda\"))[0]\n\nuncond_input = tokenizer(\"\", padding=\"max_length\", return_tensors=\"pt\") \nuncond_embeddings = text_encoder(uncond_input.input_ids.to(\"cuda\"))[0]\n\n\n\nCode\ntorch.manual_seed(105)\n\nembeddings = torch.cat([cond_embeddings, uncond_embeddings])\n\nguidance_scale = 7.5\n\nlatents = torch.randn((batch_size, unet.config.in_channels, unet.config.sample_size, unet.config.sample_size)).to(\"cuda\")               \nlatents = latents * scheduler.init_noise_sigma\n\nfor i, t in enumerate(scheduler.timesteps):\n    inputs = torch.cat([latents, latents]) # concatenate the latents\n    inputs = scheduler.scale_model_input(inputs, t)\n\n    with torch.no_grad(): \n        pred = unet(inputs, t, encoder_hidden_states=embeddings).sample\n    \n    pred_cond, pred_uncond = pred.chunk(2)\n    \n    pred = pred_uncond + guidance_scale * (pred_cond - pred_uncond)\n\n    latents = scheduler.step(pred, t, latents).prev_sample\n\nlatents_to_image(latents)\n\n\n\n\n\nAnd removing the blue chair by using a negative prompt:\n\nuncond_input = tokenizer(\"blue chair\", padding=\"max_length\", return_tensors=\"pt\") \nuncond_embeddings = text_encoder(uncond_input.input_ids.to(\"cuda\"))[0]\n\n\n\nCode\ntorch.manual_seed(105)\n\nembeddings = torch.cat([cond_embeddings, uncond_embeddings])\n\nguidance_scale = 7.5\n\nlatents = torch.randn((batch_size, unet.config.in_channels, unet.config.sample_size, unet.config.sample_size)).to(\"cuda\")               \nlatents = latents * scheduler.init_noise_sigma\n\nfor i, t in enumerate(scheduler.timesteps):\n    inputs = torch.cat([latents, latents]) # concatenate the latents\n    inputs = scheduler.scale_model_input(inputs, t)\n\n    with torch.no_grad(): \n        pred = unet(inputs, t, encoder_hidden_states=embeddings).sample\n    \n    pred_cond, pred_uncond = pred.chunk(2)\n    \n    pred = pred_uncond + guidance_scale * (pred_cond - pred_uncond)\n\n    latents = scheduler.step(pred, t, latents).prev_sample\n\nimage = latents_to_image(latents)\nimage\n\n\n\n\n\nAnd gone is the blue chair! I must admit that this doesn‚Äôt always work as great as in this example, in fact I had to try out quite a lot of prompts in combination with negative prompts to find a good example for this post.."
  },
  {
    "objectID": "posts/07_stable_diffusion_code/index.html#image-to-image-generation",
    "href": "posts/07_stable_diffusion_code/index.html#image-to-image-generation",
    "title": "Introduction to Stable Diffusion - Code",
    "section": "Image-to-image generation",
    "text": "Image-to-image generation\nImage-to-image generation is another super interesting process, in which we use both a prompt and an image to guide the generation process. This comes in handy, if for example we want to create a variant of an image we already have. Let‚Äôs say we have an awesome image of Homer eating a burger, and we want to have a similar image but instead we want Marge to eat the burger, or we want Homer to eat a slice of pizza instead. We can then feed both the correct promt as well as the already existing image to guide the image generation process even more.\nThe way this works, is by not starting with a completely random latent, but instead build a noisy latent of our existing image.\nLet‚Äôs start with the image above and add some noise to it, for example by adding the noise for level 15 (we have 50 noise levels in total, so level 15 means that we still have 35 denoising steps to go):\n\nlatent = image_to_latent(image)\nnoise_latent = torch.randn_like(latent)\n\nsampling_step = 15\nnoised_latent = scheduler.add_noise(latent, noise_latent, timesteps=torch.tensor([scheduler.timesteps[sampling_step]]))\n\nlatents_to_pil(noised_latent)[0]\n\n\n\n\nIf you squint with your eyes you can already see some structure, in the middle there is some yellow blob sitting around (eating lunch..). Let‚Äôs take this noisy latent, and do the remaining 35 denoising steps with a different prompt:\n\ntorch.manual_seed(105)\n\ncond_input = tokenizer(\"Homer Simpson eating Hot Pot\", padding=\"max_length\", return_tensors=\"pt\") \ncond_embeddings = text_encoder(cond_input.input_ids.to(\"cuda\"))[0]\n\nuncond_input = tokenizer(\"\", padding=\"max_length\", return_tensors=\"pt\") \nuncond_embeddings = text_encoder(uncond_input.input_ids.to(\"cuda\"))[0]\n\nembeddings = torch.cat([cond_embeddings, uncond_embeddings])\n\nguidance_scale = 7.5\n\n# We start with the noised_latent coming from the image, defined above\nlatents = noised_latent\n\nfor i, t in enumerate(scheduler.timesteps):\n    # we only do the steps starting from the specified level\n    if i &gt;= sampling_step:\n        \n        inputs = torch.cat([latents, latents])\n        inputs = scheduler.scale_model_input(inputs, t)\n\n        with torch.no_grad(): \n            pred = unet(inputs, t, encoder_hidden_states=embeddings).sample\n\n        pred_cond, pred_uncond = pred.chunk(2)\n        pred = pred_uncond + guidance_scale * (pred_cond - pred_uncond)\n\n        latents = scheduler.step(pred, t, latents).prev_sample\n    \nlatents_to_image(latents)\n\n\n\n\nThis image is very similar to what we started with. Color scheme, composition and camera angle are all the same. At the same time, the prompt is also reflected by a change of dishes on the table.\nAnd that‚Äôs it, that‚Äôs image-to-image generation. As you can see, it‚Äôs nothing deeply complicated, it‚Äôs just a smart way to re-use the components we have already seen.\nI hope this blog post shows how the components that are introduced in the previous post, translate to code. The examples shown here, only touch upon what can be achieved. In fact, the lessons upon which this post is based show a lot more interesting concepts such as textual inversion. If you are interested, have a look here"
  },
  {
    "objectID": "posts/10_nntrain_learner/index.html",
    "href": "posts/10_nntrain_learner/index.html",
    "title": "nntrain (2/n): Learner",
    "section": "",
    "text": "In this series, I want to discuss the creation of a small library for training neural networks: nntrain. It‚Äôs based off the excellent part 2 of Practical Deep Learning for Coders by Jeremy Howard, in which from lessons 13 to 18 (roughly) the development of the miniai library is discussed.\nThe library will build upon PyTorch. We‚Äôll try as much as possible to build from scratch to understand how it all works. Once the main functionality of components are implemented and verified, we can switch over to PyTorch‚Äôs version. This is similar to how things are done in the course. However, this is not just a ‚Äúcopy / paste‚Äù of the course: on many occasions I take a different route, and most of the code is my own. That is not to say that all of this is meant to be extremely innovative, instead I had the following goals:\nnb_dev is another great project from the fastai community, which allows python libraries to be written in jupyter notebooks. This may sound a bit weird since the mainstream paradigm is to only do experimental work in notebooks. It has the advantage though that we can create the source code for our library in the very same environment in which we want to experiment and interact with our methods, objects and structure while we are building the library. For more details on why this is a good idea and other nice features of nb_dev, see here.\nSo without further ado, let‚Äôs start with where we left off in the previous post:"
  },
  {
    "objectID": "posts/10_nntrain_learner/index.html#end-of-last-post",
    "href": "posts/10_nntrain_learner/index.html#end-of-last-post",
    "title": "nntrain (2/n): Learner",
    "section": "End of last post:",
    "text": "End of last post:\nWe finished the last post with exporting the dataloaders module into the nntrain library, which helps transforming a huggingface dataset dictionary into PyTorch dataloaders, so let‚Äôs use that:\n\nfrom datasets import load_dataset,load_dataset_builder\n\nfrom nntrain.dataloaders import DataLoaders, hf_ds_collate_fn\n\n\n #| export\nimport torchvision.transforms.functional as TF\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom operator import attrgetter\nimport fastcore.all as fc\nimport math\nfrom fastprogress import progress_bar,master_bar\nimport torcheval.metrics as tem\nimport matplotlib.pyplot as plt\n\n\nname = \"fashion_mnist\"\nds_builder = load_dataset_builder(name)\nhf_dd = load_dataset(name)\n\nbs = 1024\ndls = DataLoaders.from_hf_dd(hf_dd, batch_size=bs)\n\n# As a reminder, `DataLoaders` expose a PyTorch train and validation dataloader as `train` and `valid` attributes:\n\ndls.train, dls.valid\n\nReusing dataset fashion_mnist (/root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1)\n\n\n\n\n\n(&lt;torch.utils.data.dataloader.DataLoader&gt;,\n &lt;torch.utils.data.dataloader.DataLoader&gt;)"
  },
  {
    "objectID": "posts/10_nntrain_learner/index.html#learner-class",
    "href": "posts/10_nntrain_learner/index.html#learner-class",
    "title": "nntrain (2/n): Learner",
    "section": "Learner Class",
    "text": "Learner Class\nLet‚Äôs continue to formalize our training loop into a Learner class with a fit() method. The training loop created so far looks like this:\ndef fit(epochs):\n    for epoch in range(epochs):\n        model.train()                                       \n        n_t = train_loss_s = 0                              \n        for xb, yb in dls.train:\n            preds = model(xb)\n            train_loss = loss_func(preds, yb)\n            train_loss.backward()\n            \n            n_t += len(xb)\n            train_loss_s += train_loss.item() * len(xb)\n            \n            opt.step()\n            opt.zero_grad()\n        \n        model.eval()                                        \n        n_v = valid_loss_s = acc_s = 0                      \n        for xb, yb in dls.valid: \n            with torch.no_grad():                           \n                preds = model(xb)\n                valid_loss = loss_func(preds, yb)\n                \n                n_v += len(xb)\n                valid_loss_s += valid_loss.item() * len(xb)\n                acc_s += accuracy(preds, yb) * len(xb)\n        \n        train_loss = train_loss_s / n_t                     \n        valid_loss = valid_loss_s / n_v\n        acc = acc_s / n_v\n        print(f'{epoch=} | {train_loss=:.3f} | {valid_loss=:.3f} | {acc=:.3f}')\nLet‚Äôs build this class in steps. Initialization is straight forward:\n\nclass Learner():\n    def __init__(self, model, dls, loss_fn, metric_fn, optim_class, lr):\n        self.model = model\n        self.dls = dls\n        self.loss_fn = loss_fn\n        self.metric_fn = metric_fn\n        self.optim = optim_class(model.parameters(), lr)\n\nNext, let‚Äôs implement the training loop. Instead of writing the full loop in a single fit() method, let‚Äôs try to break the training loop down into pieces:\n\nfit() iterates through the epochs\nPer epoch we do a training round through the training data and an evaluation round through the validation set. Both rounds are quite similar, so let‚Äôs put this functionality in a separate method one_epoch()\nIn each epoch we iterate through the batches of a dataloader, let‚Äôs put this functionality in a method one_batch()\n\nlet‚Äôs define the outer most call: fit(). In this method we‚Äôll call one_epoch twice, once for the training and once for the validation. Both passes are fairly similar as can be seen from comparing lines 3-8 with 16-21\n\n@fc.patch\ndef fit(self:Learner, epochs):\n    for epoch in range(epochs):                # iterate through the epochs\n        self.one_epoch(epoch, train=True)      # one epoch through the training dataloader\n        with torch.no_grad():                  # for the validation epoch we don't need grads\n            self.one_epoch(epoch, train=False) # one epoch through the validation dataloader\n\nNext, let‚Äôs implement one_epoch(). To make sure each method does one thing, we factor do_batch() out into it‚Äôs own method:\n\n@fc.patch\ndef one_epoch(self:Learner, epoch, train):\n    self.reset_stats()                         # reset the stats at beginning of each epoch\n    self.model.train(train)                    # put the model either in train or validation mode\n    self.dl = self.dls.train if train else self.dls.valid # reference to the active dataloader\n    for self.batch in self.dl:                 # iterate through the active dataloader\n        self.one_batch(train)                  # do one batch\n    self.print_stats(epoch, train)             # print stats at the end of the epoch\n\nAnd finally the method responsible for dealing with a single batch of data:\n\n@fc.patch\ndef one_batch(self:Learner, train):\n    self.xb, self.yb = self.batch              # self.batch is either a training or validation batch\n    self.preds = self.model(self.xb)           # forward pass through the model\n    self.loss = self.loss_fn(self.preds, self.yb)  # loss\n    if train:                                  # only do a backward and weight update if train\n        self.loss.backward()\n        self.optim.step()\n        self.optim.zero_grad()\n    self.update_stats()                        # update stats\n\nAnd the methods related to the computation of the statistics:\n\n@fc.patch\ndef update_stats(self:Learner):\n    n = len(self.xb)\n    self.loss_s += self.loss.item() * n\n    self.metric_s += self.metric_fn(self.preds, self.yb).item() * n\n    self.counter += n\n\n@fc.patch\ndef reset_stats(self:Learner):\n    self.counter = 0\n    self.loss_s = 0\n    self.metric_s = 0\n\n@fc.patch\ndef print_stats(self:Learner, epoch, train):\n    loss = self.loss_s / self.counter\n    metric = self.metric_s / self.counter\n    print(f'{epoch=:02d} | {\"train\" if train else \"eval\":&lt;5} | {loss=:.3f} | {metric=:.3f}')\n\nLet‚Äôs try it out on the data:\n\nn_in = 28*28\nn_h = 50\nn_out = 10\nlr = 0.01\n\ndef accuracy(preds, targs):\n    return (preds.argmax(dim=1) == targs).float().mean()\n\ndef get_model():\n    layers = [nn.Linear(n_in, n_h), nn.ReLU(), nn.Linear(n_h, n_out)]\n    return nn.Sequential(*layers)\n\nl = Learner(get_model(), dls, F.cross_entropy, accuracy, torch.optim.SGD, lr)\n\nl.fit(5)\n\nepoch=00 | train | loss=2.202 | metric=0.240\nepoch=00 | eval  | loss=2.078 | metric=0.378\nepoch=01 | train | loss=1.943 | metric=0.464\nepoch=01 | eval  | loss=1.802 | metric=0.538\nepoch=02 | train | loss=1.667 | metric=0.578\nepoch=02 | eval  | loss=1.543 | metric=0.601\nepoch=03 | train | loss=1.435 | metric=0.628\nepoch=03 | eval  | loss=1.346 | metric=0.639\nepoch=04 | train | loss=1.266 | metric=0.652\nepoch=04 | eval  | loss=1.207 | metric=0.651"
  },
  {
    "objectID": "posts/10_nntrain_learner/index.html#callbacks-pubsub-and-event-handlers",
    "href": "posts/10_nntrain_learner/index.html#callbacks-pubsub-and-event-handlers",
    "title": "nntrain (2/n): Learner",
    "section": "Callbacks, pubsub and event handlers",
    "text": "Callbacks, pubsub and event handlers\nOn the one side we want to keep the Learner and its training loop generic on the other side we need to be able to tweak the dynamics of the training loop depending on the use-case. One way to customize the training loop, without having to re-write the training loop would be to add a publish/subscribe (pubsub) mechanism. In the FastAI course, they are referred to as ‚Äúcallbacks‚Äù, and although callbacks, event handlers and pubsub are all related and basically refer to any logic (encapsulated in a function) which we want to specify now, and execute at a later point in time whenever some condition arises. In my vocabulary a callback is a function that is passed to another function, and is executed whenever that other function is finished.\nFor the purposes of training neural networks we have the following requirements:\n\nThe Learner framework defines a number of ‚Äúevents‚Äù that are published:\n\nbefore_fit, after_fit\nbefore_epoch, after_epoch\nbefore_batch, after_batch\n\nSubscribers are classes that implement methods (e.g.¬†before_fit()) that will be triggered whenever the associated event is published. They also have an order attribute which determines the order in which they are called in case multiple Subscribers subscribed to the same event.\nAs an additional feature, subscribers will be able to redirect flow, but we will come back to that later\n\nSo let‚Äôs implement this. First, we will need to store a list of subscribers in the Learner class:\n\nclass Learner():\n    def __init__(self, model, dls, loss_fn, metric_fn, optim_class, lr, subs):\n        self.model = model\n        self.dls = dls\n        self.loss_fn = loss_fn\n        self.metric_fn = metric_fn\n        self.optim = optim(model.parameters(), lr)\n        self.subs = subs\n\nNext, let‚Äôs define a method for publishing events. The method will go through the registered subscribers and if a method with the name of the event is declared, call that method passing the learner object as an argument:\n\n@fc.patch\ndef publish(self:Learner, event):\n    for sub in sorted(self.subs, key=attrgetter('order')):\n        method = getattr(sub, name, None)\n        if method is not None: method(self)\n\nWith the before_x / after_x events, realize that we have three times the same construct:\npublish \"before_event\" event\ndo event\npublish \"after_event\" event\nWith event being either fit, epoch or batch. So instead of adding this construct multiple times in the training loop let‚Äôs define a class we can use as a decorater wrapping the do_event logic:\n\n #| export\n\nclass PublishEvents():\n    def __init__(self, event): \n        self.event = event\n    \n    def __call__(self, decorated_fn):\n        def decorated_fn_with_publishing(learner, *args, **kwargs):\n            learner.publish(f'before_{self.event}')\n            decorated_fn(learner, *args, **kwargs)\n            learner.publish(f'after_{self.event}')\n        return decorated_fn_with_publishing\n\nTo implement this into the Learner we have to factor out the exact code we want to be executed in between the publishing of the before and after, see the additional _one_epoch() method.\nNote that we are taking out the logic concerning the statistics, this will be implemented as a Subscriber as we‚Äôll see.\n\nclass Learner():\n    def __init__(self, model, dls, loss_fn, metric_fn, optim_class, lr, subs):\n        self.model = model\n        self.dls = dls\n        self.loss_fn = loss_fn\n        self.metric_fn = metric_fn\n        self.optim = optim_class(model.parameters(), lr)\n        self.subs = subs\n    \n    @PublishEvents('fit')\n    def fit(self, epochs):\n        for epoch in range(epochs):\n            self.one_epoch(epoch, train=True)\n            with torch.no_grad():\n                self.one_epoch(epoch, train=False)\n\n    def one_epoch(self, epoch, train):\n        self.model.train(train)\n        self.dl = self.dls.train if train else self.dls.valid\n        self._one_epoch(epoch, train)\n        \n    @PublishEvents('epoch')\n    def _one_epoch(self, epoch, train):\n        for self.batch in self.dl:\n            self.xb, self.yb = self.batch\n            self.one_batch(train)\n    \n    @PublishEvents('batch')\n    def one_batch(self, train):\n        self.preds = self.model(self.xb)           \n        self.loss = self.loss_fn(self.preds, self.yb)\n        if train:                                  \n            self.loss.backward()\n            self.optim.step()\n            self.optim.zero_grad()\n        \n    def publish(self, event):\n        for sub in sorted(self.subs, key=attrgetter('order')):\n            method = getattr(sub, event, None)\n            if method is not None: method(self)            \n\nLet‚Äôs create a dummy subscriber and test it out:\n\n #| export\nclass Subscriber():\n    order = 0\n\nclass DummyS(Subscriber):\n    \n    def before_fit(self, learn):\n        print('before fitüëã')\n        \n    def after_fit(self, learn):\n        print('after fitüëã')\n        \n    def before_epoch(self, learn):\n        print('before epoch üí•')\n        \n    def after_epoch(self, learn):\n        print('after epoch üí•')\n\n\nl = Learner(get_model(), dls, F.cross_entropy, accuracy, torch.optim.SGD, lr, [DummyS()])\nl.fit(1)\n\nbefore fitüëã\nbefore epoch üí•\nafter epoch üí•\nbefore epoch üí•\nafter epoch üí•\nafter fitüëã"
  },
  {
    "objectID": "posts/10_nntrain_learner/index.html#subscribers-can-cancel-execution",
    "href": "posts/10_nntrain_learner/index.html#subscribers-can-cancel-execution",
    "title": "nntrain (2/n): Learner",
    "section": "Subscribers can cancel execution",
    "text": "Subscribers can cancel execution\nNow let‚Äôs add the last component of our pubsub system: subscribers should be able to cancel processing. For example, a a subscriber that would implement Early Stopping, will have to be able to cancel any further epochs when the validation loss starts increasing. One way to implement this, is with the help of Exceptions and try / except blocks:\nIt‚Äôs actually very easy to implement this logic, we only need to define custom Exceptions, and update the PublishEvents class to catch the exceptions that are thrown in any subscriber:\n\nclass CancelFitException(Exception): pass\nclass CancelEpochException(Exception): pass\nclass CancelBatchException(Exception): pass\n\n\nclass PublishEvents():\n    def __init__(self, name): \n        self.name = name\n    \n    def __call__(self, decorated_fn):\n        def decorated_fn_with_publishing(learner, *args, **kwargs):\n            try:\n                learner.publish(f'before_{self.name}')\n                decorated_fn(learner, *args, **kwargs)\n                learner.publish(f'after_{self.name}')\n            except globals()[f'Cancel{self.name.title()}Exception']: pass\n        return decorated_fn_with_publishing\n\n\nclass DummyS(Subscriber):\n    \n    def before_fit(self, learn): print('before fitüëã')\n        \n    def before_epoch(self, learn): raise CancelFitException\n    \n    def after_fit(self, learn): print('after fit üëã')\n\n\nl = Learner(get_model(), dls, F.cross_entropy, accuracy, torch.optim.SGD, lr, [DummyS()])\nl.fit(5)\n\nbefore fitüëã\n\n\nAnd indeed, the after_fit event is never called, since the fit was cancelled during before_epoch by the dummy subscriber"
  },
  {
    "objectID": "posts/10_nntrain_learner/index.html#final-version-of-learner",
    "href": "posts/10_nntrain_learner/index.html#final-version-of-learner",
    "title": "nntrain (2/n): Learner",
    "section": "Final version of Learner",
    "text": "Final version of Learner\nWe are going to make some final changes to the Learner class:\n\nfactor out the computation of the following logic. This is practical to create subclasses of Learner with custom behavior:\n\nprediction: self.predict()\nloss: self.get_loss()\nbackward pass: self.backward()\nstepping of weights: self.step()\nzeroing of gradients: self.zero_grad()\n\nadd a Subscriber argument to fit, these subs will only be added for the duration of the fit, and afterwards removed\nadd a couple of additional events (after_predict, after_loss, after_backward and after_step) to which subscribers can listen\n\n\n #| export\nclass Learner():\n    def __init__(self, model, dls, loss_fn, optim_class, lr, subs):\n        self.model = model\n        self.dls = dls\n        self.loss_fn = loss_fn\n        self.optim_class = optim_class\n        self.lr = lr\n        self.subs = subs\n    \n    def fit(self, epochs, train=True, valid=True, subs=[], lr=None):\n        for sub in subs: self.subs.append(sub)\n        self.n_epochs = epochs\n        self.epochs = range(self.n_epochs)\n        lr = self.lr if lr is None else lr\n        self.opt = self.optim_class(self.model.parameters(), lr)\n        try:\n            self._fit(train, valid)\n        finally:\n            for sub in subs: self.subs.remove(sub)\n                    \n    @PublishEvents('fit')\n    def _fit(self, train, valid):\n        for self.epoch in self.epochs:\n            if train: \n                self.one_epoch(True)\n            if valid:\n                with torch.no_grad():\n                    self.one_epoch(False)\n        \n    def one_epoch(self, train):\n        self.model.train(train)\n        self.dl = self.dls.train if train else self.dls.valid\n        self._one_epoch()\n        \n    @PublishEvents('epoch')\n    def _one_epoch(self):\n        for self.batch in self.dl: \n            self.one_batch()\n    \n    @PublishEvents('batch')\n    def one_batch(self):\n        self.predict()\n        self.publish('after_predict')\n        self.get_loss()\n        self.publish('after_loss')\n        if self.model.training:\n            self.backward()\n            self.publish('after_backward')\n            self.step()\n            self.publish('after_step')\n            self.zero_grad()\n        \n    def publish(self, event):\n        for sub in sorted(self.subs, key=attrgetter('order')):\n            method = getattr(sub, event, None)\n            if method is not None: method(self)\n            \n    def predict(self): \n        self.preds = self.model(self.batch[0])\n        \n    def get_loss(self): \n        self.loss = self.loss_fn(self.preds, self.batch[1])\n        \n    def backward(self): self.loss.backward()\n    def step(self): self.opt.step()\n    def zero_grad(self): self.opt.zero_grad()"
  },
  {
    "objectID": "posts/10_nntrain_learner/index.html#metrics-subscriber",
    "href": "posts/10_nntrain_learner/index.html#metrics-subscriber",
    "title": "nntrain (2/n): Learner",
    "section": "Metrics Subscriber",
    "text": "Metrics Subscriber\nSince we took out the metrics, let‚Äôs create a subscriber that adds that. We want the subscriber to be generic, to it should be able to accept one or multiple metrics. Let‚Äôs make sure that it can accept the metrics from the torcheval library:\n\nmetric = tem.Mean()\n\nmetric.update(torch.tensor([1,2,3]))  # update() adds data\nmetric.update(torch.tensor([4,5,6]))  \nprint(metric.compute())               # compute() computes the metric\n\nmetric.reset()                        # remove all data\nprint(metric.compute())\n\nWARNING:root:No calls to update() have been made - returning 0.0\n\n\ntensor(3.5000, dtype=torch.float64)\ntensor(0., dtype=torch.float64)\n\n\n\n #|export\nclass MetricsS(Subscriber):\n    def __init__(self, **metrics):\n        self.metrics = metrics\n        self.loss = tem.Mean()\n        \n    def before_fit(self, learn): \n        learn.metrics = self\n    \n    def before_epoch(self, learn):\n        for m in self.metrics.values(): m.reset()\n        self.loss.reset()\n    \n    def after_batch(self, learn):\n        x,y,*_ = self.to_cpu(learn.batch)\n        for m in self.metrics.values(): m.update(self.to_cpu(learn.preds), y)\n        self.loss.update(self.to_cpu(learn.loss), weight=len(x))\n        \n    def after_epoch(self, learn):\n        log = {\n            'epoch': learn.epoch,\n            'mode': 'train' if learn.model.training else 'eval',\n            'loss' : f'{self.loss.compute():.3f}'\n        }\n        for k, v in self.metrics.items():\n            log[k] = f'{v.compute():.3f}'\n        self.output(log)\n        \n    def to_cpu(self, x):\n        if isinstance(x, list): return (self.to_cpu(el) for el in x)\n        return x.detach().cpu()\n        \n    def output(self, log): print(log)\n\n\nmetrics_s = MetricsS(accuracy=tem.MulticlassAccuracy())\n\nl = Learner(get_model(), dls, F.cross_entropy, torch.optim.SGD, lr, [metrics_s])\nl.fit(1)\n\n{'epoch': 0, 'mode': 'train', 'loss': '2.220', 'accuracy': '0.206'}\n{'epoch': 0, 'mode': 'eval', 'loss': '2.121', 'accuracy': '0.352'}"
  },
  {
    "objectID": "posts/10_nntrain_learner/index.html#device-subscriber",
    "href": "posts/10_nntrain_learner/index.html#device-subscriber",
    "title": "nntrain (2/n): Learner",
    "section": "Device Subscriber",
    "text": "Device Subscriber\nIt‚Äôs time we start training on the GPU, to do that we have to move the model (and it‚Äôs parameters) as well as all the data onto the GPU. We can easily do this with a Subscriber:\n\nmove the model (and all it‚Äôs trainable parameters) to the device before fit\nmove each batch to the device before batch\n\n\n #| export\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \nclass DeviceS(Subscriber):\n    \n    def __init__(self, device):\n        self.device = device\n    \n    def before_fit(self, learn):\n        learn.model.to(self.device)\n    \n    def before_batch(self, learn):\n        learn.batch = [x.to(self.device) for x in learn.batch]\n\n\ndevice_s = DeviceS(device)\n\nl = Learner(get_model(), dls, F.cross_entropy, torch.optim.SGD, lr, [metrics_s, device_s])\nl.fit(1)\n\n{'epoch': 0, 'mode': 'train', 'loss': '2.209', 'accuracy': '0.256'}\n{'epoch': 0, 'mode': 'eval', 'loss': '2.115', 'accuracy': '0.306'}"
  },
  {
    "objectID": "posts/10_nntrain_learner/index.html#learning-rate-finder",
    "href": "posts/10_nntrain_learner/index.html#learning-rate-finder",
    "title": "nntrain (2/n): Learner",
    "section": "Learning Rate Finder",
    "text": "Learning Rate Finder\nThe learning rate finder is very simple technique that can be used to find a good learning rate for training a network. It works like this:\n\nStart with a very small learning rate\nDo a forward pass through the network of a single batch of data and record the loss\nIncrease the learning rate with constant factor\nDo another forward pass through the network of a single batch and record the loss\nContinue to do this until at some point the loss ‚Äúexplodes‚Äù: for example because the current loss is 3 times as large as the minimum loss recorded so far\n\nAfter this, we plot the learning rate vs the recorded losses and look for a learning rate at which the loss is decreasing the most (i.e.¬†the point where the loss has the smallest derivative).\n\n #| export\nclass LRFindS(Subscriber):\n    \n    def __init__(self, mult=1.25):\n        self.mult = mult\n        self.min = math.inf\n        \n    def before_epoch(self, learn):\n        if not learn.model.training: raise CancelFitException\n        self.losses = []\n        self.lrs = []\n    \n    def after_loss(self, learn):\n        lr = learn.opt.param_groups[0]['lr']\n        self.lrs.append(lr)\n        loss = learn.loss.detach().cpu()\n        self.losses.append(loss)\n        if loss &lt; self.min: self.min = loss\n        if loss &gt; self.min*3: raise CancelFitException()\n        for g in learn.opt.param_groups: g['lr'] = lr * self.mult\n        \n    def plot(self):\n        plt.plot(self.lrs, self.losses)\n        plt.xscale('log')\n\n\nl = Learner(get_model(), dls, F.cross_entropy, torch.optim.SGD, lr, [metrics_s, device_s])\n\nlrfind_s = LRFindS()\nl.fit(5, lr=1e-4, subs=[lrfind_s])\n\n\nlrfind_s.plot()\n\n\n\n\nFrom which we see that a learning rate of around 0.2 would be best"
  },
  {
    "objectID": "posts/10_nntrain_learner/index.html#momentumlearner",
    "href": "posts/10_nntrain_learner/index.html#momentumlearner",
    "title": "nntrain (2/n): Learner",
    "section": "MomentumLearner",
    "text": "MomentumLearner\nAdditionally, we can easily subclass Learner, and implement custom functionality into any of its 5 main functionalities:\n\nprediction: self.predict()\nloss: self.get_loss()\nbackward pass: self.backward()\nstepping of weights: self.step()\nzeroing of gradients: self.zero_grad()\n\nFor example we can create a MomentumLearner which doesn‚Äôt just use the gradient of the last backward pass, but uses an exponentially weighted average of all previously computed gradients. We can do this by not zeroing out the gradients, but just reduce them by a factor between 0 and 1 (the momentum parameter). This way the ‚Äúgradient with momentum‚Äù at time \\(t\\) (\\(m_t\\)), will be a function of the normal gradient (\\(g_t\\)):\n\\[\nm_t = g_t + c \\cdot g_{t-1} + c^2 \\cdot g_{t-2} + ...\n\\]\nThis is called momentum and the idea is to add a sense of ‚Äúinertia‚Äù to the gradients, i.e.¬†if in one step we are moving through the loss manifold in a certain direction, then in the next step we want to keep moving somewhat in that direction irrespective of the gradient of the current step.\n\n #| export\nclass MomentumLearner(Learner):\n    \n    def __init__(self, model, dls, loss_fn, optim_class, lr, subs, mom=0.85):\n        self.mom = mom\n        super().__init__(model, dls, loss_fn, optim_class, lr, subs)\n        \n    def zero_grad(self):\n        with torch.no_grad():\n            for p in self.model.parameters(): p.grad *= self.mom\n\n\nl = MomentumLearner(get_model(), dls, F.cross_entropy, torch.optim.SGD, lr, [metrics_s, device_s])\nl.fit(5)\n\n{'epoch': 0, 'mode': 'train', 'loss': '1.657', 'accuracy': '0.481'}\n{'epoch': 0, 'mode': 'eval', 'loss': '1.105', 'accuracy': '0.647'}\n{'epoch': 1, 'mode': 'train', 'loss': '0.925', 'accuracy': '0.685'}\n{'epoch': 1, 'mode': 'eval', 'loss': '0.831', 'accuracy': '0.695'}\n{'epoch': 2, 'mode': 'train', 'loss': '0.763', 'accuracy': '0.732'}\n{'epoch': 2, 'mode': 'eval', 'loss': '0.736', 'accuracy': '0.739'}\n{'epoch': 3, 'mode': 'train', 'loss': '0.686', 'accuracy': '0.764'}\n{'epoch': 3, 'mode': 'eval', 'loss': '0.675', 'accuracy': '0.766'}\n{'epoch': 4, 'mode': 'train', 'loss': '0.635', 'accuracy': '0.784'}\n{'epoch': 4, 'mode': 'eval', 'loss': '0.634', 'accuracy': '0.779'}\n\n\nAnd this simple technique has a pretty good effect on training our model: the accuracy on the validation set is increasing (even with this simple linear model) from 66% with normal SGD to 78% with momentum."
  },
  {
    "objectID": "posts/10_nntrain_learner/index.html#closing-remarks",
    "href": "posts/10_nntrain_learner/index.html#closing-remarks",
    "title": "nntrain (2/n): Learner",
    "section": "Closing remarks",
    "text": "Closing remarks\nIn this post we have again covered a lot of ground. We have created a very flexible Learner framework, making heavy use of a pubsub system to customize the training loop. As examples we have seen a Subscriber that enables training on the GPU, and another one that takes care of tracking the loss and the metrics while we are training. Additionally we have implement the learning rate finder as a Subscriber, and last but not least we have seen how we can subclass the Learner class to create custom learners that for example implement momentum. Below I have added one additional Subscriber that displays the progress of the loss a bit nicer in a graph, as well as puts the outputs in a table. It has been copied from the miniai library as is (with some minor changes to make it work for nntrain).\n\n #| export\n\nclass ProgressS(Subscriber):\n    order = MetricsS.order+1\n    def __init__(self, plot=False): self.plot = plot\n    def before_fit(self, learn):\n        learn.epochs = self.mbar = master_bar(learn.epochs)\n        self.first = True\n        if hasattr(learn, 'metrics'): learn.metrics.output = self.output\n        self.losses = []\n        self.val_losses = []\n\n    def output(self, d):\n        if self.first:\n            self.mbar.write(list(d), table=True)\n            self.first = False\n        self.mbar.write(list(d.values()), table=True)\n\n    def before_epoch(self, learn): learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)\n    \n    def after_batch(self, learn):\n        learn.dl.comment = f'{learn.loss:.3f}'\n        if self.plot and hasattr(learn, 'metrics') and learn.model.training:\n            self.losses.append(learn.loss.item())\n            if self.val_losses: self.mbar.update_graph([[fc.L.range(self.losses), self.losses],[fc.L.range(learn.epoch).map(lambda x: (x+1)*len(learn.dls.train)), self.val_losses]])\n    \n    def after_epoch(self, learn): \n        if not learn.model.training:\n            if self.plot and hasattr(learn, 'metrics'): \n                self.val_losses.append(learn.metrics.loss.compute())\n                self.mbar.update_graph([[fc.L.range(self.losses), self.losses],[fc.L.range(learn.epoch+1).map(lambda x: (x+1)*len(learn.dls.train)), self.val_losses]])\n\n\nprogress_s = ProgressS(True)\n\nl = MomentumLearner(get_model(), dls, F.cross_entropy, torch.optim.SGD, lr, [metrics_s, device_s, progress_s])\nl.fit(5)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\naccuracy\n\n\n\n\n0\ntrain\n1.818\n0.425\n\n\n0\neval\n1.192\n0.640\n\n\n1\ntrain\n0.965\n0.676\n\n\n1\neval\n0.845\n0.693\n\n\n2\ntrain\n0.772\n0.730\n\n\n2\neval\n0.740\n0.737\n\n\n3\ntrain\n0.690\n0.762\n\n\n3\neval\n0.676\n0.765\n\n\n4\ntrain\n0.636\n0.784\n\n\n4\neval\n0.634\n0.780"
  },
  {
    "objectID": "posts/04_matmul/index.html",
    "href": "posts/04_matmul/index.html",
    "title": "Fast matrix multiplications",
    "section": "",
    "text": "Matrix multiplications are kind of boring, so why write a blog post about them? Well, matrix multiplications are the most basic computation that is being performed by neural networks. So it‚Äôs probably good to be familiar with them, although we never do them by hand. Also, we are going to focus on speeding them up by doing vectorization. Vectorization is something we often have to do, to make sure everything runs as quickly as possible, and it‚Äôs thus a good exercise to understand how to achieve this. Especially since it involves being very familiar with matrices, their shapes, broadcasting operations and the like.\nThis post follows the first lecture of Part 2 of the FastAI course (2019), I will provide some additional explanations, and present one other optimization that is not presented in the lecture."
  },
  {
    "objectID": "posts/04_matmul/index.html#definition",
    "href": "posts/04_matmul/index.html#definition",
    "title": "Fast matrix multiplications",
    "section": "Definition",
    "text": "Definition\nMatrix multiplication is not difficult, it goes like this:\n\nFor matrix A of size [ar x ac] ¬† ([4 x 3] in the image below)\nand matrix B of size [br x bc] ¬† ([3 x 2] in the image below)\nthe matrix product A * B is of size [ar x bc] ([4 x 2] in the image below).\nSo the matrix product is thus only defined when ac == br (3 == 3 in the image below)\n\n\n\n\n\n\nSo for any valid matrix multiplication, we have three dimensions that need to considered:\n\nar: the row dimension of matrix A. The size of this dimension will become the size of the row dimension of the output matrix (black arrow in the image above)\nbc: the column dimension of matrix B. The size of this dimension will become the size of the column dimension of the output matrix (purple arrow in the image above)\nac: the column dimension of Matrix A and br: the row dimension of matrix B: they need to be equal (red arrow in the image above)\n\nWhy do ac and bc need to be equal? Well, because we take the inner product over this dimension when computing the cell values of the new matrix, and inner-products are only defined for vectors of equal length. Below, I will also refer to this dimension as the dimension over which we collapse (or the ‚Äúcollapsible‚Äù dimension), since in the output matrix, this dimension is no longer present.\n\n\n\n\n\nIn other words, to compute cell \\(C_{i,j}\\) we take the inner product between row i of matrix A and column j of matrix B. Let‚Äôs have a look at one other cell, to make sure we understand fully what‚Äôs going on. In the next figure we compute the value for cell \\(C_{3,2}\\), we thus take the inner-product between row 3 of matrix A and column 2 of matrix B:\n\n\n\n\n\nLet‚Äôs do this in code, to confirm these statements:\n\nimport torch\n\na = torch.randn(4,3)\nb = torch.randn(3,2)\n\n\n# Confirm the shape of the output matrix\n(a@b).shape\n\ntorch.Size([4, 2])\n\n\n\n# Confirm the value of one output cell (C00)\nC00_manual = (a[0,0] * b[0,0]) + (a[0,1] * b[1,0]) + (a[0,2] * b[2,0])\nC00_auto = (a@b)[0,0]\n\nassert torch.allclose(C00_manual, C00_auto)\n\nNow, let‚Äôs create our own matrix multiplication function:\n\ndef matmul(a, b):\n    # fill in the sizes of the dimensions\n    ar, ac = a.shape\n    br, bc = b.shape\n\n    # assert that our matrices can be multiplied \n    assert ac == br\n\n    # create an output tensor of the expected size (ar x bc)\n    out = torch.zeros(ar, bc)\n\n    # iterate over the rows of the output matrix (--&gt; length ar)\n    for i in range(out.shape[0]):\n        # iterate over the columns of the output matrix (--&gt; length bc)\n        for j in range(out.shape[1]):\n            # iterate over the \"collapsed\" dimension (--&gt; length ac and length br), \n            for k in range(ac):\n                out[i, j] += a[i, k] * b[k, j]\n    \n    return out\n\n# Confirm that the result is correct\nassert torch.allclose(matmul(a,b), a@b)\n\nAlthough this is functionally correct, it‚Äôs not very efficient. In fact, to compute the value of one cell of the output matrix, we are doing three separate multiplications. In other words, for each cell out[i,j] we are calling three times (once for every value of k):\nout[i, j] += a[i, k] * b[k, j]\nLet‚Äôs try to reduce the computation of one cell to just one single call."
  },
  {
    "objectID": "posts/04_matmul/index.html#first-improvement",
    "href": "posts/04_matmul/index.html#first-improvement",
    "title": "Fast matrix multiplications",
    "section": "First improvement",
    "text": "First improvement\nTo do so, we need to get rid of the loop over the ‚Äúcollapsible‚Äù dimension k. We can simply do this by replacing the k with a :, so that we select the whole dimension instead of just one element in that dimension. The multiplication (*) is doing an element wise multiplication, so we have to wrap the result with a .sum().\n\ndef matmul2(a, b):\n    ar, ac = a.shape\n    br, bc = b.shape\n    assert ac == br\n\n    out = torch.zeros(ar,bc)\n    # iterate over the rows of the output matrix (i)\n    for i in range(out.shape[0]):\n        # iterate over the columns of the output matrix (j)\n        for j in range(out.shape[1]):\n            out[i, j] = (a[i, :] * b[:, j]).sum()\n    \n    return out\n\n# Confirm that the result is correct\nassert torch.allclose(matmul(a,b), a@b)"
  },
  {
    "objectID": "posts/04_matmul/index.html#second-improvement",
    "href": "posts/04_matmul/index.html#second-improvement",
    "title": "Fast matrix multiplications",
    "section": "Second improvement",
    "text": "Second improvement\nThe improvement above, gives us the value of a cell in one single call:\nout[i, j] = (a[i, :] * b[:, j]).sum()\nThis is great, let‚Äôs try to vectorize this even further, and get rid of the second loop (the loop over j), this means that we need to compute the values of a single row of the output matrix in one call, e.g.\nout[i,:] = ...\nWe know that the value of cell \\(C_{ij}\\) is the inner product between row i of A and column j of B. We also know that any row of matrix C will have two values. Let‚Äôs compute them manually:\n\nout_00 = (a[0,:] * b[:,0]).sum()\nout_01 = (a[0,:] * b[:,1]).sum()\n\nC0_manual = torch.stack([out_00, out_01])\nC0_auto = (a@b)[0]\n\nassert torch.allclose(C0_manual, C0_auto)\n\nprint(f'{out_00=}', f'{out_01=}', f'{C0_manual=}', sep='\\n')\n\nout_00=tensor(-0.0213)\nout_01=tensor(0.3668)\nC0_manual=tensor([-0.0213,  0.3668])\n\n\nObserve that for the computation of one row of output, we need:\n\none single row of A (a[0,:])\nthe full matrix of B, we need both the first (b[:,0]) column and the second column (b[:,1]).\n\nLet‚Äôs check the sizes of both and see whether we can use broadcasting:\n\nprint(f'{a[0,:].shape=}', f'{b.shape=}', sep='\\n')\n\na[0,:].shape=torch.Size([3])\nb.shape=torch.Size([3, 2])\n\n\nUnfortunately, size [3] and [3,2] don‚Äôt broadcast. To make them broadcast, we have to add an empty dimension at the end of the row of the A matrix. Then, the shapes [3, 1] and [3, 2] can be broadcasted to another by duplicating the former in the column direction:\n\nt = a[0,:].unsqueeze(-1) # [3, 1]\n\nt.broadcast_to(b.shape) # [3, 2]\n\ntensor([[ 0.9193,  0.9193],\n        [-0.0426, -0.0426],\n        [ 1.3566,  1.3566]])\n\n\nNow that both object are the same size we can do an element-wise multiplication and then sum over the rows to arrive at an output of size [1,2]:\n\nC0_manual = (t*b).sum(dim=0)\nC0_auto = (a@b)[0,:]\n\nassert torch.allclose(C0_manual, C0_auto)\n\nprint(f'{C0_manual=}', f'{C0_manual.shape=}', sep='\\n')\n\nC0_manual=tensor([-0.0213,  0.3668])\nC0_manual.shape=torch.Size([2])\n\n\nSo let‚Äôs implement this:\n\ndef matmul3(a, b):\n    ar, ac = a.shape\n    br, bc = b.shape\n    assert ac == br\n\n    out = torch.zeros(ar,bc)\n    # iterate over the rows of the output matrix (i)\n    for i in range(out.shape[0]):\n        out[i, :] = (a[i, :].unsqueeze(-1) * b).sum(dim=0)\n    \n    return out\n\n# Confirm that the result is correct\nassert torch.allclose(matmul(a,b), a@b)"
  },
  {
    "objectID": "posts/04_matmul/index.html#third-improvement",
    "href": "posts/04_matmul/index.html#third-improvement",
    "title": "Fast matrix multiplications",
    "section": "Third improvement",
    "text": "Third improvement\nFor the final improvement, we need to get rid of the only remaining loop over the rows of our output matrix (i). So let‚Äôs understand very well what we are having at the moment:\n\nWe are iterating over the (4) rows of our output matrix\nFor each row, we are computing the (2) values of our row at once by doing out[i, :] = (a[i, :].unsqueeze(-1) * b).sum(dim=0) and let‚Äôs break this down once again in steps:\n\na[i, :] has shape [3] and represents one row of A\nwith a[i, :].unsqueeze(-1) we add an extra dimension so that we can broadcast, the result has shape [3, 1]\nb has shape [3, 2] and is the full B matrix\nelement-wise multiplication of 2. and 3. gives a matrix of shape [3, 2]\nby summing over the rows (.sum(dim=0)) we arrive at the result of shape [2]\n\n\nWe want to improve this by instead of iterating over the 4 rows, do these computations all at once for all rows. So let‚Äôs start by not selecting one row of A (a[i,:]) but instead just the whole a matrix:\n\na has shape [4, 3]\nsimilarly to what we did before, we can a.unsqueeze(-1) to add an extra dimension, the result has shape [4, 3, 1]\nsame as before, b has shape [3, 2] and is the full B matrix\nbroadcasting of 2. and 3. will do the following:\n\na.unsqueeze(-1) has shape [4, 3, 1] and get‚Äôs expanded to [4, 3, 2] to match the shape of b ([3, 2])\nbut b also needs to match a, first an additional empty dimension is added in the front: [1, 3, 2] and then it get‚Äôs expanded to [4, 3, 2]\nnext, the element-wise multiplication of 2. and 3. gives a matrix (tensor) of shape [4, 3, 2], let‚Äôs call it t. It‚Äôs import to realize what this t represents. For that, notice that the first dimension (length 4) and last dimension (length 2) are the dimensions of our output matrix ([4, 2]). The middle dimension (length 3) represents the element wise multiplications of any row in matrix A and any column of matrix B. So by doing for example t[i, :, j].sum() we get the value for cell \\(C_{i,j}\\) of our output matrix!\n\nThis means, that to arrive at the final result, we will have to collapse (sum) over the middle dimension!\n\n\ndef matmul4(a, b):\n    return (a.unsqueeze(-1) * b).sum(dim=1)\n\n# Confirm that the result is correct\nassert torch.allclose(matmul(a,b), a@b)"
  },
  {
    "objectID": "posts/04_matmul/index.html#timings",
    "href": "posts/04_matmul/index.html#timings",
    "title": "Fast matrix multiplications",
    "section": "Timings",
    "text": "Timings\nTo see what kind of a speed-up we have achieved, let‚Äôs look at the timings of our first version with three loops and the timings of our optimized version:\n\n%timeit -n 1000 matmul(a,b)\n\n318 ¬µs ¬± 13.7 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n%timeit -n 1000 matmul4(a,b)\n\n10.7 ¬µs ¬± 1.46 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n\n\nNice, our optimized version is about 30 times faster then our un-optimized version with 3 loops! Additionally, let‚Äôs check the timings of doing the matrix multiplication with einsum:\n\n%timeit -n 1000 torch.einsum('ij,jk-&gt;ik', a, b)\n\n25.9 ¬µs ¬± 3.78 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n\n\nSurprisingly, our optimized version is twice as fast as einsum. This is certainly something I didn‚Äôt expect.\nFinally, let‚Äôs also check the timings of using the @ operator:\n\n%timeit -n 1000 a@b\n\n3.29 ¬µs ¬± 522 ns per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n\n\nAs expected, this is even faster then our optimized version, probably because it runs in optimized C / CUDA code"
  }
]