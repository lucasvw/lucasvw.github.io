[
  {
    "objectID": "posts/05_crossentropy/index.html",
    "href": "posts/05_crossentropy/index.html",
    "title": "Cross entropy any which way",
    "section": "",
    "text": "Cross entropy is one of the most commonly used loss functions. In this post, we will have a look at how it works, and compute it in a couple of different ways.\nConsider a network that is build for image classification. During the forward pass, images are passed into the network and the network processes the data layer by layer, until evenually some final activations are being returned by the model. These final activations are called “logits” and represent the unnormalized predictions of our model.\nSince we generally use mini-batches during training, these logits are of shape [bs, num_classes]\nimport torch\nimport torch.nn.functional as F\n\ng = torch.manual_seed(42) # use a generator for reproducability\n\nbs = 32 # batch size of 32\nnum_classes = 3 # image classification with 3 different classes\n\nlogits = torch.randn(size=(bs, num_classes), generator=g) # size: [32,3]\n\nlogits[0:4] # show the logits for the first couple of samples\n\ntensor([[ 1.9269,  1.4873,  0.9007],\n        [-2.1055,  0.6784, -1.2345],\n        [-0.0431, -1.6047, -0.7521],\n        [ 1.6487, -0.3925, -1.4036]])\nEach row of this tensor represents the unnormalized predictions for each of our samples in the batch. We can normalize these predictions by applying a softmax. The softmax function does two things:\nThis makes sure that we can treat the output of this as probabilities, because:\nSpecifically:\n# Unnormalized predictions for our first sample (3 classes)\nlogits[0]\n\ntensor([1.9269, 1.4873, 0.9007])\n# Exponentiated predictions, making them all positive\nexp_logits = logits[0].exp()\nexp_logits\n\ntensor([6.8683, 4.4251, 2.4614])\n# Turn these values into probabilities by dividing by the sum\nprobs = exp_logits / exp_logits.sum()\n\n# verify that the sum of the probabilities sum to 1\nassert torch.allclose(probs.sum(), torch.tensor(1.))\n\nprobs\n\ntensor([0.4993, 0.3217, 0.1789])\nSo, let’s create a softmax function that does this for a whole batch:\ndef softmax(logits):\n    exp_logits = logits.exp() # shape: [32, 3]\n    exp_logits_sum = exp_logits.sum(dim=1, keepdim=True) # shape: [32, 1]\n    \n    # Note: this get's correctly broadcasted, since the exp_logits_sum will \n    # expand to [32, 3], so each value in exp_logits gets divided by the sum over its row\n    probs = exp_logits / exp_logits_sum # shape: [32, 3]\n    \n    return probs \n\nprobs = softmax(logits)\nprobs[0:4]\n\ntensor([[0.4993, 0.3217, 0.1789],\n        [0.0511, 0.8268, 0.1221],\n        [0.5876, 0.1233, 0.2891],\n        [0.8495, 0.1103, 0.0401]])\nNext, we want to compute the loss for which also need our labels. These labels represent the ground truth class for each of our samples in the batch. Since we have 3 classes they will be between 0 and 3 (e.g. either 0, 1 or 2)\ng = torch.manual_seed(42) # use a generator for reproducability\n\nlabels = torch.randint(low=0, high=3, size=(32,), generator=g)\nlabels\n\ntensor([0, 2, 1, 1, 0, 2, 1, 2, 1, 2, 1, 1, 2, 0, 0, 1, 2, 1, 0, 1, 1, 2, 1, 2,\n        2, 1, 2, 0, 1, 1, 0, 0])\nFor classification we use the Negative Log Likelihood loss function, which is defined as such:\n\\[\n\\textrm{NLL} = - \\sum_{i}{q_i * \\log(p_i)}\n\\]\nwith \\(i\\) being the index that moves along the classes (3 in our example) and \\(q_i\\) being the probability that the ground truth label is class \\(i\\) (this is a somewhat strange formulation, since this probability is either 1 (for the correct class) or 0 (for all the non-correct classes)). Finally, \\(p_i\\) is the probability that the model associated to class \\(i\\).\nFor the very first row of our probs ([0.4993, 0.3217, 0.1789]) and our first label (0) we thus get:\n\\[\\begin{align}\n\\textrm{NLL} &= - ( (1 \\cdot \\log(0.4993)) + (0 \\cdot \\log(0.3217)) + (0 \\cdot \\log(0.1789)) ) \\\\\n\\textrm{NLL} &= - ( (1 \\cdot \\log(0.4993)) ) \\\\\n\\textrm{NLL} &= - \\log(0.4993)\n\\end{align}\\]\nFrom which we see that it’s just the negative log of the probability associated with the ground truth class.\nSince this computes only the NLL per sample, we also need a way to combine the NLL across the samples in our batch. We can do this either by summing or averaging, averaging has the advantage that the size of the loss remains the same when we change the batch-size, so let’s use that:\ndef nll(probs, labels):\n    # probs: shape [32, 3]\n    # labels: shape [32]\n    \n    # this plucks out the probability of the ground truth label per sample, \n    # it uses \"numpy's integer array indexing\":\n    # https://numpy.org/doc/stable/user/basics.indexing.html#integer-array-indexing\n    probs_ground_truth_class = probs[range(len(labels)), labels] # shape: [32]\n    \n    nll = -torch.log(probs_ground_truth_class).mean() # shape: []\n    return nll\nnll(probs, labels)\n\ntensor(1.3465)"
  },
  {
    "objectID": "posts/05_crossentropy/index.html#using-pytorch",
    "href": "posts/05_crossentropy/index.html#using-pytorch",
    "title": "Cross entropy any which way",
    "section": "Using PyTorch",
    "text": "Using PyTorch\nInstead of using our custom softmax, we can also use the build-in softmax function from PyTorch:\n\np = F.softmax(logits, dim=1) # dim=1 --&gt; compute the sum across the columns\nnll(p, labels)\n\ntensor(1.3465)\n\n\nInstead of using our custom nll we can also use the build-in version from PyTorch. However, nll_loss expects the log of the softmax (for numerical stability) so instead of softmax we have to use log_softmax:\n\np = F.log_softmax(logits, dim=1)\n\n# Assert that indeed the log_softmax is just the softmax followed by a log\nassert torch.allclose(p, F.softmax(logits, dim=1).log())\n\ntorch.nn.functional.nll_loss(p, labels)\n\ntensor(1.3465)\n\n\nThe combination of softmax and nll is called cross entropy, so we can also use PyTorch’s build-in version of that:\n\nF.cross_entropy(logits, labels)\n\ntensor(1.3465)\n\n\nInstead of the methods in nn.functional, we can also use classes. For that, we first create an instance of the object, and then “call” the instance:\n\nce = torch.nn.CrossEntropyLoss() # create a CrossEntropyLoss instance\nce(logits, labels) # calling the instance with the arguments returns the cross entropy\n\ntensor(1.3465)\n\n\nSimilarly, we can use classes for the log_softmax and nll_loss functions\n\nls = torch.nn.LogSoftmax(dim=1)\nnll = torch.nn.NLLLoss()\n\np = ls(logits)\nnll(p, labels)\n\ntensor(1.3465)\n\n\nThis is practical, if we want specify custom behavior of the loss function ahead of time of calling the actual loss function. For example, let’s say we want to compute the cross entropy loss based on ‘sums’ instead of ‘averages’. Then when using the method in F we would do:\n\nF.cross_entropy(logits, labels, reduction='sum')\n\ntensor(43.0866)\n\n\nSo whenever we call the loss, we have to specify the additional reduction argument.\nWhereas when using the loss classes, we can instantiate the class with that reduction argument, and then call the instance as per usual without passing anything but the logits and the labels:\n\n# instantiate \nce = torch.nn.CrossEntropyLoss(reduction='sum')\n\n# at some other point in your code, compute the loss as per default\nce(logits, labels)\n\ntensor(43.0866)\n\n\nThis is practical when the loss function is getting called by another object to which we don’t have easy access. So that we can’t easily change the arguments for that call. This is for example the case when using the FastAI Learner class, to which we pass the loss function which then get’s called by the Learner object with the default arguments (logits and labels). By using the classes, we can specify the reduction argument ahead of time and pass that instance to the Learner class."
  },
  {
    "objectID": "posts/04_matmul/index.html",
    "href": "posts/04_matmul/index.html",
    "title": "Fast matrix multiplications",
    "section": "",
    "text": "Matrix multiplications are kind of boring, so why write a blog post about them? Well, matrix multiplications are the most basic computation that is being performed by neural networks. So it’s probably good to be familiar with them, although we never do them by hand. Also, we are going to focus on speeding them up by doing vectorization. Vectorization is something we often have to do, to make sure everything runs as quickly as possible, and it’s thus a good exercise to understand how to achieve this. Especially since it involves being very familiar with matrices, their shapes, broadcasting operations and the like.\nThis post follows the first lecture of Part 2 of the FastAI course (2019), I will provide some additional explanations, and present one other optimization that is not presented in the lecture."
  },
  {
    "objectID": "posts/04_matmul/index.html#definition",
    "href": "posts/04_matmul/index.html#definition",
    "title": "Fast matrix multiplications",
    "section": "Definition",
    "text": "Definition\nMatrix multiplication is not difficult, it goes like this:\n\nFor matrix A of size [ar x ac]   ([4 x 3] in the image below)\nand matrix B of size [br x bc]   ([3 x 2] in the image below)\nthe matrix product A * B is of size [ar x bc] ([4 x 2] in the image below).\nSo the matrix product is thus only defined when ac == br (3 == 3 in the image below)\n\n\n\n\n\n\nSo for any valid matrix multiplication, we have three dimensions that need to considered:\n\nar: the row dimension of matrix A. The size of this dimension will become the size of the row dimension of the output matrix (black arrow in the image above)\nbc: the column dimension of matrix B. The size of this dimension will become the size of the column dimension of the output matrix (purple arrow in the image above)\nac: the column dimension of Matrix A and br: the row dimension of matrix B: they need to be equal (red arrow in the image above)\n\nWhy do ac and bc need to be equal? Well, because we take the inner product over this dimension when computing the cell values of the new matrix, and inner-products are only defined for vectors of equal length. Below, I will also refer to this dimension as the dimension over which we collapse (or the “collapsible” dimension), since in the output matrix, this dimension is no longer present.\n\n\n\n\n\nIn other words, to compute cell \\(C_{i,j}\\) we take the inner product between row i of matrix A and column j of matrix B. Let’s have a look at one other cell, to make sure we understand fully what’s going on. In the next figure we compute the value for cell \\(C_{3,2}\\), we thus take the inner-product between row 3 of matrix A and column 2 of matrix B:\n\n\n\n\n\nLet’s do this in code, to confirm these statements:\n\nimport torch\n\na = torch.randn(4,3)\nb = torch.randn(3,2)\n\n\n# Confirm the shape of the output matrix\n(a@b).shape\n\ntorch.Size([4, 2])\n\n\n\n# Confirm the value of one output cell (C00)\nC00_manual = (a[0,0] * b[0,0]) + (a[0,1] * b[1,0]) + (a[0,2] * b[2,0])\nC00_auto = (a@b)[0,0]\n\nassert torch.allclose(C00_manual, C00_auto)\n\nNow, let’s create our own matrix multiplication function:\n\ndef matmul(a, b):\n    # fill in the sizes of the dimensions\n    ar, ac = a.shape\n    br, bc = b.shape\n\n    # assert that our matrices can be multiplied \n    assert ac == br\n\n    # create an output tensor of the expected size (ar x bc)\n    out = torch.zeros(ar, bc)\n\n    # iterate over the rows of the output matrix (--&gt; length ar)\n    for i in range(out.shape[0]):\n        # iterate over the columns of the output matrix (--&gt; length bc)\n        for j in range(out.shape[1]):\n            # iterate over the \"collapsed\" dimension (--&gt; length ac and length br), \n            for k in range(ac):\n                out[i, j] += a[i, k] * b[k, j]\n    \n    return out\n\n# Confirm that the result is correct\nassert torch.allclose(matmul(a,b), a@b)\n\nAlthough this is functionally correct, it’s not very efficient. In fact, to compute the value of one cell of the output matrix, we are doing three separate multiplications. In other words, for each cell out[i,j] we are calling three times (once for every value of k):\nout[i, j] += a[i, k] * b[k, j]\nLet’s try to reduce the computation of one cell to just one single call."
  },
  {
    "objectID": "posts/04_matmul/index.html#first-improvement",
    "href": "posts/04_matmul/index.html#first-improvement",
    "title": "Fast matrix multiplications",
    "section": "First improvement",
    "text": "First improvement\nTo do so, we need to get rid of the loop over the “collapsible” dimension k. We can simply do this by replacing the k with a :, so that we select the whole dimension instead of just one element in that dimension. The multiplication (*) is doing an element wise multiplication, so we have to wrap the result with a .sum().\n\ndef matmul2(a, b):\n    ar, ac = a.shape\n    br, bc = b.shape\n    assert ac == br\n\n    out = torch.zeros(ar,bc)\n    # iterate over the rows of the output matrix (i)\n    for i in range(out.shape[0]):\n        # iterate over the columns of the output matrix (j)\n        for j in range(out.shape[1]):\n            out[i, j] = (a[i, :] * b[:, j]).sum()\n    \n    return out\n\n# Confirm that the result is correct\nassert torch.allclose(matmul(a,b), a@b)"
  },
  {
    "objectID": "posts/04_matmul/index.html#second-improvement",
    "href": "posts/04_matmul/index.html#second-improvement",
    "title": "Fast matrix multiplications",
    "section": "Second improvement",
    "text": "Second improvement\nThe improvement above, gives us the value of a cell in one single call:\nout[i, j] = (a[i, :] * b[:, j]).sum()\nThis is great, let’s try to vectorize this even further, and get rid of the second loop (the loop over j), this means that we need to compute the values of a single row of the output matrix in one call, e.g.\nout[i,:] = ...\nWe know that the value of cell \\(C_{ij}\\) is the inner product between row i of A and column j of B. We also know that any row of matrix C will have two values. Let’s compute them manually:\n\nout_00 = (a[0,:] * b[:,0]).sum()\nout_01 = (a[0,:] * b[:,1]).sum()\n\nC0_manual = torch.stack([out_00, out_01])\nC0_auto = (a@b)[0]\n\nassert torch.allclose(C0_manual, C0_auto)\n\nprint(f'{out_00=}', f'{out_01=}', f'{C0_manual=}', sep='\\n')\n\nout_00=tensor(-0.0213)\nout_01=tensor(0.3668)\nC0_manual=tensor([-0.0213,  0.3668])\n\n\nObserve that for the computation of one row of output, we need:\n\none single row of A (a[0,:])\nthe full matrix of B, we need both the first (b[:,0]) column and the second column (b[:,1]).\n\nLet’s check the sizes of both and see whether we can use broadcasting:\n\nprint(f'{a[0,:].shape=}', f'{b.shape=}', sep='\\n')\n\na[0,:].shape=torch.Size([3])\nb.shape=torch.Size([3, 2])\n\n\nUnfortunately, size [3] and [3,2] don’t broadcast. To make them broadcast, we have to add an empty dimension at the end of the row of the A matrix. Then, the shapes [3, 1] and [3, 2] can be broadcasted to another by duplicating the former in the column direction:\n\nt = a[0,:].unsqueeze(-1) # [3, 1]\n\nt.broadcast_to(b.shape) # [3, 2]\n\ntensor([[ 0.9193,  0.9193],\n        [-0.0426, -0.0426],\n        [ 1.3566,  1.3566]])\n\n\nNow that both object are the same size we can do an element-wise multiplication and then sum over the rows to arrive at an output of size [1,2]:\n\nC0_manual = (t*b).sum(dim=0)\nC0_auto = (a@b)[0,:]\n\nassert torch.allclose(C0_manual, C0_auto)\n\nprint(f'{C0_manual=}', f'{C0_manual.shape=}', sep='\\n')\n\nC0_manual=tensor([-0.0213,  0.3668])\nC0_manual.shape=torch.Size([2])\n\n\nSo let’s implement this:\n\ndef matmul3(a, b):\n    ar, ac = a.shape\n    br, bc = b.shape\n    assert ac == br\n\n    out = torch.zeros(ar,bc)\n    # iterate over the rows of the output matrix (i)\n    for i in range(out.shape[0]):\n        out[i, :] = (a[i, :].unsqueeze(-1) * b).sum(dim=0)\n    \n    return out\n\n# Confirm that the result is correct\nassert torch.allclose(matmul(a,b), a@b)"
  },
  {
    "objectID": "posts/04_matmul/index.html#third-improvement",
    "href": "posts/04_matmul/index.html#third-improvement",
    "title": "Fast matrix multiplications",
    "section": "Third improvement",
    "text": "Third improvement\nFor the final improvement, we need to get rid of the only remaining loop over the rows of our output matrix (i). So let’s understand very well what we are having at the moment:\n\nWe are iterating over the (4) rows of our output matrix\nFor each row, we are computing the (2) values of our row at once by doing out[i, :] = (a[i, :].unsqueeze(-1) * b).sum(dim=0) and let’s break this down once again in steps:\n\na[i, :] has shape [3] and represents one row of A\nwith a[i, :].unsqueeze(-1) we add an extra dimension so that we can broadcast, the result has shape [3, 1]\nb has shape [3, 2] and is the full B matrix\nelement-wise multiplication of 2. and 3. gives a matrix of shape [3, 2]\nby summing over the rows (.sum(dim=0)) we arrive at the result of shape [2]\n\n\nWe want to improve this by instead of iterating over the 4 rows, do these computations all at once for all rows. So let’s start by not selecting one row of A (a[i,:]) but instead just the whole a matrix:\n\na has shape [4, 3]\nsimilarly to what we did before, we can a.unsqueeze(-1) to add an extra dimension, the result has shape [4, 3, 1]\nsame as before, b has shape [3, 2] and is the full B matrix\nbroadcasting of 2. and 3. will do the following:\n\na.unsqueeze(-1) has shape [4, 3, 1] and get’s expanded to [4, 3, 2] to match the shape of b ([3, 2])\nbut b also needs to match a, first an additional empty dimension is added in the front: [1, 3, 2] and then it get’s expanded to [4, 3, 2]\nnext, the element-wise multiplication of 2. and 3. gives a matrix (tensor) of shape [4, 3, 2], let’s call it t. It’s import to realize what this t represents. For that, notice that the first dimension (length 4) and last dimension (length 2) are the dimensions of our output matrix ([4, 2]). The middle dimension (length 3) represents the element wise multiplications of any row in matrix A and any column of matrix B. So by doing for example t[i, :, j].sum() we get the value for cell \\(C_{i,j}\\) of our output matrix!\n\nThis means, that to arrive at the final result, we will have to collapse (sum) over the middle dimension!\n\n\ndef matmul4(a, b):\n    return (a.unsqueeze(-1) * b).sum(dim=1)\n\n# Confirm that the result is correct\nassert torch.allclose(matmul(a,b), a@b)"
  },
  {
    "objectID": "posts/04_matmul/index.html#timings",
    "href": "posts/04_matmul/index.html#timings",
    "title": "Fast matrix multiplications",
    "section": "Timings",
    "text": "Timings\nTo see what kind of a speed-up we have achieved, let’s look at the timings of our first version with three loops and the timings of our optimized version:\n\n%timeit -n 1000 matmul(a,b)\n\n318 µs ± 13.7 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n%timeit -n 1000 matmul4(a,b)\n\n10.7 µs ± 1.46 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\nNice, our optimized version is about 30 times faster then our un-optimized version with 3 loops! Additionally, let’s check the timings of doing the matrix multiplication with einsum:\n\n%timeit -n 1000 torch.einsum('ij,jk-&gt;ik', a, b)\n\n25.9 µs ± 3.78 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\nSurprisingly, our optimized version is twice as fast as einsum. This is certainly something I didn’t expect.\nFinally, let’s also check the timings of using the @ operator:\n\n%timeit -n 1000 a@b\n\n3.29 µs ± 522 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\nAs expected, this is even faster then our optimized version, probably because it runs in optimized C / CUDA code"
  },
  {
    "objectID": "posts/01_blog_setup/index.html",
    "href": "posts/01_blog_setup/index.html",
    "title": "Blog setup",
    "section": "",
    "text": "In this blog post I’ll explain how I created this blog, using Quarto and GitHub. In step 4 I’ll show how to setup GitHub Actions, this has advantages over the other ways to publish our blog:\nI’m working on a Macbook, and using VS Code for code editing. If you are on a Linux or Windows machine, be aware that things might be a bit different from what I describe here.\nI am assuming you already have a GitHub account, that VS Code is installed and configured to run Python and Jupyter Notebooks."
  },
  {
    "objectID": "posts/01_blog_setup/index.html#step-1-install-quarto",
    "href": "posts/01_blog_setup/index.html#step-1-install-quarto",
    "title": "Blog setup",
    "section": "Step 1: install Quarto",
    "text": "Step 1: install Quarto\nFirst of all you need to install Quarto, go here, download and install the software. You should do this on the machine that you want to use for writing your blog, in my case my Macbook laptop.\nOnce installed you will have access to the quarto Command Line Interface (CLI). To make sure everything works as expected, open a terminal and execute:\n\n\nTerminal\n\nquarto --help\n\nThis should render some outputs describing the different commands and options that are part of the Quarto CLI and shows that Quarto is installed successfully."
  },
  {
    "objectID": "posts/01_blog_setup/index.html#step-2-create-a-github-repo",
    "href": "posts/01_blog_setup/index.html#step-2-create-a-github-repo",
    "title": "Blog setup",
    "section": "Step 2: create a GitHub repo",
    "text": "Step 2: create a GitHub repo\nTo host our blog we will use GitHub Pages, which is a service to host a website from a GitHub repository. Based on the name you pick for your repository you will create a so-called project-website or your unique user-website. For any general repo named my-awesome-repo, the website will be hosted on https://&lt;github-username&gt;.github.io/my-awesome-repo. This is a project-websites and you can create as many as you like.\nTo create your user-website, you have to name the repo exactly like this: &lt;github-username&gt;.github.io, the user-website will be hosted at https://&lt;github-username&gt;.github.io.\nThis is exactly what I want, so I create a new repo with the name: lucasvw.github.io.\nI find it helpful to add a .gitignore file with a Python template, to which we can later add some more entries to facilitate storing the right files on GitHub. Also make sure that the repo is Public (and not set to Private). Additionally, I added a README file and choose the Apache2 License.\nNext, I clone this repo to my machine by running:\n\n\nTerminal\n\ngit clone git@github.com:lucasvw/lucasvw.github.io.git"
  },
  {
    "objectID": "posts/01_blog_setup/index.html#step-3-add-a-quarto-project-to-the-repo",
    "href": "posts/01_blog_setup/index.html#step-3-add-a-quarto-project-to-the-repo",
    "title": "Blog setup",
    "section": "Step 3: add a Quarto project to the repo",
    "text": "Step 3: add a Quarto project to the repo\nNext, open VS Code and open the cloned repo. Then access the VS Code terminal and run:\n\n\nTerminal\n\nquarto create-project --type website:blog\n\nThis will add a number of files to our repo, which represent the basic structure of our blog. Most importantly:\n\nposts: here we will create our blog entries (one subfolder per blog entry)\n_quarto.yml: configuration file for our blog such as the theme, name, GitHub and Twitter links\nabout.qmd: source code for the “about” page.\nindex.qmd: source code for the landing page.\n\n\n\n\n\n\n\nNote\n\n\n\n.qmd files are like markdown files, but with lots of additional functionality from Quarto. Go here for more information on Markdown syntax and here for Quarto Markdown\n\n\nTo see what we currently have, let’s render our blog locally:\n\n\nTerminal\n\nquarto preview\n\nAlternatively, we can install the Quarto extension in VS Code, which will show a render button in the top right corner on any opened qmd file.\nTo publish the current contents to GitHub pages, we can run:\n\n\nTerminal\n\nquarto publish gh-pages\n\nWhen doing so, we get a message that we have to change the branch from which GitHub Pages builds the site. To do this, I go to https://github.com/lucasvw/lucasvw.github.io/settings/pages and select gh-pages instead of the main branch.\nAnd voila, in a few moments our blog will be running live at https://lucasvw.github.io/"
  },
  {
    "objectID": "posts/01_blog_setup/index.html#step-4-finalize-set-up-github-actions",
    "href": "posts/01_blog_setup/index.html#step-4-finalize-set-up-github-actions",
    "title": "Blog setup",
    "section": "Step 4: Finalize set-up: GitHub Actions",
    "text": "Step 4: Finalize set-up: GitHub Actions\nWhen we run the quarto publish gh-pages command, Quarto processes our files and turns them into web readable files (HTML, JS, CSS etc). It stores these files in our gh-pages branch and pushes them to our remote GitHub repo. This is great, but it means that this doesn’t store our source files.\nTo do so, let’s first open our .gitignore file and make sure that it contains the following entries so that we don’t check in any files we don’t need.\n\n\n.gitignore\n\n# Quarto\n/.quarto/\n_site/\n\n# Mac files\n.DS_Store\n\nNext, we can commit all the remaining files to Git and push them to our remote repo. If we ever lose access to our local machine, we can restore everything we need from GitHub.\nHowever, now we have 2 things we need to do whenever we finish our work:\n\nstore our source files on the main branch and push to GitHub\nrun the publish command to update the blog\n\nThis is a bit annoying and it would be much better if we can just push to the main branch and GitHub would take care of building our website and updating it. This also allows us to create blog entries on any machine that has access to git, we don’t need to have quarto installed. This is particularly practical if we want to write blog entries from our deep learning server. So let’s use GitHub actions for this.\n\n\n\n\n\n\nNote\n\n\n\nBefore you continue make sure you have at least once run a quarto publish gh-pages command, this is necessary for the things below to work\n\n\nFirst we need to add the following snippet to _quarto.yml\n\n\n_quarto.yml\n\nexecute:\n  freeze: auto\n\nThis will make sure that GitHub actions doesn’t execute any executable code, but will show the pre-rendered outputs it finds in the _freeze folder.\nFinally, create the file .github/workflows/publish.yml and populate it with the following code:\n\n\n.github/workflows/publish.yml\n\non:\n  workflow_dispatch:\n  push:\n    branches: main\n\nname: Quarto Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v3\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n\n      - name: Render and Publish\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\nOnce we push these things to GitHub, we are good to go. Whenever we push anything to the main branch, this workflow will execute and take care of updating the gh-pages branch and updating the blog."
  },
  {
    "objectID": "posts/07_stable_diffusion_code/index.html",
    "href": "posts/07_stable_diffusion_code/index.html",
    "title": "Introduction to Stable Diffusion - Code",
    "section": "",
    "text": "In the previous blog post, the main components and some intuition behind Stable Diffusion were introduced. Now, let’s see how we can use the HuggingFace diffusers library to generate images. The content of this blog post is based on Lesson 9 and Lesson 10 of Deep Learning for Coders. The end-to-end pipeline is very practical and easy to use, it’s basically a one-liner. We create a diffusion pipeline by downloading pre-trained models from a repo in the HuggingFace hub. Then, we can call this pipe object with a certain prompt:\n# pip install diffusers==0.12.1\n# pip install accelerate\n# pip install transformers==4.25.1\n\nfrom torchvision import transforms as tfms\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom PIL import Image\n\nnum_inference_steps = 50\nbatch_size = 1\n\npipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\").to(\"cuda\")\n\nprompt = \"Homer from the Simpsons on his roadbike climbing a mountain in the Pyrenees\"\n\n\n\n\nThe config attributes {'scaling_factor': 0.18215} were passed to AutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\ntorch.manual_seed(114)\npipe(prompt, num_inference_steps=num_inference_steps, guidance_scale=7.5).images[0]\nNot bad, but not great either. Let’s dive one layer deeper, and create the components described in the previous post: the Unet, the autoencoder, text encoder and noise scheduler:\nfrom diffusers import AutoencoderKL, LMSDiscreteScheduler, UNet2DConditionModel\nfrom transformers import CLIPTextModel, CLIPTokenizer, logging\n\nlogging.set_verbosity_error()\n\n# Autoencoder, to go from image -&gt; latents (encoder) and back (decoder)\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(\"cuda\")\n\n# UNet, to predict the noise (latents) from noisy image (latents)\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\").to(\"cuda\")\n\n# Tokenizer and Text encoder to create prompt embeddings\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(\"cuda\")\n\n# The noise scheduler\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\nscheduler.set_timesteps(num_inference_steps)\n\nThe config attributes {'scaling_factor': 0.18215} were passed to AutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\nTo use these components, we have to first tokenize the prompt. Tokenization is nothing more then transforming each word of the prompt into it’s associated integer according to a “vocabulary”. The “vocabulary” is the mapping of words to integers and is thus generally quite large.\ntext_input = tokenizer(prompt,               # the prompt we want to tokenize\n                       padding=\"max_length\", # pad the tokenized input to the max length\n                       return_tensors=\"pt\")  # return PyTorch tensors\ntext_input.input_ids\n\ntensor([[49406, 16931,   633,   518, 21092,   525,   787,  4370,  3701,  9877,\n           320,  3965,   530,   518, 39744, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407]])\nAbove we see the integers that are associated with each word in our prompt. We can decode the integers back into words and see if it matches our prompt. Let’s have a look at the first 5 tokens:\n[tokenizer.decode(token) for token in text_input.input_ids[0]][:5]\n\n['&lt;|startoftext|&gt;', 'homer', 'from', 'the', 'simpsons']\nWe see that all capital letters have been removed by the tokenization, and a special token is inserted at the beginning of the prompt. Also, we see the integer 49407 is being used to pad our input to the maximum length:\ntokenizer.decode(49407)\n\n'&lt;|endoftext|&gt;'\nNext, we will pass these tokens through the text-encoder to turn each token into an embedding vector. Since we have 77 tokens and the embeddings are of size 768, this will be a tensor of shape [77, 768].\ntext_embeddings = text_encoder(text_input.input_ids.to(\"cuda\"))[0]\ntext_embeddings.shape\n\ntorch.Size([1, 77, 768])\nWhen generating a completely new image, we start with a fully random noisy latent, so let’s create one:\ntorch.manual_seed(1024)\nlatents = torch.randn((batch_size,              # batch size: 1\n                       unet.config.in_channels, # input channels of the unet: 4\n                       unet.config.sample_size, # height dimension of the unet: 64\n                       unet.config.sample_size) # width dimension of the unet: 64\n                     ).to(\"cuda\")               # put the tensor on the GPU\n\nlatents = latents * scheduler.init_noise_sigma  # scale the noise\n\nlatents.shape\n\ntorch.Size([1, 4, 64, 64])\nThe latents thus carry 4 channels and are of size 64 by 64. Let’s pass this latent iteratively through the Unet, each time subtracting partly the amount of predicted noise (the output of the Unet)\nfor i, t in enumerate(scheduler.timesteps):\n    inputs = latents\n    inputs = scheduler.scale_model_input(inputs, t)\n\n    # predict the noise \n    with torch.no_grad(): \n        pred = unet(inputs, t, encoder_hidden_states=text_embeddings).sample\n\n    # update the latents by removing the predicted noise according to the noise schedule\n    latents = scheduler.step(pred, t, latents).prev_sample\nLet’s visualize the four channels of this latent representation in grey-scale:\nfig, axs = plt.subplots(1, 4, figsize=(16, 4))\nfor c in range(4):\n    axs[c].imshow(latents[0][c].cpu(), cmap='Greys')\nTo transform the latent representation to full-size images, we can use the decoder of the VAE. Note that when we do that, we move from a tensor of shape [4, 64, 64] to [3, 512, 512]:\nprint(latents.shape, vae.decode(latents).sample.shape)\n\ntorch.Size([1, 4, 64, 64]) torch.Size([1, 3, 512, 512])\nAnd let’s visualize the result:\n#scale back according to the VAE paper\nwith torch.no_grad(): image = vae.decode(1 / 0.18215 * latents).sample\n\n# move tensor to numpy\nimage = image[0].detach().cpu().permute(1, 2, 0).numpy()\n# scale the values to 0-255\nimage = ((image / 2 + 0.5).clip(0, 1) * 255).round().astype(\"uint8\")\nImage.fromarray(image)\nUnfortunately, the result looks very bad and especially much worse then our one-liner. The main reason for this, is that the StableDiffusionPipeline is using something called Classifier Free Diffusion Guidance. So let’s have a look at that. But before we do, let’s add two code snippets to transfrom from the latent representation to the full size image representation and back. We will do this a couple of times, so it helps to keep the code a bit cleaner:\ndef latents_to_image(latent):\n    with torch.no_grad(): \n        image = vae.decode(1 / 0.18215 * latent).sample\n\n    image = image[0].detach().cpu().permute(1, 2, 0).numpy()\n    image = ((image / 2 + 0.5).clip(0, 1) * 255).round().astype(\"uint8\")\n    return Image.fromarray(image)\n    \ndef image_to_latent(input_im):\n    with torch.no_grad():\n        latent = vae.encode(torch.Tensor(np.transpose(np.array(input_im) / 255., (2, 0, 1))).unsqueeze(0).to('cuda')*2-1)\n    return 0.18215 * (latent).latent_dist.sample()"
  },
  {
    "objectID": "posts/07_stable_diffusion_code/index.html#classifier-free-diffusion-guidance",
    "href": "posts/07_stable_diffusion_code/index.html#classifier-free-diffusion-guidance",
    "title": "Introduction to Stable Diffusion - Code",
    "section": "Classifier Free Diffusion Guidance",
    "text": "Classifier Free Diffusion Guidance\nClassifier Free Guidance refers to a technique in which two images are being constructed at the same time from the same latent. One of the images is being reconstructed based on the specified prompt (conditional generation), the other image is being generated by an empty prompt (unconditional generation). By mixing the two images in the process according to a parameter (called the guidance-scale) the generated image for the prompt is going to look much better:\n\ntorch.manual_seed(3016)\n\ncond_input = tokenizer(\"Homer from the Simpsons on his roadbike climbing a mountain in the Pyrenees\", padding=\"max_length\", return_tensors=\"pt\") \ncond_embeddings = text_encoder(cond_input.input_ids.to(\"cuda\"))[0]\n\n# Create embeddings for the unconditioned process\nuncond_input = tokenizer(\"\", padding=\"max_length\", return_tensors=\"pt\") \nuncond_embeddings = text_encoder(uncond_input.input_ids.to(\"cuda\"))[0]\n\n# Concatenate the embeddings\nembeddings = torch.cat([cond_embeddings, uncond_embeddings])\n\nguidance_scale = 7.5\n\n# Create a \"fresh\" random latent to start with\nlatents = torch.randn((batch_size, unet.config.in_channels, unet.config.sample_size, unet.config.sample_size)).to(\"cuda\")               \nlatents = latents * scheduler.init_noise_sigma\n\nfor i, t in enumerate(scheduler.timesteps):\n    inputs = torch.cat([latents, latents]) # concatenate the latents\n    inputs = scheduler.scale_model_input(inputs, t)\n\n    # predict the noise \n    with torch.no_grad(): \n        pred = unet(inputs, t, encoder_hidden_states=embeddings).sample\n    \n    # pull both images apart again\n    pred_cond, pred_uncond = pred.chunk(2)\n    \n    # mix the results according to the guidance scale parameter\n    pred = pred_uncond + guidance_scale * (pred_cond - pred_uncond)\n\n    # update the latents by removing the predicted noise according to the noise schedule\n    latents = scheduler.step(pred, t, latents).prev_sample\n    \nlatents_to_image(latents)\n\n\n\n\nMuch better! As you can see, Classifier Free Guidance is a simple technique but it works very well. This morning (03-07-2023) I saw a tweet that introduced the same concept to the world of Large Language Models (LLMs):"
  },
  {
    "objectID": "posts/07_stable_diffusion_code/index.html#negative-prompt",
    "href": "posts/07_stable_diffusion_code/index.html#negative-prompt",
    "title": "Introduction to Stable Diffusion - Code",
    "section": "Negative Prompt",
    "text": "Negative Prompt\nAs mentioned, the unconditional image with Classifier Free Guidance is created from an empty prompt. It turns out that we can use the prompt of this second image as a so-called negative prompt. If there are certain elements we don’t want to see in our image, we can specify it in this prompt.\nWe can see this by rewriting the Classifier Free Guidance equation:\n\\[\\begin{align}\np &= p_{uc} + g (p_{c} - p_{uc}) \\\\\np &= g p_{c} + (1 - g) p_{uc} \\\\\n\\end{align}\\]\nSo with a guidance scale value larger than 1, the unconditional prediction \\(p_{uc}\\) is being subtracted from the conditional prediction \\(p_c\\), which has the effect of removing the concept from the conditional image.\nAn example of Homer Simpson eating lunch:\n\ncond_input = tokenizer(\"Homer Simpson eating lunch\", padding=\"max_length\", return_tensors=\"pt\") \ncond_embeddings = text_encoder(cond_input.input_ids.to(\"cuda\"))[0]\n\nuncond_input = tokenizer(\"\", padding=\"max_length\", return_tensors=\"pt\") \nuncond_embeddings = text_encoder(uncond_input.input_ids.to(\"cuda\"))[0]\n\n\n\nCode\ntorch.manual_seed(105)\n\nembeddings = torch.cat([cond_embeddings, uncond_embeddings])\n\nguidance_scale = 7.5\n\nlatents = torch.randn((batch_size, unet.config.in_channels, unet.config.sample_size, unet.config.sample_size)).to(\"cuda\")               \nlatents = latents * scheduler.init_noise_sigma\n\nfor i, t in enumerate(scheduler.timesteps):\n    inputs = torch.cat([latents, latents]) # concatenate the latents\n    inputs = scheduler.scale_model_input(inputs, t)\n\n    with torch.no_grad(): \n        pred = unet(inputs, t, encoder_hidden_states=embeddings).sample\n    \n    pred_cond, pred_uncond = pred.chunk(2)\n    \n    pred = pred_uncond + guidance_scale * (pred_cond - pred_uncond)\n\n    latents = scheduler.step(pred, t, latents).prev_sample\n\nlatents_to_image(latents)\n\n\n\n\n\nAnd removing the blue chair by using a negative prompt:\n\nuncond_input = tokenizer(\"blue chair\", padding=\"max_length\", return_tensors=\"pt\") \nuncond_embeddings = text_encoder(uncond_input.input_ids.to(\"cuda\"))[0]\n\n\n\nCode\ntorch.manual_seed(105)\n\nembeddings = torch.cat([cond_embeddings, uncond_embeddings])\n\nguidance_scale = 7.5\n\nlatents = torch.randn((batch_size, unet.config.in_channels, unet.config.sample_size, unet.config.sample_size)).to(\"cuda\")               \nlatents = latents * scheduler.init_noise_sigma\n\nfor i, t in enumerate(scheduler.timesteps):\n    inputs = torch.cat([latents, latents]) # concatenate the latents\n    inputs = scheduler.scale_model_input(inputs, t)\n\n    with torch.no_grad(): \n        pred = unet(inputs, t, encoder_hidden_states=embeddings).sample\n    \n    pred_cond, pred_uncond = pred.chunk(2)\n    \n    pred = pred_uncond + guidance_scale * (pred_cond - pred_uncond)\n\n    latents = scheduler.step(pred, t, latents).prev_sample\n\nimage = latents_to_image(latents)\nimage\n\n\n\n\n\nAnd gone is the blue chair! I must admit that this doesn’t always work as great as in this example, in fact I had to try out quite a lot of prompts in combination with negative prompts to find a good example for this post.."
  },
  {
    "objectID": "posts/07_stable_diffusion_code/index.html#image-to-image-generation",
    "href": "posts/07_stable_diffusion_code/index.html#image-to-image-generation",
    "title": "Introduction to Stable Diffusion - Code",
    "section": "Image-to-image generation",
    "text": "Image-to-image generation\nImage-to-image generation is another super interesting process, in which we use both a prompt and an image to guide the generation process. This comes in handy, if for example we want to create a variant of an image we already have. Let’s say we have an awesome image of Homer eating a burger, and we want to have a similar image but instead we want Marge to eat the burger, or we want Homer to eat a slice of pizza instead. We can then feed both the correct promt as well as the already existing image to guide the image generation process even more.\nThe way this works, is by not starting with a completely random latent, but instead build a noisy latent of our existing image.\nLet’s start with the image above and add some noise to it, for example by adding the noise for level 15 (we have 50 noise levels in total, so level 15 means that we still have 35 denoising steps to go):\n\nlatent = image_to_latent(image)\nnoise_latent = torch.randn_like(latent)\n\nsampling_step = 15\nnoised_latent = scheduler.add_noise(latent, noise_latent, timesteps=torch.tensor([scheduler.timesteps[sampling_step]]))\n\nlatents_to_pil(noised_latent)[0]\n\n\n\n\nIf you squint with your eyes you can already see some structure, in the middle there is some yellow blob sitting around (eating lunch..). Let’s take this noisy latent, and do the remaining 35 denoising steps with a different prompt:\n\ntorch.manual_seed(105)\n\ncond_input = tokenizer(\"Homer Simpson eating Hot Pot\", padding=\"max_length\", return_tensors=\"pt\") \ncond_embeddings = text_encoder(cond_input.input_ids.to(\"cuda\"))[0]\n\nuncond_input = tokenizer(\"\", padding=\"max_length\", return_tensors=\"pt\") \nuncond_embeddings = text_encoder(uncond_input.input_ids.to(\"cuda\"))[0]\n\nembeddings = torch.cat([cond_embeddings, uncond_embeddings])\n\nguidance_scale = 7.5\n\n# We start with the noised_latent coming from the image, defined above\nlatents = noised_latent\n\nfor i, t in enumerate(scheduler.timesteps):\n    # we only do the steps starting from the specified level\n    if i &gt;= sampling_step:\n        \n        inputs = torch.cat([latents, latents])\n        inputs = scheduler.scale_model_input(inputs, t)\n\n        with torch.no_grad(): \n            pred = unet(inputs, t, encoder_hidden_states=embeddings).sample\n\n        pred_cond, pred_uncond = pred.chunk(2)\n        pred = pred_uncond + guidance_scale * (pred_cond - pred_uncond)\n\n        latents = scheduler.step(pred, t, latents).prev_sample\n    \nlatents_to_image(latents)\n\n\n\n\nThis image is very similar to what we started with. Color scheme, composition and camera angle are all the same. At the same time, the prompt is also reflected by a change of dishes on the table.\nAnd that’s it, that’s image-to-image generation. As you can see, it’s nothing deeply complicated, it’s just a smart way to re-use the components we have already seen.\nI hope this blog post shows how the components that are introduced in the previous post, translate to code. The examples shown here, only touch upon what can be achieved. In fact, the lessons upon which this post is based show a lot more interesting concepts such as textual inversion. If you are interested, have a look here"
  },
  {
    "objectID": "posts/09_nntrain_ds/index.html",
    "href": "posts/09_nntrain_ds/index.html",
    "title": "nntrain: Datasets and Dataloaders (1/n)",
    "section": "",
    "text": "In this series, I want to discuss the creation of a small library for training neural networks: nntrain. It’s based off the excellent part 2 of Practical Deep Learning for Coders by Jeremy Howard, in which from lessons 13 to 18 (roughly) the development of the miniai library is discussed.\nThe library will build upon PyTorch. We’ll try as much as possible to build from scratch to understand how it all works. Once the main functionality of components are implemented and verified, we can switch over to PyTorch’s version. This is similar to how things are done in the course. However, this is not just a “copy / paste” of the course: on many occasions I take a different route, and most of the code is my own. That is not to say that all of this is meant to be extremely innovative, instead I had the following goals:\nnb_dev is another great project from the fastai community, which allows python libraries to be written in jupyter notebooks. This may sound a bit weird since the mainstream paradigm is to only do experimental work in notebooks. It has the advantage though that we can create the source code for our library in the very same environment in which we want to experiment and interact with our methods, objects and structure while we are building the library. For more details on why this is a good idea and other nice features of nb_dev, see here.\nSo without further ado, let’s start with where we left off in the previous post:"
  },
  {
    "objectID": "posts/09_nntrain_ds/index.html#end-of-last-post",
    "href": "posts/09_nntrain_ds/index.html#end-of-last-post",
    "title": "nntrain: Datasets and Dataloaders (1/n)",
    "section": "End of last post:",
    "text": "End of last post:\nfrom datasets import load_dataset,load_dataset_builder\nimport torchvision.transforms.functional as TF\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nname = \"fashion_mnist\"\nds_builder = load_dataset_builder(name)\nds_hf = load_dataset(name, split='train')\n\nx_train = torch.stack([TF.to_tensor(i).view(-1) for i in ds_hf['image']])\ny_train = torch.stack([torch.tensor(i) for i in ds_hf['label']])\n\ndef fit(epochs):\n    for epoch in range(epochs):\n        for i in range(0,len(x_train), bs):\n            xb = x_train[i:i+bs]\n            yb = y_train[i:i+bs]\n\n            preds = model(xb)\n            acc = accuracy(preds, yb)\n            loss = loss_func(preds, yb)\n            loss.backward()\n\n            opt.step()\n            opt.zero_grad()\n        print(f'{epoch=} | {loss=:.3f} | {acc=:.3f}')\n\ndef accuracy(preds, targs):\n    return (preds.argmax(dim=1) == targs).float().mean()        \n\ndef get_model_opt():\n    layers = [nn.Linear(n_in, n_h), nn.ReLU(), nn.Linear(n_h, n_out)]\n    model = nn.Sequential(*layers)\n    \n    opt = torch.optim.SGD(model.parameters(), lr)\n    \n    return model, opt\n\nn_in  = 28*28\nn_h   = 50\nn_out = 10\nlr    = 0.01\nbs    = 1024\nloss_func = F.cross_entropy\n\nmodel, opt = get_model_opt()\nfit(5)"
  },
  {
    "objectID": "posts/09_nntrain_ds/index.html#datasets",
    "href": "posts/09_nntrain_ds/index.html#datasets",
    "title": "nntrain: Datasets and Dataloaders (1/n)",
    "section": "Datasets:",
    "text": "Datasets:\nAll the stuff in this post will be based on tackling the minibatch construct we currently have in the training loop on lines 16-18:\n...\nfor i in range(0,len(x_train), bs):\n    xb = x_train[i:i+bs]\n    yb = y_train[i:i+bs]\n...\nAnd the first refactor will be to create a Dataset object, which allows us to simplify:\n...\nfor i in range(0,len(x_train), bs):\n    xb, yb = dataset[i:i+bs]\n...\nThis is pretty straight-forward, a Dataset is something that holds our data and upon “indexing into” it returns a sample of the data:\n\nclass Dataset():\n    \n    def __init__(self, x_train, y_train):\n        self.x_train = x_train\n        self.y_train = y_train\n        \n    def __getitem__(self, i):\n        return self.x_train[i], self.y_train[i]\n    \n    def __len__(self):\n        return len(self.x_train)\n\n\nds = Dataset(x_train, y_train)\nprint([i.shape for i in ds[0]])\n\n[torch.Size([784]), torch.Size([])]\n\n\nNext, we want to further improve the training loop and get to this behavior:\n...\nfor xb, yb in dataloader:\n...\nSo our dataloader needs to wrap the dataset, and provide some kind of an iterator returning batches of data, based on the specified batch size. Let’s create one:\n\nclass DataLoader():\n    \n    def __init__(self, dataset, batch_size):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        \n    def __iter__(self):\n        for i in range(0,len(self.dataset),self.batch_size):\n            yield self.dataset[i:i+self.batch_size]\n\nNow the training loop is simplified to:\n\ndef fit(epochs):\n    for epoch in range(epochs):\n        for xb, yb in dl:\n            preds = model(xb)\n            acc = accuracy(preds, yb)\n            loss = loss_func(preds, yb)\n            loss.backward()\n\n            opt.step()\n            opt.zero_grad()\n        print(f'{epoch=} | {loss=:.3f} | {acc=:.3f}')\n\n\ndl = DataLoader(ds, bs)\n\nmodel, opt = get_model_opt()\nfit(5)\n\nepoch=0 | loss=2.062 | acc=0.441\nepoch=1 | loss=1.785 | acc=0.597\nepoch=2 | loss=1.531 | acc=0.637\nepoch=3 | loss=1.334 | acc=0.645\nepoch=4 | loss=1.190 | acc=0.660"
  },
  {
    "objectID": "posts/09_nntrain_ds/index.html#next-up-shuffling-the-data",
    "href": "posts/09_nntrain_ds/index.html#next-up-shuffling-the-data",
    "title": "nntrain: Datasets and Dataloaders (1/n)",
    "section": "Next up: shuffling the data",
    "text": "Next up: shuffling the data\nThe above training loop already looks pretty good, it’s small and concise, and fairly generic. The next improvement we are going to make is something that doesn’t improve the code of the training loop, but improves training of the model. So far during training, we cycle each epoch through the data in the exact same order. This means that all training samples are always batched together with the exact same other samples. This is not good for training our model, instead we want to shuffle the data up. So that each epoch, we have batches of data that have not yet been batched up together. This additional variation helps the model to generalize as we will see.\nThe simplest implementation would be to create a list of indices, which we put in between the dataset and the sampling of the mini-batches. In case we don’t need to shuffle, this list will just be [0, 1, ... len(dataset)].\n\nimport random\n\nclass DataLoader():\n    \n    def __init__(self, dataset, batch_size, shuffle):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        \n    def __iter__(self):\n        self.indices = list(range(0, len(self.dataset)))\n        if self.shuffle: \n            random.shuffle(self.indices)\n            \n        for i in range(0,len(self.dataset),self.batch_size):\n            yield self.dataset[self.indices[i:i+self.batch_size]]\n\n\nmodel, opt = get_model_opt()\ndl = DataLoader(ds, bs, shuffle=True)\nfit(5)\n\nepoch=0 | loss=2.067 | acc=0.429\nepoch=1 | loss=1.800 | acc=0.515\nepoch=2 | loss=1.539 | acc=0.592\nepoch=3 | loss=1.358 | acc=0.618\nepoch=4 | loss=1.187 | acc=0.692\n\n\nThis works just fine, but let’s see if we can encapsulate this logic in a separate class. We start with a simple Sampler class that we can iterate through and either gives indices in order, or shuffled:\n\nclass Sampler():\n    def __init__(self, ds, shuffle=False):\n        self.range = list(range(0, len(ds)))\n        self.shuffle = shuffle\n        \n    def __iter__(self):\n        if self.shuffle: random.shuffle(self.range)\n        for i in self.range:\n            yield i\n\n\ns = Sampler(ds, False)           # shuffle = False\nfor i, sample in enumerate(s): \n    print(sample, end=', ')\n    if i == 5: break\n\n0, 1, 2, 3, 4, 5, \n\n\n\ns = Sampler(ds, True)            # shuffle = TRUE\nfor i, sample in enumerate(s): \n    print(sample, end=', ')\n    if i == 5: break\n\n58844, 19394, 36509, 38262, 51037, 46835, \n\n\nNext, let’s create a BatchSampler that does the same, but returns the indexes in batches. For that we can use the islice() function from the itertools module:\n\nfrom itertools import islice\n\ndef printlist(this): print(list(this))\n\nlst = list(range(0, 10))         # create a list of 10 numbers\n\nprintlist(islice(lst, 0, 3))     # with islice we can get a slice out of the list\nprintlist(islice(lst, 5, 10))\n\n[0, 1, 2]\n[5, 6, 7, 8, 9]\n\n\n\nprintlist(islice(lst, 4))        # we can also get the \"next\" 4 elements\nprintlist(islice(lst, 4))        # doing that twice gives the same first 4 elements\n\n[0, 1, 2, 3]\n[0, 1, 2, 3]\n\n\n\nlst = iter(lst)                  # however if we put an iterator on the list:\n\nprintlist(islice(lst, 4))        # first 4 elements\nprintlist(islice(lst, 4))        # second 4 elements\nprintlist(islice(lst, 4))        # remaining 2 elements\nprintlist(islice(lst, 4))        # iterator has finished..\n\n[0, 1, 2, 3]\n[4, 5, 6, 7]\n[8, 9]\n[]\n\n\nAnd thus we create our BatchSampler:\n\nclass BatchSampler():\n    def __init__(self, sampler, batch_size):\n        self.sampler = sampler\n        self.batch_size = batch_size\n        \n    def __iter__(self):\n        it = iter(self.sampler)\n        while True:\n            res = list(islice(it, self.batch_size))\n            if len(res) == 0:    # return when the iterator has finished          \n                return           \n            yield res\n\nLet’s see the BatchSamepler in action:\n\ns = Sampler(list(range(0,10)), shuffle=False)\nbatchs = BatchSampler(s, 4)\nfor i in batchs:\n    printlist(i)\n\n[0, 1, 2, 3]\n[4, 5, 6, 7]\n[8, 9]\n\n\nAnd let’s incorporate it into the DataLoader:\n\nclass DataLoader():\n    \n    def __init__(self, dataset, batch_sampler):\n        self.dataset = dataset\n        self.batch_sampler = batch_sampler\n        \n    def __iter__(self):\n        for batch in self.batch_sampler:\n            yield self.dataset[batch]\n\n\ns = Sampler(ds, shuffle=True)\ndl = DataLoader(ds, BatchSampler(s, bs))\n\nmodel, opt = get_model_opt()\nfit(5)\n\nepoch=0 | loss=1.981 | acc=0.462\nepoch=1 | loss=1.698 | acc=0.567\nepoch=2 | loss=1.468 | acc=0.620\nepoch=3 | loss=1.346 | acc=0.613\nepoch=4 | loss=1.202 | acc=0.656"
  },
  {
    "objectID": "posts/09_nntrain_ds/index.html#collation",
    "href": "posts/09_nntrain_ds/index.html#collation",
    "title": "nntrain: Datasets and Dataloaders (1/n)",
    "section": "Collation",
    "text": "Collation\nAnd this works pretty good. However, there is one caveat. In the very beginning of this post we did:\nx_train = torch.stack([TF.to_tensor(i).view(-1) for i in ds_hf['image']])\ny_train = torch.stack([torch.tensor(i) for i in ds_hf['label']])\nAnd we ideally would like these transformations to be part of the Dataloaders / Dataset paradigm. So instead of first transforming the Huggingface Dataset into x_train and y_train, we want to directly use the dataset. We can do so by adding a collate function. This wraps around a list of individual samples into the datasets, and receives a list of individual x,y tuples ([(x1,y1), (x2,y2), ..]) as argument. In that function, we can determine how to treat these items and parse it in a way that is suitable to our needs. i.e.:\n\nbatch the x and y, so that we transform from [(x1,y1), (x2,y2), ..] to [(x_1,x_2, ..), (y_1,y_2, ..)]\nmove individual items x_i and y_i to tensors\nstack the x tensors and y tensors respectively into one big tensor\n\nSo let’s update our DataLoader with a collate_func that wraps around individual samples:\n\nclass DataLoader():\n    \n    def __init__(self, dataset, batch_sampler, collate_func):\n        self.dataset = dataset\n        self.batch_sampler = batch_sampler\n        self.collate_func = collate_func\n        \n    def __iter__(self):\n        for batch in self.batch_sampler:\n            yield self.collate_func(self.dataset[sample] for sample in batch)\n\nAnd now let’s create a custom collate function to deal with our data. Specifically, remember that a sample of our huggingface dataset is a dictionary (and not a tuple) with keys image and label holding a PIL.Image.image object and a number (representing any out of 10 classes) respectively.\nSo our collate_func should:\n\ntransform the dictionary into a tuple\nmove everything to a tensor\nzip the results so that x and y are batched\nand combine the list of tensors for x and y respectively into one big tensor\n\n\ndef collate_func(data):\n    data = [(TF.to_tensor(sample['image']).view(-1), torch.tensor(sample['label'])) for sample in data]\n    x, y = zip(*data)\n    return torch.stack(x), torch.stack(y)\n\nAnd let’s see it in action, now using the huggingface dataset ds_hf:\n\ns = Sampler(ds_hf, shuffle=True)\ndl = DataLoader(ds_hf, BatchSampler(s, bs), collate_func)\n\nmodel, opt = get_model_opt()\nfit(5)\n\nepoch=0 | loss=2.125 | acc=0.345\nepoch=1 | loss=1.899 | acc=0.497\nepoch=2 | loss=1.635 | acc=0.609\nepoch=3 | loss=1.389 | acc=0.640\nepoch=4 | loss=1.260 | acc=0.641\n\n\nNot bad, we have replicated the main logic of PyTorch’s DataLoader. The version from PyTorch has a slightly different API as we don’t have to specify the BatchSampler, instead we can just pass shuffle=True:\n\nfrom torch.utils.data import DataLoader\n\ns = Sampler(ds_hf, shuffle=True)\ndl = DataLoader(ds_hf, batch_size=bs, shuffle=True, collate_fn=collate_func)\n\nmodel, opt = get_model_opt()\nfit(5)\n\nepoch=0 | loss=2.107 | acc=0.434\nepoch=1 | loss=1.840 | acc=0.620\nepoch=2 | loss=1.605 | acc=0.641\nepoch=3 | loss=1.354 | acc=0.641\nepoch=4 | loss=1.258 | acc=0.618"
  },
  {
    "objectID": "posts/09_nntrain_ds/index.html#validation-set",
    "href": "posts/09_nntrain_ds/index.html#validation-set",
    "title": "nntrain: Datasets and Dataloaders (1/n)",
    "section": "Validation set",
    "text": "Validation set\nLet’s add a validation set to make sure we validate on data we are not training on. For that we are going to pull the data from the datasets library without the splits argument, which will give us a dataset dictionary containing both a training and a test dataset:\n\nhf_dd = load_dataset(name)\nhf_dd\n\nReusing dataset fashion_mnist (/root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1)\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 60000\n    })\n    test: Dataset({\n        features: ['image', 'label'],\n        num_rows: 10000\n    })\n})\n\n\nAnd let’s create two dataloaders, one for the train and one for the validation set. For the validation loader we can double the batch size since we won’t be computing gradients for the forward pass:\n\ntrain_loader = DataLoader(hf_dd['train'], batch_size=bs, shuffle=True, collate_fn=collate_func)\nvalid_loader = DataLoader(hf_dd['test'], batch_size=2*bs, shuffle=False, collate_fn=collate_func)\n\nWe change the training loop in a couple of ways:\n\ncompute loss and metrics more correctly, by taking care of the batch-size and taking the average over all data\nadd a seperate forward pass for the validation set\n\n\ndef fit(epochs):\n    for epoch in range(epochs):\n        model.train()                                       # put the model in \"train\" mode\n        n_t = train_loss_s = 0                              # initialize variables for computing averages\n        for xb, yb in train_loader:\n            preds = model(xb)\n            train_loss = loss_func(preds, yb)\n            train_loss.backward()\n            \n            n_t += len(xb)\n            train_loss_s += train_loss.item() * len(xb)\n            \n            opt.step()\n            opt.zero_grad()\n        \n        model.eval()                                        # put the model in \"eval\" mode\n        n_v = valid_loss_s = acc_s = 0                      # initialize variables for computing averages\n        for xb, yb in valid_loader:\n            with torch.no_grad():                           # no need to compute gradients on validation set\n                preds = model(xb)\n                valid_loss = loss_func(preds, yb)\n                \n                n_v += len(xb)\n                valid_loss_s += valid_loss.item() * len(xb)\n                acc_s += accuracy(preds, yb) * len(xb)\n        \n        train_loss = train_loss_s / n_t                     # compute averages of loss and metrics\n        valid_loss = valid_loss_s / n_v\n        acc = acc_s / n_v\n        print(f'{epoch=} | {train_loss=:.3f} | {valid_loss=:.3f} | {acc=:.3f}')\n\n\nmodel, opt = get_model_opt()\n\nfit(5)\n\nepoch=0 | train_loss=2.198 | valid_loss=2.095 | acc=0.276\nepoch=1 | train_loss=1.980 | valid_loss=1.852 | acc=0.539\nepoch=2 | train_loss=1.718 | valid_loss=1.591 | acc=0.617\nepoch=3 | train_loss=1.481 | valid_loss=1.387 | acc=0.624\nepoch=4 | train_loss=1.305 | valid_loss=1.241 | acc=0.637\n\n\nAnd that’s it for this post (almost)! We have seen a lot of details on Datasets, Dataloaders and the transformation of data. We have used these concepts to improve our training loop: shuffling the training data on each epoch, and the computation of the metrics on the validation set. But before we close off, let’s make our very first exports into the library, so that next time we can continue where we finished off."
  },
  {
    "objectID": "posts/09_nntrain_ds/index.html#first-exports",
    "href": "posts/09_nntrain_ds/index.html#first-exports",
    "title": "nntrain: Datasets and Dataloaders (1/n)",
    "section": "First exports",
    "text": "First exports\nWhen exporting code to a module with nbdev the first thing we need to do is declare the default_exp directive. This makes sure that when we run the export, the module will be exported to dataloaders.py\n\n #| default_exp dataloaders\n\nNext, we can export any code into the module by adding #|export on top of the cell we want to export. For example:\n\n #| export\n\ndef print_hello():\n    print('hello')\n\nTo export, we simply execute:\n\nimport nbdev; nbdev.nbdev_export()\n\nThis will create a file called dataloaders.py in the library folder (in my case nntrain) with the contents:\n# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_dataloaders.ipynb.\n\n# %% auto 0\n__all__ = ['func']\n\n# %% ../nbs/01_dataloaders.ipynb 59\ndef print_hello():\n    print('hello')\nSo what do we want to export here? Let’s see if we can create some generic code for loading data from the Huggingface datasets library into a PyTorch Dataloader:\n\n #|export\nimport torchvision.transforms.functional as TF\nfrom torch.utils.data import DataLoader\nimport torch\nimport PIL\n\n\n #|export\ndef hf_ds_collate_func(data):\n    '''\n    Collation function for building a PyTorch DataLoader from a a huggingface dataset.\n    Tries to put all items from an entry into the dataset to tensor.\n    PIL images are converted to tensor.\n    '''\n\n    def to_tensor(i):\n        if isinstance(i, PIL.Image.Image):\n            return TF.to_tensor(i).view(-1)\n        else:\n            return torch.tensor(i)\n    \n    data = [map(to_tensor, el.values()) for el in data]  # map each item from a dataset entry through to_tensor()\n    data = zip(*data)                                    # zip data of any length not just (x,y) but also (x,y,z)\n    return (torch.stack(i) for i in data)                \n\n\n #|export\nclass DataLoaders:\n    def __init__(self, train, valid):\n        '''Dataloaders class wrapping two PyTorch dataloaders: train and valid'''\n        self.train = train\n        self.valid = valid\n    \n    @classmethod\n    def _get_dls(cls, train_ds, valid_ds, bs, collate_fn):\n        '''Helper function returning 2 PyTorch Dataloaders as a tuple for 2 Datasets'''\n        return (DataLoader(train_ds, batch_size=bs, shuffle=True, collate_fn=collate_fn),\n                DataLoader(valid_ds, batch_size=bs*2, collate_fn=collate_fn))\n        \n    @classmethod\n    def from_hf_dd(cls, dd, batch_size):\n        '''Factory method to create a Dataloaders object for a Huggingface Dataset dict,\n        uses the `hf_ds_collate_func` collation function'''\n        return cls(*cls._get_dls(*dd.values(), batch_size, hf_ds_collate_func))\n\nWith show_doc() we can include the documentations of class methods:\n\n #|hide\nfrom nbdev.showdoc import *\n\n\nshow_doc(DataLoaders.from_hf_dd)\n\n\n\nDataLoaders.from_hf_dd\n\n DataLoaders.from_hf_dd (dd, batch_size)\n\nFactory method to create a Dataloaders object for a Huggingface Dataset dict, uses the hf_ds_collate_func collation function\n\n\n\nExample usage:\n\ndef fit(epochs):\n    for epoch in range(epochs):\n        model.train()                                       \n        n_t = train_loss_s = 0                              \n        for xb, yb in dls.train:\n            preds = model(xb)\n            train_loss = loss_func(preds, yb)\n            train_loss.backward()\n            \n            n_t += len(xb)\n            train_loss_s += train_loss.item() * len(xb)\n            \n            opt.step()\n            opt.zero_grad()\n        \n        model.eval()                                        \n        n_v = valid_loss_s = acc_s = 0                      \n        for xb, yb in dls.valid: \n            with torch.no_grad():                           \n                preds = model(xb)\n                valid_loss = loss_func(preds, yb)\n                \n                n_v += len(xb)\n                valid_loss_s += valid_loss.item() * len(xb)\n                acc_s += accuracy(preds, yb) * len(xb)\n        \n        train_loss = train_loss_s / n_t                     \n        valid_loss = valid_loss_s / n_v\n        acc = acc_s / n_v\n        print(f'{epoch=} | {train_loss=:.3f} | {valid_loss=:.3f} | {acc=:.3f}')\n\n\ndls = DataLoaders.from_hf_dd(hf_dd, bs)\n\n\nmodel, opt = get_model_opt()\n\nfit(5)\n\nepoch=0 | train_loss=2.183 | valid_loss=2.051 | acc=0.359\nepoch=1 | train_loss=1.923 | valid_loss=1.790 | acc=0.512\nepoch=2 | train_loss=1.662 | valid_loss=1.542 | acc=0.611\nepoch=3 | train_loss=1.438 | valid_loss=1.350 | acc=0.633\nepoch=4 | train_loss=1.271 | valid_loss=1.210 | acc=0.639\n\n\n\n #|hide\nimport nbdev; nbdev.nbdev_export()\n\nAnd that’s it. We have created our first module of the nntrain library🕺. Links:\n\nDataloaders Notebook: the “source” of the source code\nDataloaders module: the .py source code exported from the notebook\nDocumentation: automatically created from the notebook and hosted on Github"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Another Machine Learning Blog",
    "section": "",
    "text": "nntrain: Datasets and Dataloaders (1/n)\n\n\n\n\n\n\n\ncode\n\n\nneural network\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nnntrain: preliminaries (0/n)\n\n\n\n\n\n\n\ncode\n\n\nneural network\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nAug 9, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Stable Diffusion - Code\n\n\n\n\n\n\n\ngenerative\n\n\nstable diffusion\n\n\ndiffusers\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\n\nJul 3, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Stable Diffusion - Concepts\n\n\n\n\n\n\n\ngenerative\n\n\nstable diffusion\n\n\ndiffusers\n\n\nconcepts\n\n\n\n\n\n\n\n\n\n\n\n\nMar 18, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nCross entropy any which way\n\n\n\n\n\n\n\nloss functions\n\n\nsoftmax\n\n\nnll\n\n\ncross entropy\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nFast matrix multiplications\n\n\n\n\n\n\n\nfoundations\n\n\nmaths\n\n\nvectorization\n\n\nlinear algebra\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nFirst competition🏅\n\n\n\n\n\n\n\ndeep learning\n\n\nimage classification\n\n\ncompetition\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nPaperspace setup\n\n\n\n\n\n\n\nsetup\n\n\npaperspace\n\n\ngpu\n\n\nhow-to\n\n\nmlops\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nBlog setup\n\n\n\n\n\n\n\nblogging\n\n\nsetup\n\n\nhow-to\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello👋 and welcome to my blog!"
  },
  {
    "objectID": "posts/06_stable_diffusion_basics/index.html",
    "href": "posts/06_stable_diffusion_basics/index.html",
    "title": "Introduction to Stable Diffusion - Concepts",
    "section": "",
    "text": "Stable Diffusion, a generative deep learning algorithm developed in 2022, is capable of creating images from prompts. For example, when presented the prompt: A group of people having lunch on the moon, the algorithm creates the following image:\nAnd although this image isn’t perfect, it’s pretty amazing that it took less then 30 seconds to create this image. The algorithm “imagined” that people on the moon should be wearing space suits, and that lunch is generally eaten in a sitting position and around a table. Also, the surroundings look indeed pretty moonish. Not bad at all!\nIn this post, we will have a look at the main components involved in creating this image, and follows largely the steps of Lesson 9 of Deep Learning for Coders."
  },
  {
    "objectID": "posts/06_stable_diffusion_basics/index.html#intuition",
    "href": "posts/06_stable_diffusion_basics/index.html#intuition",
    "title": "Introduction to Stable Diffusion - Concepts",
    "section": "Intuition",
    "text": "Intuition\nConsider some kind of black box system that takes some input data, and based on it creates some output data. Let’s say, it takes an image of a handwritten digit as input, and outputs the probability that the image is indeed a hand written digit. Visually something like this:\n\n\n\n\n\nIn statistics, we would call this a probability density function. It’s a function that takes data as input, and gives the probability that\n\nif the input data is coming indeed from the distribution,\nwhat’s the probability that we see this data?\n\nApplied to our use-case: if the presented image is indeed from the distribution (of images) that represent hand written digits, what’s the probability that we observe the presented image?\nWith such a system, we could start with an image made up of pure noise and iteratively do:\n\nget the probability \\(p_0\\) of the image being a handwritten digit from the black box system\nchange the value of one of the pixels at random\nget the new probability \\(p_1\\) whether the image is a handwritten digit from the black box system\nwhen \\(p_1 &gt; p_0\\) update the image with the changed pixel value\n\nWhen following this procedure long enough and thus updating pixel for pixel, we would gradually change all the values of our pixels of our image, until eventually it will start to resemble a handwritten digit.\nIn principle, this is the simple intuition behind stable diffusion."
  },
  {
    "objectID": "posts/06_stable_diffusion_basics/index.html#the-main-component-unet",
    "href": "posts/06_stable_diffusion_basics/index.html#the-main-component-unet",
    "title": "Introduction to Stable Diffusion - Concepts",
    "section": "The main component: Unet",
    "text": "The main component: Unet\nSo how are we going to create this system that will return the probability that an image is depicting a handwritten digit? Let’s try to create a model, that will do so. To get the training data, we need lots of images that depict handwritten digits. Something like this:\n\n\n\n\n\nSince these images represent actual hand-written digits, the system will need to output a probability close to 1 for these images. But how do we get images that “somewhat” or “rarely” represent handwritten digits and are associated with lower probability values? We somehow have to “crappify” these existing images. We can do this by using these same images and sprinkle them with different amounts of noise. The more noise we add, the less the image will resemble a handwritten digit. Visually:\n\n\n\n\n\nNow we can train a network which we feed the noisified images as input and use the noise image as label. So instead of predicting the probability that an image depicts a handwritten digit, the model will predict the noise. By using a simple MSE loss on the actual noise (labels) and the predictions the model will learn how to predict the noise from looking at a noisified images.\nThe idea behind this model is that once this model is trained, we could run inference on some random noise. The model will give us a prediction of all the noise in the image, which when removed from the input, renders an image of a digit.\nIt turns out that this process works much better if, instead of removing all the noise that was predicted by the model at once, we just remove a little bit of the noise that was predicted. This way, we end up with an image which is just a bit less noisy then what we started with. We then feed this less noisy image again into our network, and thus iteratively remove more and more noise from the image, until after a certain amount of steps (50 for example) we end-up with an image that is free of noise.\nOne model architecture that is takes images as input and also outputs images is called a Unet and forms the first component of our Stable Diffusion system:\n\n\n\n\n\n\n\n\nComponent\nInputs\nOutputs\n\n\n\n\nUnet\nNoisy images\nNoise"
  },
  {
    "objectID": "posts/06_stable_diffusion_basics/index.html#compression-variational-autoencoder",
    "href": "posts/06_stable_diffusion_basics/index.html#compression-variational-autoencoder",
    "title": "Introduction to Stable Diffusion - Concepts",
    "section": "Compression: Variational Autoencoder",
    "text": "Compression: Variational Autoencoder\nWhen working with images in neural networks we often reduce the resolution of images or use smaller patches of the original image to make sure everything fits on the GPU. With stable diffusion, we naturally want to output images of high resolution, so we either need very large GPUs, or instead we use a compression trick by making use of a Variational Autoencoder (VAE).\nA VAE is a network architecture having an encoder and a decoder. In the encoder the image input is being transformed through a series of convolutional layers into a compressed representation, the latent. In the decoder this compressed latent is passed through a series of layers that are trying to reconstruct the original image. Visually:\n\n\n\n\n\nThis might look like a boring network architecture at first. But it’s actually a very neat way to compress things: We can feed this model all the different noisified images mentioned earlier, and use an MSE loss on the inputs and outputs. This will train the model to create compressed representations of our images (the latents) that can be used by the decoder to recreate the original image. This means that the latent representation carries close to the same amount of “information” as our full-size images.\nWith this, we can now train the previously discussed Unet on all the latents instead of the full size images!\nDuring inference the combined architecture looks like this: we run any input first through the encoder returning a highly compressed version of our input (i.e. the latents). We then run it through the Unet, which will output a latent representation of the noise. If we (partly) subtract the noise latent from the noisy image latent, we end up with a latent representation of our image which is a bit less noisy then what we started with. Finally, to move from latent representation to full-size images, we can use the decoder of the VAE. Visually:\n\n\n\n\n\nTo summarize:\n\n\n\n\n\n\n\n\nComponent\nInputs\nOutputs\n\n\n\n\nVAE encoder\nNoisy image\nNoisy image latents\n\n\nUnet\nNoisy image latents\nNoise latents\n\n\nVAE decoder\nNoise latents\nNoise"
  },
  {
    "objectID": "posts/06_stable_diffusion_basics/index.html#prompting-clip",
    "href": "posts/06_stable_diffusion_basics/index.html#prompting-clip",
    "title": "Introduction to Stable Diffusion - Concepts",
    "section": "Prompting: CLIP",
    "text": "Prompting: CLIP\nSo how can we create prompting? Let’s start simple and imagine we just want to specify which handwritten digit we would like to generate, so any number between 0 and 9. We could do this by training the Unet not only on the noisy image (input) and noise (output), but instead also give it a representation of the digit we sprinkled the noise on as input. The most generic way to do this, would be to create a one-hot encoded representation of the digit, visually:\n\n\n\n\n\nTo create an image depicting the digit “three” from pure noise, we would then start with a random noise latent and feed it together with the one-hot encoded representation of the digit into the Unet. This way, the Unet is “guided” to create an image of digit “three” and not just any image, visually:\n\n\n\n\n\nTo continue, how are we going to scale this for any text prompt besides our 10 digits? We can’t possibly create a one-hot encoding of any possible prompt, that would make our vector infinitely large. Instead, we want to compress the encoding in some finite, high dimensional space, e.g. we want to create an embedding encoding of our prompt.\nTo create these embeddings, we first of all need again lots of data. For example by capturing a lot of images from the internet, these image generally have a textual description in the HTML tag.\nWe can feed the text and images into two separate encoders. These encoders take the text and image respectively and output a vector. Next, we can align the vector representations in a matrix and take the dot-product between them. We want the text and image vectors of the same “object” to align, this means their dot-product should be large. Also, we want the vectors of different objects to not align, so their dot-product should be small. Visually:\n\n\n\n\n\nA loss function that does exactly this, is called the Contrastive Loss. And the model described here is called Contrastive Language Image Pre-training (CLIP).\nDuring inference, we can use the trained text-encoder and apply it to the prompt. The outputted embedding can then be used as the encoding we feed into our Unet in combination with the noisy image latent.\nTo summarize:\n\n\n\n\n\n\n\n\nComponent\nInputs\nOutputs\n\n\n\n\nCLIP text encoder\nPrompt\nEmbedding\n\n\nVAE encoder\nNoisy image\nNoisy image latents\n\n\nUnet\nNoisy image latents + Prompt embedding\nNoise latents\n\n\nVAE decoder\nNoise latents\nNoise"
  },
  {
    "objectID": "posts/06_stable_diffusion_basics/index.html#noise-scheduler",
    "href": "posts/06_stable_diffusion_basics/index.html#noise-scheduler",
    "title": "Introduction to Stable Diffusion - Concepts",
    "section": "Noise scheduler",
    "text": "Noise scheduler\nAbove it was stated, that “different” amounts of noise are sprinkled on our images during training, and during inference “some” amount of noise is being subtracted from the image. In the next post, which will be a “code” version of this post, we will see more how this exactly works, but let’s introduce one more concept here:\nTo formalize the amounts of noise we will use something called a noise schedule, which maps an integer value (called the timestep \\(t\\)) to an amount of noise we will add to our image. This noise schedule is a monotonically decreasing function of \\(t\\), so large values of \\(t\\) will add a small amount of noise and small values of \\(t\\) add a large amount of noise. A typical noise schedule looks something like this:\n\n\n\n\n\nWith this noise schedule, we can pick different amounts of noise during training and add it to the images in the batch. Additionally, we will feed the noise parameter to the Unet, so that it knows how much noise was added to the image. This sould make it easier for the model to reconstruct the noise.\nTo summarize:\n\n\n\n\n\n\n\n\nComponent\nInputs\nOutputs\n\n\n\n\nCLIP text encoder\nPrompt\nEmbedding\n\n\nVAE encoder\nNoisy image\nNoisy image latents\n\n\nUnet\nNoisy image latents + Prompt embedding + Noise level\nNoise latents\n\n\nVAE decoder\nNoise latents\nNoise\n\n\n\nThat’s it for now! If you came this far, I hope you enjoyed it. For me, it helped a lot in my understanding by writing all this down. In the next blog post, we will have a look at how these concepts translate into code by making use of HuggingFace libraries such as diffusers and transformers"
  },
  {
    "objectID": "posts/03_aiornot/index.html",
    "href": "posts/03_aiornot/index.html",
    "title": "First competition🏅",
    "section": "",
    "text": "In the past couple of weeks I have participated in the first ever Hugging Face competition: aiornot. And as a matter of fact, it was also my first competition to participate in! The competition consisted of 62060 images (18618 train and 43442 test images) which were either created by an AI or not (binary image classification).\nToday, the competition has finished and the private leaderboard has been made public. I’m super happy (and proud 😇) that I finished in 15th place (98 participants):"
  },
  {
    "objectID": "posts/03_aiornot/index.html#credit-where-credit-is-due",
    "href": "posts/03_aiornot/index.html#credit-where-credit-is-due",
    "title": "First competition🏅",
    "section": "Credit where credit is due:",
    "text": "Credit where credit is due:\n\n🤗 Hugging Face\nI would like to thank Hugging Face and in particular Abhishek Thakur for organizing this competition. I started looking for a first competition at Kaggle a few weeks back, and was very interested in the RSNA competition but quickly found that it was probably a bit too complicated for my first competition. I then saw a tweet from Abhishek announcing this competition and found it a perfect competition to get started.\n\n\nfastai\nIn the past month I have been following the fastai course and I am extremely grateful to Jeremy Howard and Sylvain Gugger for creating fastai. The book, the course, the videos and the great community they have built is really something special and is perfectly tailored for anybody who wants to get started with Deep Learning. Without fastai I could never have pulled this off 🙏."
  },
  {
    "objectID": "posts/03_aiornot/index.html#learnings-and-notes",
    "href": "posts/03_aiornot/index.html#learnings-and-notes",
    "title": "First competition🏅",
    "section": "Learnings and notes",
    "text": "Learnings and notes\n\nI quickly learned that data augmentation didn’t work well on this data. Initially I was a bit surprised by this, but upon inspection of the images I arrived at the following intuition. Normally we want to classify images by what’s being displayed in the image. So 2 images of a bike should both be classified as such. However, in this dataset we can have images of the same object but if one is created by an AI, and the other is not then they should be classified differently. So instead of looking at what’s being displayed, it probably has to learn more about the style or the way the image is built up. I can imagine that data augmentation makes this more difficult, especially warping, affine transformations and brightness, contrast augmentations. I was happily surprised to find that the 2nd and 4th place solutions also didn’t use these data augmentation!\nTraining on larger images works very well. I got a large performance boost for switching to sizes of 416. Jeremy Howard mentioned that this generally works well, and I think because of the nature of these images it worked especially well. To train large models on large images, I heavily relied on Gradient Accumulation to not have to reduce the batchsize.\nTransformer based models such as SWIN and VIT performed not as good as models based on convolutions, I used the convnext models.\nProgressive resizing didn’t work for me.\nI tried training on 5 epochs and 10 epochs. 10 epochs never gave me better results.\n\nLast but not least:\nParticipating in competitions is very motivating and rewarding. Working individually through courses, exercises and lecture notes is very interesting, but you don’t get a lot of feedback to how you are doing. Am I doing well? Should I spend more time on investigations into certain areas? When participating in a real-world competition you have a very clear goal, and you get immediate feedback on how you are doing. This type of project based learning has the advantage that it’s very clear what you need to focus on: anything that you encounter during the project.\nIt’s also great that it has a finite timeline, so that afterwards you can have a sense of achievement which motivates a lot. The Germans have a very nice word for this: Erfolgserlebnis.\n\n\n\nImage for Stable Diffusion prompt: “Sense of achievement when finishing my first ever machine learning competition”"
  },
  {
    "objectID": "posts/02_paperspace_setup/index.html",
    "href": "posts/02_paperspace_setup/index.html",
    "title": "Paperspace setup",
    "section": "",
    "text": "Most people don’t have a GPU installed in their working machine that is suited for Deep Learning, and in fact you don’t need to. It’s quite easy to setup a remote GPU server nowadays, and in this blog I will explain how to do so with Paperspace Gradient.\nI started using Paperspace because of a recommendation from Jeremy Howard in his Live Coding Videos. If you haven’t seen these lectures, I can highly recommend them. They are a great resource on many things related to getting started with Deep Learning. Jeremy shows a lot of productivity hacks and practical tips on getting a good setup.\nHowever, the Paperspace setup explanations are a bit out-dated which can lead to confusion when following along with the video’s. Also, after the recording of the videos Jeremy created some nice scripts which simplify the setup. This blog will hopefully help others to navigate this and quickly set-up a remote GPU server. I would advice anybody who wants to try Paperspace, to first watch the videos from Jeremy to have a general idea of how it works, and then follow these steps to quickly get set-up.\nOnce you have signed up to Paperspace, go to their Gradient service and create a new project. Paperspace has a free tier, as well as a pro- ($8/month) and growth-plan ($39/month). I personally signed up for the pro-plan, which has a very good value for money. You get 15Gb persistent storage and free Mid instance types. If available, I use the A4000, which is the fastest and comes with 16GB of GPU memory.\nWith the pro-plan you can create up to 3 servers, or “Notebooks” as they are called by Paperspace (throughout this blog I’ll refer to them as Notebook Servers). So let’s create one:"
  },
  {
    "objectID": "posts/02_paperspace_setup/index.html#first-look-at-our-notebook-server",
    "href": "posts/02_paperspace_setup/index.html#first-look-at-our-notebook-server",
    "title": "Paperspace setup",
    "section": "First look at our Notebook Server",
    "text": "First look at our Notebook Server\nNext, let’s open a terminal and get familiar with our Server\n\n\nTerminal\n\n&gt; which python\n/usr/local/bin/python\n\n&gt; python --version\nPython 3.9.13\n\nAnd let’s also check the PATH variable:\n\n\nTerminal\n\n&gt; echo $PATH\n/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin: /usr/sbin:/usr/bin:/sbin:/bin:/root/mambaforge/bin\n\nThe python command is thus pointing to the system Python installation. However, on the PATH variable we are also seeing an entry at the end mentioning mambaforge.\nAnd indeed we can execute:\n\n\nTerminal\n\n&gt; mamba list | grep python\n\nipython                   8.5.0              pyh41d4057_1    conda-forge\nipython_genutils          0.2.0                      py_1    conda-forge\npython                    3.10.6          h582c2e5_0_cpython    conda-forge\npython-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge\npython-fastjsonschema     2.16.2             pyhd8ed1ab_0    conda-forge\npython_abi                3.10                    2_cp310    conda-forge\n\nSo we are having both a mamba based Python 3.10.6 and a system installation of Python 3.9.13.\nLet’s open a Jupyter Notebook and see which Python version is running:\n\n\nUntitled.ipynb\n\nimport sys\nsys.version\n\nWhich returns: '3.9.13 (main, May 23 2022, 22:01:06) \\n[GCC 9.4.0]'. Jupyter is thus running the system Python installation.\n\n\n\n\n\n\nNote\n\n\n\nIn the videos Jeremy mentions that we should never use the system Python but instead always create a Mamba installation. However, since we are working here on a virtual machine that is only used for running Python, this shouldn’t be a problem. Just be aware that we are using the system Python which is totally separate from the Mamba setup.\n\n\nSince we are running the system Python version, we can inspect all the packages that are installed:\n\n\nTerminal\n\n&gt; pip list\n\n...\nfastai                            2.7.10\nfastapi                           0.92.0\nfastbook                          0.0.28\nfastcore                          1.5.27\nfastdownload                      0.0.7\nfastjsonschema                    2.15.3\nfastprogress                      1.0.3\n...\ntorch                             1.12.0+cu116\ntorchaudio                        0.12.0+cu116\ntorchvision                       0.13.0+cu116\n..."
  },
  {
    "objectID": "posts/02_paperspace_setup/index.html#persisted-storage-at-paperspace",
    "href": "posts/02_paperspace_setup/index.html#persisted-storage-at-paperspace",
    "title": "Paperspace setup",
    "section": "Persisted Storage at Paperspace",
    "text": "Persisted Storage at Paperspace\nIn general, things are not persisted on Paperspace. So anything we store during a session, will be gone when we restart our Notebook Server. However, Paperspace comes with two special folders that are persisted. It’s important to understand how these folder works since we obviously need to persist our work. Not only that, but we also need to persist our configuration files from services lik GitHub, Kaggle and HuggingFace and potentially any other config files for tools or services we are using.\nThe persisted folders are called /storage and /notebooks. Anything in our /storage is shared among all the Notebook Servers we are running, whereas anything that is stored in the /notebooks folder is only persisted on that specific Notebook Server."
  },
  {
    "objectID": "posts/02_paperspace_setup/index.html#set-up",
    "href": "posts/02_paperspace_setup/index.html#set-up",
    "title": "Paperspace setup",
    "section": "Set up",
    "text": "Set up\nIn the first few videos, Jeremy shows a lot of tricks on how to install new packages and set up things like Git and GitHub. After the recording of these videos, he made a GitHub repo which facilitates this setup greatly and makes most of the steps from the videos unnecessary. So let’s use that:\n\n\nTerminal\n\n&gt; git clone https://github.com/fastai/paperspace-setup.git\n&gt; cd paperspace-setup\n&gt; ./setup.sh\n\nTo understand what this does, let’s have a look at setup.sh:\n\n\nsetup.py\n\n#!/usr/bin/env bash\n\nmkdir /storage/cfg\ncp pre-run.sh /storage/\ncp .bash.local /storage/\necho install complete. please start a new instance\n\nFirst it’s creating a new directory inside of our /storage folder called cfg. As we will see, this is where we will store all our configuration files and folders.\nNext, the script copies 2 files to our storage folder. Let’s have a closer look at those\n\npre-run.sh\nDuring startup of a Notebook Server (upon creation or restart), Paperspace automatically executes the script it finds at /storage/pre-run.sh. This is really neat, since we can create a script at this location to automate our setup!\nFor the full script, click here, and let’s have a closer look at this first snippet:\n\n\npre-run.sh (snippet)\n\nfor p in .local .ssh .config .ipython .fastai .jupyter .conda .kaggle\ndo\n        if [ ! -e /storage/cfg/$p ]; then\n                mkdir /storage/cfg/$p\n        fi\n        rm -rf ~/$p\n        ln -s /storage/cfg/$p ~/\ndone\n\nSo we are iterating through a list of folder names (.local .ssh ...) on line 1, and for each one we create a directory inside of /storage/cfg on line 4. We only do this if the directory doesn’t already exist on line 3. Next, each of these folders is symlinked to the home directory (~/) on line 7.\nThis means that:\n\nWhen we store something in any of these symlinked folders (e.g. ~/.local), it’s actually being written to the associated storage folder (e.g. /storage/cfg/.local) because of the symlink.\nWhenever we restart our Notebook Server, all the stuff that has previously been persisted (e.g. in /storage/cfg/.local) are made available again in the home directory (e.g. ~/.local).\n\nThis is very nice, because as it turns out: many tools keep their configuration files in this home folder. So by persisting this data, they will keep working across restarts of our Notebook servers.\nLet’s a closer look at the folders we are persisting:\n\n.local\nWe saw before that the FastAI runtime comes with a number of installed Python packages. If we want to install additional packages, we could do: pip install &lt;package&gt;. However, pip installs the packages in /usr/local/lib, and are thus not persisted. To make sure our packages are persisted, we can instead install with pip install --user &lt;package&gt;. This --user flag, tells pip to install the package only for the current user, and so it installs into the ~/.local directory. So by persisting this folder, we make sure that we our custom installed python packages are persisted, awesome!\n\n\n.ssh\nTo authenticate with GitHub without using passwords, we use ssh keys. To create a pair of keys, we run: ssh-keygen. This creates the private key (id_rsa) and the public key (id_rsa.pub) to the ~/.ssh folder. Once we upload the public key to GitHub we can authenticate with GitHub, and by persisting this folder we can authenticate upon restart!\nBy now you probably get the idea, any of these folders represent a certain configuration we want to persist:\n\n.conda: contains conda/mamba installed packages\n.kaggle: contains a kaggle.json authentication file\n.fastai: contains downloaded datasets and some other configuration\n.config, .ipython and .jupyter: contain config files for various pieces of software such as matplotlib, ipython and jupyter.\n\nI personally also added .huggingface to this list, to make sure my HuggingFace credentials are also persisted. See here for the PR back into the main repo.\nIn the second part of the script we do exactly the same thing, but for a number of files instead of directories.\n\n\npre-run.sh (snippet)\n\nfor p in .git-credentials .gitconfig .bash_history\ndo\n        if [ ! -e /storage/cfg/$p ]; then\n                touch /storage/cfg/$p\n        fi\n        rm -rf ~/$p\n        ln -s /storage/cfg/$p ~/\ndone\n\nNow that we understand pre-run.sh, let’s have a look at the second file we store in our /storage folder:\n\n\n\n.bash.local\n\n\n.bash.local\n\n#!/usr/bin/env bash\n\nalias mambai='mamba install -p ~/.conda '\nalias pipi='pip install --user '\n\nexport PATH=~/.local/bin:~/.conda/bin/:$PATH\n\nPaperspace runs this script whenever we open a terminal. As you can see it defines two aliases to easily install things persistently with either mamba (mambai) or pip (pipi).\nAny binaries that are installed this way, are installed in ~/.local/bin (through pip) and to ~/.conda/bin/ (through mamba). We need to add these paths to the PATH variable, to make sure we can call them from the command line.\n\n\nNote on Mamba\nAt this point you might wonder why we have the Mamba installation at all, since we have seen that the system Python is used. In fact, our Mamba environment is totally decoupled from what we are using in our Jupyter notebook, and installing packages through mamba will not make them available in Jupyter. Instead, we should install Python packages through pip.\nSo what do we need Mamba for? I guess Jeremy has done this to be able to install binaries that he wants to use from the Terminal. For example, in the videos he talks about ctags which he installs through mamba. Since installing none-Python specific binaries through pip can be complicated, we can use Mamba instead. In other words, we can use it as a general package manager, somewhat similar to apt-get.\n\n\nFinal words\nIn my opinion Paperspace offers a great product for very fair money, especially if combined with the setup described in this blog!"
  },
  {
    "objectID": "posts/08_nntrain_setup/index.html",
    "href": "posts/08_nntrain_setup/index.html",
    "title": "nntrain: preliminaries (0/n)",
    "section": "",
    "text": "In this series, I want to discuss the creation of a small library for training neural networks: nntrain. It’s based off the excellent part 2 of Practical Deep Learning for Coders by Jeremy Howard, in which from lessons 13 to 18 (roughly) the development of the miniai library is discussed.\nThe library will build upon PyTorch. We’ll try as much as possible to build from scratch to understand how it all works. Once the main functionality of components are implemented and verified, we can switch over to PyTorch’s version. This is similar to how things are done in the course. However, this is not just a “copy / paste” of the course: on many occasions I take a different route, and most of the code is my own. That is not to say that all of this is meant to be extremely innovative, instead I had the following goals:\nnb_dev is another great project from the fastai community, which allows python libraries to be written in jupyter notebooks. This may sound a bit weird and controversial, but it has the advantage that we can create the source code for our library in the very same environment in which we want to experiment and interact with our methods, objects and structure while we are building the library. For more details on why this is a good idea and other nice features of nb_dev, see here.\nSo without further ado, let’s start with some data!"
  },
  {
    "objectID": "posts/08_nntrain_setup/index.html#data",
    "href": "posts/08_nntrain_setup/index.html#data",
    "title": "nntrain: preliminaries (0/n)",
    "section": "Data",
    "text": "Data\nTo keep things simple, let’s use the fashion-mnist dataset. We can get the data from the huggingface datasets library:\n\nfrom datasets import load_dataset,load_dataset_builder\n\nname = \"fashion_mnist\"\nds_builder = load_dataset_builder(name)\nprint(ds_builder.info.description)\n\n\n\n\n\n\n\nFashion-MNIST is a dataset of Zalando's article images—consisting of a training set of\n60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image,\nassociated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in\nreplacement for the original MNIST dataset for benchmarking machine learning algorithms.\nIt shares the same image size and structure of training and testing splits.\n\n\n\n\nds = load_dataset(name, split='train')\n\nDownloading and preparing dataset fashion_mnist/fashion_mnist (download: 29.45 MiB, generated: 34.84 MiB, post-processed: Unknown size, total: 64.29 MiB) to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1...\nDataset fashion_mnist downloaded and prepared to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1. Subsequent calls will reuse this data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nds is a Dataset object. These kind of objects appear in many Deep Learning libraries and have two main functionalities: you can index into them and they have a length:\n\nds[0]\n\n{'image': &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;,\n 'label': 9}\n\n\n\nlen(ds)\n\n60000\n\n\n\nds\n\nDataset({\n    features: ['image', 'label'],\n    num_rows: 60000\n})\n\n\nHugginface datasets (as opposed to PyTorch datasets) also have some properties, in this case num_rows, which is the length of the dataset (60000) and features, a dictionary giving metadata on what is returned when we index into the dataset:\n\nds.features\n\n{'image': Image(decode=True, id=None),\n 'label': ClassLabel(num_classes=10, names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)}\n\n\nLet’s visualize one single item:\n\nimport matplotlib.pyplot as plt\n\nimage = ds[0]['image']\nlabel = ds[0]['label']\n\nfigure, axs = plt.subplots()\n\naxs.imshow(ds[0]['image'], cmap='Greys')\naxs.set_title(f'Image of the first item in the dataset: label={label} -&gt; \"{ds.features[\"label\"].int2str(label)}\"');\naxs.axis('off');\n\n\n\n\nSince we want to start simple, and only later get to Datsets and Dataloaders: let’s pull out the data into a tensor so we can build simple linear layers.\n\nimport torchvision.transforms.functional as TF   # to transform from PIL to tensor\nimport torch\n\nx_train = [TF.to_tensor(i).view(-1) for i in ds['image']]\ny_train = [torch.tensor(i) for i in ds['label']]\n\nlen(x_train), len(y_train), len(x_train[0])\n\n(60000, 60000, 784)\n\n\nSo x_train and y_train are both lists of length 60000, and an element in x_train has length 784 (28x28 pixels)."
  },
  {
    "objectID": "posts/08_nntrain_setup/index.html#linear-layers",
    "href": "posts/08_nntrain_setup/index.html#linear-layers",
    "title": "nntrain: preliminaries (0/n)",
    "section": "Linear layers",
    "text": "Linear layers\nNow that we have the data, let’s create our very first network operation: a linear layer which takes the 784 long flattened out image vector, and maps it to an output vector of length 10\n\nimport torch\n\ndef lin(x, a, b):\n    return x@a + b\n\na = torch.randn(784, 10)\nb = torch.randn(10)\n\nout = lin(x_train[0], a, b)\nout.shape\n\ntorch.Size([10])\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor details on matrix multiplications, check out this post I wrote earlier.\n\n\nLet’s do the same for all our training data at once:\n\nx_train = torch.stack(x_train)\nout = lin(x_train, a,b)\nout.shape\n\ntorch.Size([60000, 10])\n\n\nNice, that’s basically a forward pass through our model on all our training data!\nNow if we want to increase the depth of our network by adding an additional layer, we need to add a non-linearity in the middle. Why? See for example the first paragraphs of this answer.\nLet’s add a ReLu nonlinearity:\n\ndef relu(x):\n    return x.clamp_min(0.0)\n\nAnd let’s combine these into our first “model”, consisting of two linear layers and a relu nonlinearity in the middle:\n\nn_in = 784 # number of input units (28x28)\nn_h = 50   # number of hidden units\nn_out = 10 # number of output units\n\nw1 = torch.randn(n_in, n_h)\nb1 = torch.zeros(n_h)\nw2 = torch.randn(n_h, n_out)\nb2 = torch.zeros(n_out)\n\ndef model(x):\n    a1 = lin(x, w1, b1)\n    z1 = relu(a1)\n    return lin(z1, w2, b2)\n\n\nout = model(x_train)\n\n\nout.shape\n\ntorch.Size([60000, 10])\n\n\nOur “model” currently only does a forward pass through the network. And as a matter of fact, it’s doing a forward pass with random weights. When training a neural network, we want to change these parameters in a way that the outputs of the network align with the outputs (y_train). I will not go into the details of this, but here is a great video by Andrej Karpathy which in my opinion gives one of the best explanations into how this works.\nBefore doing a backward pass, we first have to calculate the loss. Since the outputs represent any of the 10 classes the image corresponds with, cross entropy is a straight forward loss function. Some details about cross entropy loss can be found in a post I wrote earlier. However, since we want to add the backpropagation ourselves and I don’t know how to backpropagate through cross entropy (and I don’t feel like spending a lot of time on it), let’s use a much easier loss function for now: mean squared error (MSE). This obviously doesn’t make any sense in the context of our data, but mathematically it’s possible. We just have to end up with a single activation of our model instead of 10:\n\nn_out = 1  # number of output units changed to 1\n\nw2 = torch.randn(n_h, n_out)\nb2 = torch.zeros(n_out)\n\ndef model(x):\n    a1 = lin(x, w1, b1)\n    z1 = relu(a1)\n    return lin(z1, w2, b2)\n\nout = model(x_train)\n\n\nout.shape\n\ntorch.Size([60000, 1])\n\n\nFrom which we see that the outputs have an empty trailing dimension. y_train doesn’t have this, so we have to squeeze out this empty dimension when computing the MSE:\n\ndef mse(pred, targ): \n    return (pred.squeeze(-1)-targ).pow(2).mean() \n\ny_train = torch.stack(y_train)\nmse(out, y_train)\n\ntensor(3015.2351)\n\n\nThe next step will be to add the backward pass. But let’s refactor our code to put things into classes, that way the backward pass can be added more easily:\n\nclass Linear():\n    def __init__(self, n_in, n_out):\n        self.w = torch.randn(n_in, n_out)\n        self.b = torch.zeros(n_out)\n    \n    def __call__(self, x):\n        self.inp = x                      # storing this for the backward pass\n        self.out = x@self.w + self.b      # storing this for the backward pass\n        return self.out\n    \nclass Relu():\n    def __call__(self, x):\n        self.inp = x                      # storing this for the backward pass\n        self.out = x.clamp_min(0.)        # storing this for the backward pass\n        return self.out\n    \nclass MSE():\n    def __call__(self, pred, targ):\n        self.pred = pred                   # storing this for the backward pass\n        self.targ = targ                   # storing this for the backward pass\n        self.out = (pred.squeeze(-1)-targ).pow(2).mean()\n        return self.out\n    \nclass Model():\n    def __init__(self, n_in, n_h, n_out):\n        self.layers = [Linear(n_in, n_h), Relu(), Linear(n_h, n_out)]\n        self.loss = MSE()\n        \n    def __call__(self, x, y):\n        for l in self.layers: x = l(x)\n        return self.loss(x, y)\n\n\nx_train.shape\n\ntorch.Size([60000, 784])\n\n\n\nm = Model(n_in, n_h, n_out)\nl = m(x_train, y_train)\n\nTo add in the functionality for the backward pass, redefining the whole class is a nuisance. So instead we’ll patch the classes. We can do this very easily by using the fastcore library. Let’s see a small example:\n\nimport fastcore.all as fc\n\nclass A():\n    def hi(self): print('hello 😎')\n    \na = A()\na.hi()\n\n@fc.patch\ndef hi(self:A): print('howdy 🤠')\n\na.hi()\n\nhello 😎\nhowdy 🤠\n\n\nSo with fc.patch we can extend or change the behavior of Classes that have been defined elsewhere, even on instances of the objects that are already created. Nice!\n\n@fc.patch\ndef backward(self: Linear):\n    self.inp.g = self.out.g @ self.w.t()\n    self.w.g = self.inp.t() @ self.out.g\n    self.b.g = self.out.g.sum(0)\n    \n@fc.patch\ndef backward(self: Relu):\n    self.inp.g = (self.inp&gt;0).float() * self.out.g\n    \n@fc.patch\ndef backward(self: MSE):\n    self.pred.g = 2. * (self.pred.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]\n    \n@fc.patch\ndef backward(self: Model):\n    self.loss.backward()\n    for l in reversed(self.layers): l.backward()\n\n\nm = Model(n_in, n_h, n_out)\nl = m(x_train, y_train)\nm.backward()\n\nNow the actual operations in the backward methods you will just have to take for granted as I am not going to derive them. If you want, you can have some fun (?) to try and derive it yourself. What I think is most important about these formulas:\n\nNotice that each layer has a reference to it’s inputs and it’s outputs\nDuring the backward pass, each layer uses the gradient from the outputs and uses it to set the gradient on the inputs\nThe inputs from layer \\(n\\) are the outputs from layer \\(n-1\\), so when the gradients are being set on the inputs from layer \\(n\\), this means that layer \\(n-1\\) it’s outputs are being set at the same time\nThis is the fundamental point about backpropagation of the gradient: in reverse order, layer by layer the gradients are being propagated back through the network using the chain rule\nAlthough we don’t derive the operations, we can see that that there exist operations that do this. These operations are not magical, they are just the result of calculus: not very different from the fact that if \\(f(x) = x^2\\) then \\(f'(x) = 2x\\) and if \\(h(x) = f(g(x))\\) then \\(h'(x) = f'(g(x)) * g'(x)\\)"
  },
  {
    "objectID": "posts/08_nntrain_setup/index.html#first-refactor-module-baseclass-and-training-loop",
    "href": "posts/08_nntrain_setup/index.html#first-refactor-module-baseclass-and-training-loop",
    "title": "nntrain: preliminaries (0/n)",
    "section": "First refactor: Module baseclass and training loop",
    "text": "First refactor: Module baseclass and training loop\nNow let’s see how we can make this a little better. One thing that seems a bit silly is that in each of the Linear, MSE and Relu classes, we are storing explicitly the inputs and outputs when doing a forward call. As mentioned, we need this to backpropagate the gradients. However, we rather not store that explicitly all the time when creating a new layer.\nSo let’s create a base class that takes care of this:\n\nPack the forward functionality of each layer in a dedicated forward method\nlet the storing of inputs and ouputs be done in the __call__ method of the baseclass, and call the self.forward method in between.\n\nThis works, but there is one caveat: most layers just have one input when they are called (x), but the loss has 2 (pred and targ). To make this storing of the inputs generic we can store them as an array on the base class, and also pass them as positional arguments to _backward. This way, forward and _backward have the same arguments.\n\nclass Module():\n    def __call__(self, *args):\n        self.args = args\n        self.out = self.forward(*args)\n        return self.out\n    \n    def backward(self): self._backward(*self.args)\n\n    \nclass Linear(Module):\n    def __init__(self, n_in, n_out):\n        self.w = torch.randn(n_in, n_out)\n        self.b = torch.zeros(n_out)\n    \n    def forward(self, x):\n        return x@self.w + self.b\n    \n    def _backward(self, inp):\n        inp.g = self.out.g @ self.w.t()\n        self.w.g = inp.t() @ self.out.g\n        self.b.g = self.out.g.sum(0)\n    \n    \nclass Relu(Module):\n    def forward(self, x):\n        return x.clamp_min(0.)\n    \n    def _backward(self, inp):\n        inp.g = (inp&gt;0).float() * self.out.g\n\n    \nclass MSE(Module):\n    def forward(self, pred, targ):\n        return (pred.squeeze(-1)-targ).pow(2).mean()\n    \n    def _backward(self, pred, targ):\n        pred.g = 2. * (pred.squeeze() - targ).unsqueeze(-1) / targ.shape[0]\n    \n    \nclass Model(Module):\n    def __init__(self, n_in, n_h, n_out):\n        self.layers = [Linear(n_in, n_h), Relu(), Linear(n_h, n_out)]\n        self.loss = MSE()\n        \n    def forward(self, x, y):\n        for l in self.layers: x = l(x)\n        return self.loss(x, y)\n    \n    def backward(self):\n        self.loss.backward()\n        for l in reversed(self.layers): l.backward()\n\nWith these objects, let’s create our first training loop:\n\nepochs = 5                              # train for nr of epochs\nbs     = 1024                           # batch-size\nlr     = 0.01                           # learning rate\nm = Model(n_in, n_h, n_out)             # instantiate our model\n\nfor epoch in range(epochs):             # iterate through epochs\n    for i in range(0,len(x_train), bs): # iterate through the batches\n        xb = x_train[i:i+bs]            # get minibatch \n        yb = y_train[i:i+bs]\n        \n        loss = m(xb, yb)                # forward pass\n        m.backward()                    # backward pass\n        \n        for l in m.layers:              # iterate through the layers\n            if isinstance(l, Linear):   # only update the linear layers\n                l.w += - lr * l.w.g     # update the weights\n                l.b += - lr * l.b.g     # update the bias\n\n                l.w.g = None            # reset the gradients\n                l.b.g = None\n    print(f'{epoch=} | {loss=:.1f}')\n\nepoch=0 | loss=14242.1\nepoch=1 | loss=1329.6\nepoch=2 | loss=135.2\nepoch=3 | loss=21.2\nepoch=4 | loss=9.7\n\n\nAwesome, the loss is decreasing i.e. the model is training!"
  },
  {
    "objectID": "posts/08_nntrain_setup/index.html#second-refactor-simplify-the-weight-update",
    "href": "posts/08_nntrain_setup/index.html#second-refactor-simplify-the-weight-update",
    "title": "nntrain: preliminaries (0/n)",
    "section": "Second refactor: simplify the weight update",
    "text": "Second refactor: simplify the weight update\nLet’s try to simplify our training loop, and make it more generic. By adding functionality to our Module class so that it has a reference to it’s trainable parameters, we can update the weights as shown below.\n\ndef fit(epochs):\n    for epoch in range(epochs):\n        for i in range(0,len(x_train), bs):\n            xb = x_train[i:i+bs]\n            yb = y_train[i:i+bs]\n\n            loss = m(xb, yb)\n            m.backward()\n\n            for p in m.parameters():    # model has a reference to the trainable parameters\n                p -= lr * p.g           \n            m.zero_grad()               # model can reset the gradients\n        print(f'{epoch=} | {loss=:.1f}')\n\nTo do so, we will create a new baseclass (NNModule), from which our model and all the layers will inherit. We have the following conditions and properties:\n\nThe class will hold a dictionary _named_args, in which all the named arguments are stored that are set on the Module.\nThis is done by defining a __setattr__ method, which stores any named argument that doesn’t start with an _ in this dictionary\nFor the Linear, these named arguments will be the parameters w and b\nFor the Model, these named arguments will be layers (an array containing the layer objects) and loss containing the MSE object.\nBecause we want to get the parameters directly out of a layer, as well as out of the model, we need to implement some logic in _parameters() to iterate through the lowest “level” and get the actual parameters out\nLast but not least we have to implement a zero_grad() method to zero the gradients on the parameters\n\n\nclass NNModule:\n    def __init__(self):\n        self._named_args = {}                           # [1]\n        \n    def __setattr__(self, name, value):                 # [2]\n        if not name.startswith(\"_\"): self._named_args[name] = value\n        super().__setattr__(name, value)\n        \n    def _parameters(self, obj):                         # [5]\n        for i in obj:\n            if isinstance(i, torch.Tensor): yield i\n            if isinstance(i, NNModule):\n                yield from iter(self._parameters(i._named_args.values()))\n            if isinstance(i, list):\n                yield from iter(self._parameters(i))\n        \n    def parameters(self):\n        return list(self._parameters(self._named_args.values()))\n    \n    def zero_grad(self):\n        for p in self.parameters():\n            p.g = None                                   # [6]\n        \n    def __call__(self, *args):\n        self._args = args                                # NOT stored under _named_args as \\\n        self._out = self.forward(*args)                  # it starts with \"_\"\n        return self._out\n    \n    def backward(self): self._backward(*self._args)\n\n\nclass Linear(NNModule):\n    def __init__(self, n_in, n_out):\n        super().__init__()\n        self.w = torch.randn(n_in, n_out)               # [3] stored under _named_args \n        self.b = torch.zeros(n_out)                     # [3] stored under _named_args\n    \n    def forward(self, x):\n        return x@self.w + self.b\n    \n    def _backward(self, inp):\n        inp.g = self._out.g @ self.w.t()\n        self.w.g = inp.t() @ self._out.g\n        self.b.g = self._out.g.sum(0)\n        \n        \nclass Relu(NNModule):\n    def forward(self, x):\n        return x.clamp_min(0.)\n    \n    def _backward(self, inp):\n        inp.g = (inp&gt;0).float() * self._out.g\n\n    \nclass MSE(NNModule):\n    def forward(self, pred, targ):\n        return (pred.squeeze(-1)-targ).pow(2).mean()\n    \n    def _backward(self, pred, targ):\n        pred.g = 2. * (pred.squeeze() - targ).unsqueeze(-1) / targ.shape[0]\n        \n        \nclass Model(NNModule):\n    def __init__(self, n_in, n_h, n_out):\n        super().__init__()\n        self.layers = [Linear(n_in, n_h), Relu(), Linear(n_h, n_out)]\n        self.loss = MSE()                              # [4] &lt; and ^ are stored under _named_args\n        \n    def forward(self, x, y):\n        for l in self.layers: x = l(x)\n        return self.loss(x, y)\n    \n    def backward(self):\n        self.loss.backward()\n        for l in reversed(self.layers): l.backward()\n\nAnd now we can indeed call parameters on both the model as well as on individual layers:\n\nm = Model(n_in, n_h, n_out)\n[p.shape for p in m.parameters()]\n\n[torch.Size([784, 50]), torch.Size([50]), torch.Size([50, 1]), torch.Size([1])]\n\n\n\n[p.shape for p in Linear(n_in, n_h).parameters()]\n\n[torch.Size([784, 50]), torch.Size([50])]\n\n\nLet’s fit with our new training loop:\n\nfit(5)\n\nepoch=0 | loss=2118316928.0\nepoch=1 | loss=195283376.0\nepoch=2 | loss=18002500.0\nepoch=3 | loss=1659511.5\nepoch=4 | loss=152958.9"
  },
  {
    "objectID": "posts/08_nntrain_setup/index.html#third-refactor-use-nn.module",
    "href": "posts/08_nntrain_setup/index.html#third-refactor-use-nn.module",
    "title": "nntrain: preliminaries (0/n)",
    "section": "Third refactor: use nn.Module",
    "text": "Third refactor: use nn.Module\nFinally we are in a position to use PyTorch’s nn.Module, since we understand all of it’s behavior! We can simplify:\n\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, n_in, n_h, n_out):\n        super().__init__()\n        self.layers = [nn.Linear(n_in, n_h), nn.ReLU(), nn.Linear(n_h, n_out)]\n        for i,l in enumerate(self.layers):               # ^ we use the nn.Linear and nn.ReLU from PyTorch\n            self.add_module(f'layer_{i}', l)             # we need to register the modules explicitly\n        self.loss = nn.MSELoss()                         # we use the MSELoss from PyTorch\n        \n    def forward(self, x, y):\n        for l in self.layers: x = l(x)\n        return self.loss(x.squeeze(-1), y)\n\n\n# Autograd needs all tensors to be float\nx_train = x_train.to(torch.float32)\ny_train = y_train.to(torch.float32)\nm = Model(n_in, n_h, n_out)\n\n\ndef fit(epochs):\n    for epoch in range(epochs):\n        for i in range(0,len(x_train), bs):\n            xb = x_train[i:i+bs]\n            yb = y_train[i:i+bs]\n\n            loss = m(xb, yb)\n            loss.backward()\n\n            with torch.no_grad():\n                for p in m.parameters():\n                    p -= lr * p.grad\n                m.zero_grad()\n        print(f'{epoch=} | {loss=:.1f}')"
  },
  {
    "objectID": "posts/08_nntrain_setup/index.html#fourth-refactor-nn.modulelist-and-nn.sequential",
    "href": "posts/08_nntrain_setup/index.html#fourth-refactor-nn.modulelist-and-nn.sequential",
    "title": "nntrain: preliminaries (0/n)",
    "section": "Fourth refactor: nn.ModuleList and nn.Sequential",
    "text": "Fourth refactor: nn.ModuleList and nn.Sequential\nTo simplify the storing of the layers array and the registration of the modules, we can use nn.ModuleList. Up till now, we compute the loss as part of the forward pass of the model, let’s change that and let the model return the predictions. With these predictions we can now also compute a metric: accuracy, which will represent the percentage of images correctly classified by the model.\n\nclass Model(nn.Module):\n    def __init__(self, n_in, n_h, n_out):\n        super().__init__()\n        self.layers = nn.ModuleList([nn.Linear(n_in, n_h), nn.ReLU(), nn.Linear(n_h, n_out)])\n        \n    def forward(self, x, y):\n        for l in self.layers: x = l(x)\n        return x\n\nThis turns out to be such an elementary operation, that PyTorch has a module for it: nn.Sequential.\n\nimport torch.nn.functional as F\n\n\nlayers = [nn.Linear(n_in,n_h), nn.ReLU(), nn.Linear(n_h, n_out)]\nmodel = nn.Sequential(*layers)\n\nAnd let’s update our training loop as we mentioned:\n\nThe loss needs to be computed separately, since we took it out of the model\nLet’s now also use a loss function that actually makes sense: cross entropy loss instead of MSE\nWe then need to switch back to using 10 output activations conforming with the 10 categories\n\n\nn_out = 10\n\nlayers = [nn.Linear(n_in,n_h), nn.ReLU(), nn.Linear(n_h, n_out)]\nmodel = nn.Sequential(*layers)\n\nLet’s also add a metric: accuracy, to see how our model is doing. For this, we need to find the class that our model predicts. However, the model is outputting not a single class, it outputs logits: the unweighted predictions for any of the 10 classes. When applying a softmax to these logits, we turn them into 10 probabilities: the probability that our model assigns to each class.\nWhen computing the accuracy, we don’t actually just use the logits instead of the probabilities, since the softmax is a monotonically increasing we largest logit, will also have the largest probability.\n\nx0 = x_train[0]\nlogits = model(x0)\n\nprint(f'{logits=}')                           # Logit output of the model\n\nprobs = logits.softmax(dim=0)\n\nprint(f'{probs=}')                            # class probabilites\n\nassert torch.allclose(probs.sum(),            # probabilities sum to 1\n                      torch.tensor(1.0))      \n\nassert torch.all(probs &gt; 0)                   # no negative probabilities\n\nassert (logits.argmax() == probs.argmax())\n\nlogits=tensor([-0.1345,  0.1549, -0.0635,  0.0619,  0.0516, -0.0358,  0.1625, -0.0322,\n        -0.0614,  0.1931], grad_fn=&lt;AddBackward0&gt;)\nprobs=tensor([0.0844, 0.1127, 0.0906, 0.1027, 0.1016, 0.0931, 0.1136, 0.0935, 0.0908,\n        0.1171], grad_fn=&lt;SoftmaxBackward0&gt;)\n\n\n\ndef accuracy(preds, targs):\n    return (preds.argmax(dim=1) == targs).float().mean()\n\n\nloss_func = F.cross_entropy\ny_train = y_train.to(torch.long)\n\nfor epoch in range(epochs):\n    for i in range(0,len(x_train), bs):\n        xb = x_train[i:i+bs]\n        yb = y_train[i:i+bs]\n\n        preds = model(xb)\n        acc = accuracy(preds, yb)\n        loss = loss_func(preds, yb)\n        loss.backward()\n\n        with torch.no_grad():\n            for p in model.parameters():\n                p -= lr * p.grad\n            model.zero_grad()\n    print(f'{epoch=} | {loss=:.3f} | {acc=:.3f}')\n\nepoch=0 | loss=1.071 | acc=0.684\nepoch=1 | loss=0.992 | acc=0.681\nepoch=2 | loss=0.934 | acc=0.688\nepoch=3 | loss=0.889 | acc=0.697\nepoch=4 | loss=0.853 | acc=0.706"
  },
  {
    "objectID": "posts/08_nntrain_setup/index.html#fifth-refactor-add-an-optimizer",
    "href": "posts/08_nntrain_setup/index.html#fifth-refactor-add-an-optimizer",
    "title": "nntrain: preliminaries (0/n)",
    "section": "Fifth refactor: add an Optimizer",
    "text": "Fifth refactor: add an Optimizer\nWe can further refactor the model by adding an Optimizer, this is an object that will have access to the parameters and does the updating of the weights (step) and zeroing the gradient. Most notably, we want to go from:\n\n# ...\n# with torch.no_grad():\n#     for p in model.parameters():\n#         p -= lr * p.grad\n#     model.zero_grad()\n# ...\n\nto:\n\n# opt.step()\n# opt.zero_grad()\n\nSo that the training loop becomes:\n\ndef fit(epochs):\n    for epoch in range(epochs):\n        for i in range(0,len(x_train), bs):\n            xb = x_train[i:i+bs]\n            yb = y_train[i:i+bs]\n\n            preds = model(xb)\n            acc = accuracy(preds, yb)\n            loss = loss_func(preds, yb)\n            loss.backward()\n\n            opt.step()                       # optimizer takes care of the weight update\n            opt.zero_grad()                  # as well as zeroing the grad\n        print(f'{epoch=} | {loss=:.3f} | {acc=:.3f}')\n\nSo we introduce the Optimizer, which has exactly these two methods:\n\nclass Optimizer():\n    def __init__(self, params, lr=0.5):\n        self.params = list(params)\n        self.lr = lr\n        \n    def step(self):\n        with torch.no_grad():\n            for p in self.params: p -= self.lr * p.grad\n        \n    def zero_grad(self):\n        with torch.no_grad():\n            for p in self.params: p.grad.zero_()\n\n\nlayers = [nn.Linear(n_in,n_h), nn.ReLU(), nn.Linear(n_h, n_out)]\nmodel = model = nn.Sequential(*layers)\nopt = Optimizer(model.parameters(), lr)\n\n\nfit(5)\n\nepoch=0 | loss=2.074 | acc=0.447\nepoch=1 | loss=1.832 | acc=0.582\nepoch=2 | loss=1.571 | acc=0.653\nepoch=3 | loss=1.354 | acc=0.676\nepoch=4 | loss=1.195 | acc=0.673\n\n\nThe optimizer we just created is basically the SGD optimizer from PyTorch so let’s use that:\n\ndef get_model():\n    layers = [nn.Linear(n_in, n_h), nn.ReLU(), nn.Linear(n_h, n_out)]\n    model = nn.Sequential(*layers)\n    \n    opt = torch.optim.SGD(model.parameters(), lr)\n    \n    return model, opt\n\nmodel, opt = get_model()\nfit(5)\n\nepoch=0 | loss=2.026 | acc=0.456\nepoch=1 | loss=1.751 | acc=0.559\nepoch=2 | loss=1.502 | acc=0.605\nepoch=3 | loss=1.314 | acc=0.630\nepoch=4 | loss=1.179 | acc=0.635"
  },
  {
    "objectID": "posts/08_nntrain_setup/index.html#end",
    "href": "posts/08_nntrain_setup/index.html#end",
    "title": "nntrain: preliminaries (0/n)",
    "section": "End",
    "text": "End\nWe have come a long way, and covered a lot of ground. We have seen many of the fundamental components of training a neural network: the data, a simple model, training loops, loss functions, metrics and optimizers. We have seen why things like nn.Module exist, and understand it’s behavior. Furthermore, we have seen that the need for nn.Module and torch.optim comes out of the need for simplifying things in the training loop.\nIn the next post, we will get to datasets and dataloaders as a way to further improve the training loop, and we will start adding our first things into the nntrain library 🕺."
  }
]