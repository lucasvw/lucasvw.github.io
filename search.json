[
  {
    "objectID": "posts/02_paperspace_setup/index.html",
    "href": "posts/02_paperspace_setup/index.html",
    "title": "Paperspace setup",
    "section": "",
    "text": "Most people don‚Äôt have a GPU installed in their working machine that is suited for Deep Learning, and in fact you don‚Äôt need to. It‚Äôs quite easy to setup a remote GPU server nowadays, and in this blog I will explain how to do so with Paperspace Gradient.\nI started using Paperspace because of a recommendation from Jeremy Howard in his Live Coding Videos. If you haven‚Äôt seen these lectures, I can highly recommend them. They are a great resource on many things related to getting started with Deep Learning. Jeremy shows a lot of productivity hacks and practical tips on getting a good setup.\nHowever, the Paperspace setup explanations are a bit out-dated which can lead to confusion when following along with the video‚Äôs. Also, after the recording of the videos Jeremy created some nice scripts which simplify the setup. This blog will hopefully help others to navigate this and quickly set-up a remote GPU server. I would advice anybody who wants to try Paperspace, to first watch the videos from Jeremy to have a general idea of how it works, and then follow these steps to quickly get set-up.\nOnce you have signed up to Paperspace, go to their Gradient service and create a new project. Paperspace has a free tier, as well as a pro- ($8/month) and growth-plan ($39/month). I personally signed up for the pro-plan, which has a very good value for money. You get 15Gb persistent storage and free Mid instance types. If available, I use the A4000, which is the fastest and comes with 16GB of GPU memory.\nWith the pro-plan you can create up to 3 servers, or ‚ÄúNotebooks‚Äù as they are called by Paperspace (throughout this blog I‚Äôll refer to them as Notebook Servers). So let‚Äôs create one:"
  },
  {
    "objectID": "posts/02_paperspace_setup/index.html#first-look-at-our-notebook-server",
    "href": "posts/02_paperspace_setup/index.html#first-look-at-our-notebook-server",
    "title": "Paperspace setup",
    "section": "First look at our Notebook Server",
    "text": "First look at our Notebook Server\nNext, let‚Äôs open a terminal and get familiar with our Server\n\n\nTerminal\n\n> which python\n/usr/local/bin/python\n\n> python --version\nPython 3.9.13\n\nAnd let‚Äôs also check the PATH variable:\n\n\nTerminal\n\n> echo $PATH\n/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin: /usr/sbin:/usr/bin:/sbin:/bin:/root/mambaforge/bin\n\nThe python command is thus pointing to the system Python installation. However, on the PATH variable we are also seeing an entry at the end mentioning mambaforge.\nAnd indeed we can execute:\n\n\nTerminal\n\n> mamba list | grep python\n\nipython                   8.5.0              pyh41d4057_1    conda-forge\nipython_genutils          0.2.0                      py_1    conda-forge\npython                    3.10.6          h582c2e5_0_cpython    conda-forge\npython-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge\npython-fastjsonschema     2.16.2             pyhd8ed1ab_0    conda-forge\npython_abi                3.10                    2_cp310    conda-forge\n\nSo we are having both a mamba based Python 3.10.6 and a system installation of Python 3.9.13.\nLet‚Äôs open a Jupyter Notebook and see which Python version is running:\n\n\nUntitled.ipynb\n\nimport sys\nsys.version\n\nWhich returns: '3.9.13 (main, May 23 2022, 22:01:06) \\n[GCC 9.4.0]'. Jupyter is thus running the system Python installation.\n\n\n\n\n\n\nNote\n\n\n\nIn the videos Jeremy mentions that we should never use the system Python but instead always create a Mamba installation. However, since we are working here on a virtual machine that is only used for running Python, this shouldn‚Äôt be a problem. Just be aware that we are using the system Python which is totally separate from the Mamba setup.\n\n\nSince we are running the system Python version, we can inspect all the packages that are installed:\n\n\nTerminal\n\n> pip list\n\n...\nfastai                            2.7.10\nfastapi                           0.92.0\nfastbook                          0.0.28\nfastcore                          1.5.27\nfastdownload                      0.0.7\nfastjsonschema                    2.15.3\nfastprogress                      1.0.3\n...\ntorch                             1.12.0+cu116\ntorchaudio                        0.12.0+cu116\ntorchvision                       0.13.0+cu116\n..."
  },
  {
    "objectID": "posts/02_paperspace_setup/index.html#persisted-storage-at-paperspace",
    "href": "posts/02_paperspace_setup/index.html#persisted-storage-at-paperspace",
    "title": "Paperspace setup",
    "section": "Persisted Storage at Paperspace",
    "text": "Persisted Storage at Paperspace\nIn general, things are not persisted on Paperspace. So anything we store during a session, will be gone when we restart our Notebook Server. However, Paperspace comes with two special folders that are persisted. It‚Äôs important to understand how these folder works since we obviously need to persist our work. Not only that, but we also need to persist our configuration files from services lik GitHub, Kaggle and HuggingFace and potentially any other config files for tools or services we are using.\nThe persisted folders are called /storage and /notebooks. Anything in our /storage is shared among all the Notebook Servers we are running, whereas anything that is stored in the /notebooks folder is only persisted on that specific Notebook Server."
  },
  {
    "objectID": "posts/02_paperspace_setup/index.html#set-up",
    "href": "posts/02_paperspace_setup/index.html#set-up",
    "title": "Paperspace setup",
    "section": "Set up",
    "text": "Set up\nIn the first few videos, Jeremy shows a lot of tricks on how to install new packages and set up things like Git and GitHub. After the recording of these videos, he made a GitHub repo which facilitates this setup greatly and makes most of the steps from the videos unnecessary. So let‚Äôs use that:\n\n\nTerminal\n\n> git clone https://github.com/fastai/paperspace-setup.git\n> cd paperspace-setup\n> ./setup.sh\n\nTo understand what this does, let‚Äôs have a look at setup.sh:\n\n\nsetup.py\n\n#!/usr/bin/env bash\n\nmkdir /storage/cfg\ncp pre-run.sh /storage/\ncp .bash.local /storage/\necho install complete. please start a new instance\n\nFirst it‚Äôs creating a new directory inside of our /storage folder called cfg. As we will see, this is where we will store all our configuration files and folders.\nNext, the script copies 2 files to our storage folder. Let‚Äôs have a closer look at those\n\npre-run.sh\nDuring startup of a Notebook Server (upon creation or restart), Paperspace automatically executes the script it finds at /storage/pre-run.sh. This is really neat, since we can create a script at this location to automate our setup!\nFor the full script, click here, and let‚Äôs have a closer look at this first snippet:\n\n\npre-run.sh (snippet)\n\nfor p in .local .ssh .config .ipython .fastai .jupyter .conda .kaggle\ndo\n        if [ ! -e /storage/cfg/$p ]; then\n                mkdir /storage/cfg/$p\n        fi\n        rm -rf ~/$p\n        ln -s /storage/cfg/$p ~/\ndone\n\nSo we are iterating through a list of folder names (.local .ssh ...) on line 1, and for each one we create a directory inside of /storage/cfg on line 4. We only do this if the directory doesn‚Äôt already exist on line 3. Next, each of these folders is symlinked to the home directory (~/) on line 7.\nThis means that:\n\nWhen we store something in any of these symlinked folders (e.g.¬†~/.local), it‚Äôs actually being written to the associated storage folder (e.g.¬†/storage/cfg/.local) because of the symlink.\nWhenever we restart our Notebook Server, all the stuff that has previously been persisted (e.g.¬†in /storage/cfg/.local) are made available again in the home directory (e.g.¬†~/.local).\n\nThis is very nice, because as it turns out: many tools keep their configuration files in this home folder. So by persisting this data, they will keep working across restarts of our Notebook servers.\nLet‚Äôs a closer look at the folders we are persisting:\n\n.local\nWe saw before that the FastAI runtime comes with a number of installed Python packages. If we want to install additional packages, we could do: pip install <package>. However, pip installs the packages in /usr/local/lib, and are thus not persisted. To make sure our packages are persisted, we can instead install with pip install --user <package>. This --user flag, tells pip to install the package only for the current user, and so it installs into the ~/.local directory. So by persisting this folder, we make sure that we our custom installed python packages are persisted, awesome!\n\n\n.ssh\nTo authenticate with GitHub without using passwords, we use ssh keys. To create a pair of keys, we run: ssh-keygen. This creates the private key (id_rsa) and the public key (id_rsa.pub) to the ~/.ssh folder. Once we upload the public key to GitHub we can authenticate with GitHub, and by persisting this folder we can authenticate upon restart!\nBy now you probably get the idea, any of these folders represent a certain configuration we want to persist:\n\n.conda: contains conda/mamba installed packages\n.kaggle: contains a kaggle.json authentication file\n.fastai: contains downloaded datasets and some other configuration\n.config, .ipython and .jupyter: contain config files for various pieces of software such as matplotlib, ipython and jupyter.\n\nI personally also added .huggingface to this list, to make sure my HuggingFace credentials are also persisted. See here for the PR back into the main repo.\nIn the second part of the script we do exactly the same thing, but for a number of files instead of directories.\n\n\npre-run.sh (snippet)\n\nfor p in .git-credentials .gitconfig .bash_history\ndo\n        if [ ! -e /storage/cfg/$p ]; then\n                touch /storage/cfg/$p\n        fi\n        rm -rf ~/$p\n        ln -s /storage/cfg/$p ~/\ndone\n\nNow that we understand pre-run.sh, let‚Äôs have a look at the second file we store in our /storage folder:\n\n\n\n.bash.local\n\n\n.bash.local\n\n#!/usr/bin/env bash\n\nalias mambai='mamba install -p ~/.conda '\nalias pipi='pip install --user '\n\nexport PATH=~/.local/bin:~/.conda/bin/:$PATH\n\nPaperspace runs this script whenever we open a terminal. As you can see it defines two aliases to easily install things persistently with either mamba (mambai) or pip (pipi).\nAny binaries that are installed this way, are installed in ~/.local/bin (through pip) and to ~/.conda/bin/ (through mamba). We need to add these paths to the PATH variable, to make sure we can call them from the command line.\n\n\nNote on Mamba\nAt this point you might wonder why we have the Mamba installation at all, since we have seen that the system Python is used. In fact, our Mamba environment is totally decoupled from what we are using in our Jupyter notebook, and installing packages through mamba will not make them available in Jupyter. Instead, we should install Python packages through pip.\nSo what do we need Mamba for? I guess Jeremy has done this to be able to install binaries that he wants to use from the Terminal. For example, in the videos he talks about ctags which he installs through mamba. Since installing none-Python specific binaries through pip can be complicated, we can use Mamba instead. In other words, we can use it as a general package manager, somewhat similar to apt-get.\n\n\nFinal words\nIn my opinion Paperspace offers a great product for very fair money, especially if combined with the setup described in this blog!"
  },
  {
    "objectID": "posts/01_blog_setup/index.html",
    "href": "posts/01_blog_setup/index.html",
    "title": "Blog setup",
    "section": "",
    "text": "In this blog post I‚Äôll explain how I created this blog, using Quarto and GitHub. In step 4 I‚Äôll show how to setup GitHub Actions, this has advantages over the other ways to publish our blog:\nI‚Äôm working on a Macbook, and using VS Code for code editing. If you are on a Linux or Windows machine, be aware that things might be a bit different from what I describe here.\nI am assuming you already have a GitHub account, that VS Code is installed and configured to run Python and Jupyter Notebooks."
  },
  {
    "objectID": "posts/01_blog_setup/index.html#step-1-install-quarto",
    "href": "posts/01_blog_setup/index.html#step-1-install-quarto",
    "title": "Blog setup",
    "section": "Step 1: install Quarto",
    "text": "Step 1: install Quarto\nFirst of all you need to install Quarto, go here, download and install the software. You should do this on the machine that you want to use for writing your blog, in my case my Macbook laptop.\nOnce installed you will have access to the quarto Command Line Interface (CLI). To make sure everything works as expected, open a terminal and execute:\n\n\nTerminal\n\nquarto --help\n\nThis should render some outputs describing the different commands and options that are part of the Quarto CLI and shows that Quarto is installed successfully."
  },
  {
    "objectID": "posts/01_blog_setup/index.html#step-2-create-a-github-repo",
    "href": "posts/01_blog_setup/index.html#step-2-create-a-github-repo",
    "title": "Blog setup",
    "section": "Step 2: create a GitHub repo",
    "text": "Step 2: create a GitHub repo\nTo host our blog we will use GitHub Pages, which is a service to host a website from a GitHub repository. Based on the name you pick for your repository you will create a so-called project-website or your unique user-website. For any general repo named my-awesome-repo, the website will be hosted on https://<github-username>.github.io/my-awesome-repo. This is a project-websites and you can create as many as you like.\nTo create your user-website, you have to name the repo exactly like this: <github-username>.github.io, the user-website will be hosted at https://<github-username>.github.io.\nThis is exactly what I want, so I create a new repo with the name: lucasvw.github.io.\nI find it helpful to add a .gitignore file with a Python template, to which we can later add some more entries to facilitate storing the right files on GitHub. Also make sure that the repo is Public (and not set to Private). Additionally, I added a README file and choose the Apache2 License.\nNext, I clone this repo to my machine by running:\n\n\nTerminal\n\ngit clone git@github.com:lucasvw/lucasvw.github.io.git"
  },
  {
    "objectID": "posts/01_blog_setup/index.html#step-3-add-a-quarto-project-to-the-repo",
    "href": "posts/01_blog_setup/index.html#step-3-add-a-quarto-project-to-the-repo",
    "title": "Blog setup",
    "section": "Step 3: add a Quarto project to the repo",
    "text": "Step 3: add a Quarto project to the repo\nNext, open VS Code and open the cloned repo. Then access the VS Code terminal and run:\n\n\nTerminal\n\nquarto create-project --type website:blog\n\nThis will add a number of files to our repo, which represent the basic structure of our blog. Most importantly:\n\nposts: here we will create our blog entries (one subfolder per blog entry)\n_quarto.yml: configuration file for our blog such as the theme, name, GitHub and Twitter links\nabout.qmd: source code for the ‚Äúabout‚Äù page.\nindex.qmd: source code for the landing page.\n\n\n\n\n\n\n\nNote\n\n\n\n.qmd files are like markdown files, but with lots of additional functionality from Quarto. Go here for more information on Markdown syntax and here for Quarto Markdown\n\n\nTo see what we currently have, let‚Äôs render our blog locally:\n\n\nTerminal\n\nquarto preview\n\nAlternatively, we can install the Quarto extension in VS Code, which will show a render button in the top right corner on any opened qmd file.\nTo publish the current contents to GitHub pages, we can run:\n\n\nTerminal\n\nquarto publish gh-pages\n\nWhen doing so, we get a message that we have to change the branch from which GitHub Pages builds the site. To do this, I go to https://github.com/lucasvw/lucasvw.github.io/settings/pages and select gh-pages instead of the main branch.\nAnd voila, in a few moments our blog will be running live at https://lucasvw.github.io/"
  },
  {
    "objectID": "posts/01_blog_setup/index.html#step-4-finalize-set-up-github-actions",
    "href": "posts/01_blog_setup/index.html#step-4-finalize-set-up-github-actions",
    "title": "Blog setup",
    "section": "Step 4: Finalize set-up: GitHub Actions",
    "text": "Step 4: Finalize set-up: GitHub Actions\nWhen we run the quarto publish gh-pages command, Quarto processes our files and turns them into web readable files (HTML, JS, CSS etc). It stores these files in our gh-pages branch and pushes them to our remote GitHub repo. This is great, but it means that this doesn‚Äôt store our source files.\nTo do so, let‚Äôs first open our .gitignore file and make sure that it contains the following entries so that we don‚Äôt check in any files we don‚Äôt need.\n\n\n.gitignore\n\n# Quarto\n/.quarto/\n_site/\n\n# Mac files\n.DS_Store\n\nNext, we can commit all the remaining files to Git and push them to our remote repo. If we ever lose access to our local machine, we can restore everything we need from GitHub.\nHowever, now we have 2 things we need to do whenever we finish our work:\n\nstore our source files on the main branch and push to GitHub\nrun the publish command to update the blog\n\nThis is a bit annoying and it would be much better if we can just push to the main branch and GitHub would take care of building our website and updating it. This also allows us to create blog entries on any machine that has access to git, we don‚Äôt need to have quarto installed. This is particularly practical if we want to write blog entries from our deep learning server. So let‚Äôs use GitHub actions for this.\n\n\n\n\n\n\nNote\n\n\n\nBefore you continue make sure you have at least once run a quarto publish gh-pages command, this is necessary for the things below to work\n\n\nFirst we need to add the following snippet to _quarto.yml\n\n\n_quarto.yml\n\nexecute:\n  freeze: auto\n\nThis will make sure that GitHub actions doesn‚Äôt execute any executable code, but will show the pre-rendered outputs it finds in the _freeze folder.\nFinally, create the file .github/workflows/publish.yml and populate it with the following code:\n\n\n.github/workflows/publish.yml\n\non:\n  workflow_dispatch:\n  push:\n    branches: main\n\nname: Quarto Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v3\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n\n      - name: Render and Publish\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\nOnce we push these things to GitHub, we are good to go. Whenever we push anything to the main branch, this workflow will execute and take care of updating the gh-pages branch and updating the blog."
  },
  {
    "objectID": "posts/03_aiornot/index.html",
    "href": "posts/03_aiornot/index.html",
    "title": "First competitionüèÖ",
    "section": "",
    "text": "In the past couple of weeks I have participated in the first ever Hugging Face competition: aiornot. And as a matter of fact, it was also my first competition to participate in! The competition consisted of 62060 images (18618 train and 43442 test images) which were either created by an AI or not (binary image classification).\nToday, the competition has finished and the private leaderboard has been made public. I‚Äôm super happy (and proud üòá) that I finished in 15th place (98 participants):"
  },
  {
    "objectID": "posts/03_aiornot/index.html#credit-where-credit-is-due",
    "href": "posts/03_aiornot/index.html#credit-where-credit-is-due",
    "title": "First competitionüèÖ",
    "section": "Credit where credit is due:",
    "text": "Credit where credit is due:\n\nü§ó Hugging Face\nI would like to thank Hugging Face and in particular Abhishek Thakur for organizing this competition. I started looking for a first competition at Kaggle a few weeks back, and was very interested in the RSNA competition but quickly found that it was probably a bit too complicated for my first competition. I then saw a tweet from Abhishek announcing this competition and found it a perfect competition to get started.\n\n\nfastai\nIn the past month I have been following the fastai course and I am extremely grateful to Jeremy Howard and Sylvain Gugger for creating fastai. The book, the course, the videos and the great community they have built is really something special and is perfectly tailored for anybody who wants to get started with Deep Learning. Without fastai I could never have pulled this off üôè."
  },
  {
    "objectID": "posts/03_aiornot/index.html#learnings-and-notes",
    "href": "posts/03_aiornot/index.html#learnings-and-notes",
    "title": "First competitionüèÖ",
    "section": "Learnings and notes",
    "text": "Learnings and notes\n\nI quickly learned that data augmentation didn‚Äôt work well on this data. Initially I was a bit surprised by this, but upon inspection of the images I arrived at the following intuition. Normally we want to classify images by what‚Äôs being displayed in the image. So 2 images of a bike should both be classified as such. However, in this dataset we can have images of the same object but if one is created by an AI, and the other is not then they should be classified differently. So instead of looking at what‚Äôs being displayed, it probably has to learn more about the style or the way the image is built up. I can imagine that data augmentation makes this more difficult, especially warping, affine transformations and brightness, contrast augmentations. I was happily surprised to find that the 2nd and 4th place solutions also didn‚Äôt use these data augmentation!\nTraining on larger images works very well. I got a large performance boost for switching to sizes of 416. Jeremy Howard mentioned that this generally works well, and I think because of the nature of these images it worked especially well. To train large models on large images, I heavily relied on Gradient Accumulation to not have to reduce the batchsize.\nTransformer based models such as SWIN and VIT performed not as good as models based on convolutions, I used the convnext models.\nProgressive resizing didn‚Äôt work for me.\nI tried training on 5 epochs and 10 epochs. 10 epochs never gave me better results.\n\nLast but not least:\nParticipating in competitions is very motivating and rewarding. Working individually through courses, exercises and lecture notes is very interesting, but you don‚Äôt get a lot of feedback to how you are doing. Am I doing well? Should I spend more time on investigations into certain areas? When participating in a real-world competition you have a very clear goal, and you get immediate feedback on how you are doing. This type of project based learning has the advantage that it‚Äôs very clear what you need to focus on: anything that you encounter during the project.\nIt‚Äôs also great that it has a finite timeline, so that afterwards you can have a sense of achievement which motivates a lot. The Germans have a very nice word for this: Erfolgserlebnis.\n\n\n\nImage for Stable Diffusion prompt: ‚ÄúSense of achievement when finishing my first ever machine learning competition‚Äù"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Helloüëã and welcome to my blog!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Another Machine Learning Blog",
    "section": "",
    "text": "First competitionüèÖ\n\n\n\n\n\n\n\nDeep Learning\n\n\nImage Classification\n\n\nCompetition\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nPaperspace setup\n\n\n\n\n\n\n\nsetup\n\n\npaperspace\n\n\nGPU\n\n\nhow-to\n\n\nMLOps\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nBlog setup\n\n\n\n\n\n\n\nblogging\n\n\nsetup\n\n\nhow-to\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\nNo matching items"
  }
]