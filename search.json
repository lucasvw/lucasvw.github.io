[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Helloüëã and welcome to my blog!"
  },
  {
    "objectID": "posts/10_nntrain_learner/index.html",
    "href": "posts/10_nntrain_learner/index.html",
    "title": "nntrain (2/n): Learner",
    "section": "",
    "text": "In this series, I want to discuss the creation of a small PyTorch based library for training neural networks: nntrain. It‚Äôs based off the excellent part 2 of Practical Deep Learning for Coders by Jeremy Howard, in which from lessons 13 to 18 (roughly) the development of the miniai library is discussed.\nWe‚Äôll try to build everything as much as possible from scratch to understand how things work. Once the main functionality of components is implemented and verified, we can switch over to PyTorch‚Äôs version. This is similar to how things are done in the course. However, this is not just a ‚Äúcopy / paste‚Äù of the course: on many occasions I take a different route, and most of the code is my own. That is not to say that all of this is meant to be extremely innovative, instead I had the following goals:\nnb_dev is another great project from the fastai community, which allows python libraries to be written in jupyter notebooks. This may sound a bit weird and controversial, but it has the advantage that we can create the source code for our library in the very same environment in which we want to experiment and interact with our methods, objects and structure while we are building the library. For more details on why this is a good idea and other nice features of nb_dev, see here.\nSo without further ado, let‚Äôs start with where we left off in the previous post:"
  },
  {
    "objectID": "posts/10_nntrain_learner/index.html#end-of-last-post",
    "href": "posts/10_nntrain_learner/index.html#end-of-last-post",
    "title": "nntrain (2/n): Learner",
    "section": "End of last post:",
    "text": "End of last post:\nWe finished the last post with exporting the dataloaders module into the nntrain library, which helps transforming a huggingface dataset dictionary into PyTorch dataloaders, so let‚Äôs use that:\n\nfrom datasets import load_dataset,load_dataset_builder\n\nfrom nntrain.dataloaders import DataLoaders, hf_ds_collate_fn\n\n\n #| export\nimport torchvision.transforms.functional as TF\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom operator import attrgetter\nimport fastcore.all as fc\nimport math\nfrom fastprogress import progress_bar,master_bar\nimport torcheval.metrics as tem\nimport matplotlib.pyplot as plt\n\n\nname = \"fashion_mnist\"\nds_builder = load_dataset_builder(name)\nhf_dd = load_dataset(name)\n\nbs = 1024\ndls = DataLoaders.from_hf_dd(hf_dd, batch_size=bs)\n\n# As a reminder, `DataLoaders` expose a PyTorch train and validation dataloader as `train` and `valid` attributes:\n\ndls.train, dls.valid\n\nReusing dataset fashion_mnist (/root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1)\n\n\n\n\n\n(&lt;torch.utils.data.dataloader.DataLoader&gt;,\n &lt;torch.utils.data.dataloader.DataLoader&gt;)"
  },
  {
    "objectID": "posts/10_nntrain_learner/index.html#learner-class",
    "href": "posts/10_nntrain_learner/index.html#learner-class",
    "title": "nntrain (2/n): Learner",
    "section": "Learner Class",
    "text": "Learner Class\nLet‚Äôs continue to formalize our training loop into a Learner class with a fit() method. The training loop created so far looks like this:\ndef fit(epochs):\n    for epoch in range(epochs):\n        model.train()                                       \n        n_t = train_loss_s = 0                              \n        for xb, yb in dls.train:\n            preds = model(xb)\n            train_loss = loss_func(preds, yb)\n            train_loss.backward()\n            \n            n_t += len(xb)\n            train_loss_s += train_loss.item() * len(xb)\n            \n            opt.step()\n            opt.zero_grad()\n        \n        model.eval()                                        \n        n_v = valid_loss_s = acc_s = 0                      \n        for xb, yb in dls.valid: \n            with torch.no_grad():                           \n                preds = model(xb)\n                valid_loss = loss_func(preds, yb)\n                \n                n_v += len(xb)\n                valid_loss_s += valid_loss.item() * len(xb)\n                acc_s += accuracy(preds, yb) * len(xb)\n        \n        train_loss = train_loss_s / n_t                     \n        valid_loss = valid_loss_s / n_v\n        acc = acc_s / n_v\n        print(f'{epoch=} | {train_loss=:.3f} | {valid_loss=:.3f} | {acc=:.3f}')\nLet‚Äôs build this class in steps. Initialization is straight forward:\n\nclass Learner():\n    def __init__(self, model, dls, loss_fn, metric_fn, optim_class, lr):\n        self.model = model\n        self.dls = dls\n        self.loss_fn = loss_fn\n        self.metric_fn = metric_fn\n        self.optim = optim_class(model.parameters(), lr)\n\nNext, let‚Äôs implement the training loop. Instead of writing the full loop in a single fit() method, let‚Äôs try to break the training loop down into pieces:\n\nfit() iterates through the epochs\nPer epoch we do a training round through the training data and an evaluation round through the validation set. Both rounds are quite similar, so let‚Äôs put this functionality in a separate method one_epoch()\nIn each epoch we iterate through the batches of a dataloader, let‚Äôs put this functionality in a method one_batch()\n\nlet‚Äôs define the outer most call: fit(). In this method we‚Äôll call one_epoch twice, once for the training and once for the validation. Both passes are fairly similar as can be seen from comparing lines 3-8 with 16-21\n\n@fc.patch\ndef fit(self:Learner, epochs):\n    for epoch in range(epochs):                # iterate through the epochs\n        self.one_epoch(epoch, train=True)      # one epoch through the training dataloader\n        with torch.no_grad():                  # for the validation epoch we don't need grads\n            self.one_epoch(epoch, train=False) # one epoch through the validation dataloader\n\nNext, let‚Äôs implement one_epoch(). To make sure each method does one thing, we factor do_batch() out into it‚Äôs own method:\n\n@fc.patch\ndef one_epoch(self:Learner, epoch, train):\n    self.reset_stats()                         # reset the stats at beginning of each epoch\n    self.model.train(train)                    # put the model either in train or validation mode\n    self.dl = self.dls.train if train else self.dls.valid # reference to the active dataloader\n    for self.batch in self.dl:                 # iterate through the active dataloader\n        self.one_batch(train)                  # do one batch\n    self.print_stats(epoch, train)             # print stats at the end of the epoch\n\nAnd finally the method responsible for dealing with a single batch of data:\n\n@fc.patch\ndef one_batch(self:Learner, train):\n    self.xb, self.yb = self.batch              # self.batch is either a training or validation batch\n    self.preds = self.model(self.xb)           # forward pass through the model\n    self.loss = self.loss_fn(self.preds, self.yb)  # loss\n    if train:                                  # only do a backward and weight update if train\n        self.loss.backward()\n        self.optim.step()\n        self.optim.zero_grad()\n    self.update_stats()                        # update stats\n\nAnd the methods related to the computation of the statistics:\n\n@fc.patch\ndef update_stats(self:Learner):\n    n = len(self.xb)\n    self.loss_s += self.loss.item() * n\n    self.metric_s += self.metric_fn(self.preds, self.yb).item() * n\n    self.counter += n\n\n@fc.patch\ndef reset_stats(self:Learner):\n    self.counter = 0\n    self.loss_s = 0\n    self.metric_s = 0\n\n@fc.patch\ndef print_stats(self:Learner, epoch, train):\n    loss = self.loss_s / self.counter\n    metric = self.metric_s / self.counter\n    print(f'{epoch=:02d} | {\"train\" if train else \"eval\":&lt;5} | {loss=:.3f} | {metric=:.3f}')\n\nLet‚Äôs try it out on the data:\n\nn_in = 28*28\nn_h = 50\nn_out = 10\nlr = 0.01\n\ndef accuracy(preds, targs):\n    return (preds.argmax(dim=1) == targs).float().mean()\n\ndef get_model():\n    layers = [nn.Linear(n_in, n_h), nn.ReLU(), nn.Linear(n_h, n_out)]\n    return nn.Sequential(*layers)\n\nl = Learner(get_model(), dls, F.cross_entropy, accuracy, torch.optim.SGD, lr)\n\nl.fit(5)\n\nepoch=00 | train | loss=2.202 | metric=0.240\nepoch=00 | eval  | loss=2.078 | metric=0.378\nepoch=01 | train | loss=1.943 | metric=0.464\nepoch=01 | eval  | loss=1.802 | metric=0.538\nepoch=02 | train | loss=1.667 | metric=0.578\nepoch=02 | eval  | loss=1.543 | metric=0.601\nepoch=03 | train | loss=1.435 | metric=0.628\nepoch=03 | eval  | loss=1.346 | metric=0.639\nepoch=04 | train | loss=1.266 | metric=0.652\nepoch=04 | eval  | loss=1.207 | metric=0.651"
  },
  {
    "objectID": "posts/10_nntrain_learner/index.html#callbacks-pubsub-and-event-handlers",
    "href": "posts/10_nntrain_learner/index.html#callbacks-pubsub-and-event-handlers",
    "title": "nntrain (2/n): Learner",
    "section": "Callbacks, pubsub and event handlers",
    "text": "Callbacks, pubsub and event handlers\nOn the one side we want to keep the Learner and its training loop generic on the other side we need to be able to tweak the dynamics of the training loop depending on the use-case. One way to customize the training loop, without having to re-write the training loop would be to add a publish/subscribe (pubsub) mechanism. In the FastAI course, they are referred to as ‚Äúcallbacks‚Äù, and although callbacks, event handlers and pubsub are all related and basically refer to any logic (encapsulated in a function) which we want to specify now, and execute at a later point in time whenever some condition arises. In my vocabulary a callback is a function that is passed to another function, and is executed whenever that other function is finished.\nFor the purposes of training neural networks we have the following requirements:\n\nThe Learner framework defines a number of ‚Äúevents‚Äù that are published:\n\nbefore_fit, after_fit\nbefore_epoch, after_epoch\nbefore_batch, after_batch\n\nSubscribers are classes that implement methods (e.g.¬†before_fit()) that will be triggered whenever the associated event is published. They also have an order attribute which determines the order in which they are called in case multiple Subscribers subscribed to the same event.\nAs an additional feature, subscribers will be able to redirect flow, but we will come back to that later\n\nSo let‚Äôs implement this. First, we will need to store a list of subscribers in the Learner class:\n\nclass Learner():\n    def __init__(self, model, dls, loss_fn, metric_fn, optim_class, lr, subs):\n        self.model = model\n        self.dls = dls\n        self.loss_fn = loss_fn\n        self.metric_fn = metric_fn\n        self.optim = optim(model.parameters(), lr)\n        self.subs = subs\n\nNext, let‚Äôs define a method for publishing events. The method will go through the registered subscribers and if a method with the name of the event is declared, call that method passing the learner object as an argument:\n\n@fc.patch\ndef publish(self:Learner, event):\n    for sub in sorted(self.subs, key=attrgetter('order')):\n        method = getattr(sub, name, None)\n        if method is not None: method(self)\n\nWith the before_x / after_x events, realize that we have three times the same construct:\npublish \"before_event\" event\ndo event\npublish \"after_event\" event\nWith event being either fit, epoch or batch. So instead of adding this construct multiple times in the training loop let‚Äôs define a class we can use as a decorater wrapping the do_event logic:\n\n #| export\n\nclass PublishEvents():\n    def __init__(self, event): \n        self.event = event\n    \n    def __call__(self, decorated_fn):\n        def decorated_fn_with_publishing(learner, *args, **kwargs):\n            learner.publish(f'before_{self.event}')\n            decorated_fn(learner, *args, **kwargs)\n            learner.publish(f'after_{self.event}')\n        return decorated_fn_with_publishing\n\nTo implement this into the Learner we have to factor out the exact code we want to be executed in between the publishing of the before and after, see the additional _one_epoch() method.\nNote that we are taking out the logic concerning the statistics, this will be implemented as a Subscriber as we‚Äôll see.\n\nclass Learner():\n    def __init__(self, model, dls, loss_fn, metric_fn, optim_class, lr, subs):\n        self.model = model\n        self.dls = dls\n        self.loss_fn = loss_fn\n        self.metric_fn = metric_fn\n        self.optim = optim_class(model.parameters(), lr)\n        self.subs = subs\n    \n    @PublishEvents('fit')\n    def fit(self, epochs):\n        for epoch in range(epochs):\n            self.one_epoch(epoch, train=True)\n            with torch.no_grad():\n                self.one_epoch(epoch, train=False)\n\n    def one_epoch(self, epoch, train):\n        self.model.train(train)\n        self.dl = self.dls.train if train else self.dls.valid\n        self._one_epoch(epoch, train)\n        \n    @PublishEvents('epoch')\n    def _one_epoch(self, epoch, train):\n        for self.batch in self.dl:\n            self.xb, self.yb = self.batch\n            self.one_batch(train)\n    \n    @PublishEvents('batch')\n    def one_batch(self, train):\n        self.preds = self.model(self.xb)           \n        self.loss = self.loss_fn(self.preds, self.yb)\n        if train:                                  \n            self.loss.backward()\n            self.optim.step()\n            self.optim.zero_grad()\n        \n    def publish(self, event):\n        for sub in sorted(self.subs, key=attrgetter('order')):\n            method = getattr(sub, event, None)\n            if method is not None: method(self)            \n\nLet‚Äôs create a dummy subscriber and test it out:\n\n #| export\nclass Subscriber():\n    order = 0\n\nclass DummyS(Subscriber):\n    \n    def before_fit(self, learn):\n        print('before fitüëã')\n        \n    def after_fit(self, learn):\n        print('after fitüëã')\n        \n    def before_epoch(self, learn):\n        print('before epoch üí•')\n        \n    def after_epoch(self, learn):\n        print('after epoch üí•')\n\n\nl = Learner(get_model(), dls, F.cross_entropy, accuracy, torch.optim.SGD, lr, [DummyS()])\nl.fit(1)\n\nbefore fitüëã\nbefore epoch üí•\nafter epoch üí•\nbefore epoch üí•\nafter epoch üí•\nafter fitüëã"
  },
  {
    "objectID": "posts/10_nntrain_learner/index.html#subscribers-can-cancel-execution",
    "href": "posts/10_nntrain_learner/index.html#subscribers-can-cancel-execution",
    "title": "nntrain (2/n): Learner",
    "section": "Subscribers can cancel execution",
    "text": "Subscribers can cancel execution\nNow let‚Äôs add the last component of our pubsub system: subscribers should be able to cancel processing. For example, a a subscriber that would implement Early Stopping, will have to be able to cancel any further epochs when the validation loss starts increasing. One way to implement this, is with the help of Exceptions and try / except blocks:\nIt‚Äôs actually very easy to implement this logic, we only need to define custom Exceptions, and update the PublishEvents class to catch the exceptions that are thrown in any subscriber:\n\nclass CancelFitException(Exception): pass\nclass CancelEpochException(Exception): pass\nclass CancelBatchException(Exception): pass\n\n\nclass PublishEvents():\n    def __init__(self, name): \n        self.name = name\n    \n    def __call__(self, decorated_fn):\n        def decorated_fn_with_publishing(learner, *args, **kwargs):\n            try:\n                learner.publish(f'before_{self.name}')\n                decorated_fn(learner, *args, **kwargs)\n                learner.publish(f'after_{self.name}')\n            except globals()[f'Cancel{self.name.title()}Exception']: pass\n        return decorated_fn_with_publishing\n\n\nclass DummyS(Subscriber):\n    \n    def before_fit(self, learn): print('before fitüëã')\n        \n    def before_epoch(self, learn): raise CancelFitException\n    \n    def after_fit(self, learn): print('after fit üëã')\n\n\nl = Learner(get_model(), dls, F.cross_entropy, accuracy, torch.optim.SGD, lr, [DummyS()])\nl.fit(5)\n\nbefore fitüëã\n\n\nAnd indeed, the after_fit event is never called, since the fit was cancelled during before_epoch by the dummy subscriber"
  },
  {
    "objectID": "posts/10_nntrain_learner/index.html#final-version-of-learner",
    "href": "posts/10_nntrain_learner/index.html#final-version-of-learner",
    "title": "nntrain (2/n): Learner",
    "section": "Final version of Learner",
    "text": "Final version of Learner\nWe are going to make some final changes to the Learner class:\n\nfactor out the computation of the following logic. This is practical to create subclasses of Learner with custom behavior:\n\nprediction: self.predict()\nloss: self.get_loss()\nbackward pass: self.backward()\nstepping of weights: self.step()\nzeroing of gradients: self.zero_grad()\n\nadd a Subscriber argument to fit, these subs will only be added for the duration of the fit, and afterwards removed\nadd a couple of additional events (after_predict, after_loss, after_backward and after_step) to which subscribers can listen\n\n\n #| export\nclass Learner():\n    def __init__(self, model, dls, loss_fn, optim_class, lr, subs):\n        self.model = model\n        self.dls = dls\n        self.loss_fn = loss_fn\n        self.optim_class = optim_class\n        self.lr = lr\n        self.subs = subs\n    \n    def fit(self, epochs, train=True, valid=True, subs=[], lr=None):\n        for sub in subs: self.subs.append(sub)\n        self.n_epochs = epochs\n        self.epochs = range(self.n_epochs)\n        lr = self.lr if lr is None else lr\n        self.opt = self.optim_class(self.model.parameters(), lr)\n        try:\n            self._fit(train, valid)\n        finally:\n            for sub in subs: self.subs.remove(sub)\n                    \n    @PublishEvents('fit')\n    def _fit(self, train, valid):\n        for self.epoch in self.epochs:\n            if train: \n                self.one_epoch(True)\n            if valid:\n                with torch.no_grad():\n                    self.one_epoch(False)\n        \n    def one_epoch(self, train):\n        self.model.train(train)\n        self.dl = self.dls.train if train else self.dls.valid\n        self._one_epoch()\n        \n    @PublishEvents('epoch')\n    def _one_epoch(self):\n        for self.batch in self.dl: \n            self.one_batch()\n    \n    @PublishEvents('batch')\n    def one_batch(self):\n        self.predict()\n        self.publish('after_predict')\n        self.get_loss()\n        self.publish('after_loss')\n        if self.model.training:\n            self.backward()\n            self.publish('after_backward')\n            self.step()\n            self.publish('after_step')\n            self.zero_grad()\n        \n    def publish(self, event):\n        for sub in sorted(self.subs, key=attrgetter('order')):\n            method = getattr(sub, event, None)\n            if method is not None: method(self)\n            \n    def predict(self): \n        self.preds = self.model(self.batch[0])\n        \n    def get_loss(self): \n        self.loss = self.loss_fn(self.preds, self.batch[1])\n        \n    def backward(self): self.loss.backward()\n    def step(self): self.opt.step()\n    def zero_grad(self): self.opt.zero_grad()"
  },
  {
    "objectID": "posts/10_nntrain_learner/index.html#metrics-subscriber",
    "href": "posts/10_nntrain_learner/index.html#metrics-subscriber",
    "title": "nntrain (2/n): Learner",
    "section": "Metrics Subscriber",
    "text": "Metrics Subscriber\nSince we took out the metrics, let‚Äôs create a subscriber that adds that. We want the subscriber to be generic, to it should be able to accept one or multiple metrics. Let‚Äôs make sure that it can accept the metrics from the torcheval library:\n\nmetric = tem.Mean()\n\nmetric.update(torch.tensor([1,2,3]))  # update() adds data\nmetric.update(torch.tensor([4,5,6]))  \nprint(metric.compute())               # compute() computes the metric\n\nmetric.reset()                        # remove all data\nprint(metric.compute())\n\nWARNING:root:No calls to update() have been made - returning 0.0\n\n\ntensor(3.5000, dtype=torch.float64)\ntensor(0., dtype=torch.float64)\n\n\n\n #|export\nclass MetricsS(Subscriber):\n    def __init__(self, **metrics):\n        self.metrics = metrics\n        self.loss = tem.Mean()\n        \n    def before_fit(self, learn): \n        learn.metrics = self\n    \n    def before_epoch(self, learn):\n        for m in self.metrics.values(): m.reset()\n        self.loss.reset()\n    \n    def after_batch(self, learn):\n        x,y,*_ = self.to_cpu(learn.batch)\n        for m in self.metrics.values(): m.update(self.to_cpu(learn.preds), y)\n        self.loss.update(self.to_cpu(learn.loss), weight=len(x))\n        \n    def after_epoch(self, learn):\n        log = {\n            'epoch': learn.epoch,\n            'mode': 'train' if learn.model.training else 'eval',\n            'loss' : f'{self.loss.compute():.3f}'\n        }\n        for k, v in self.metrics.items():\n            log[k] = f'{v.compute():.3f}'\n        self.output(log)\n        \n    def to_cpu(self, x):\n        if isinstance(x, list): return (self.to_cpu(el) for el in x)\n        return x.detach().cpu()\n        \n    def output(self, log): print(log)\n\n\nmetrics_s = MetricsS(accuracy=tem.MulticlassAccuracy())\n\nl = Learner(get_model(), dls, F.cross_entropy, torch.optim.SGD, lr, [metrics_s])\nl.fit(1)\n\n{'epoch': 0, 'mode': 'train', 'loss': '2.220', 'accuracy': '0.206'}\n{'epoch': 0, 'mode': 'eval', 'loss': '2.121', 'accuracy': '0.352'}"
  },
  {
    "objectID": "posts/10_nntrain_learner/index.html#device-subscriber",
    "href": "posts/10_nntrain_learner/index.html#device-subscriber",
    "title": "nntrain (2/n): Learner",
    "section": "Device Subscriber",
    "text": "Device Subscriber\nIt‚Äôs time we start training on the GPU, to do that we have to move the model (and it‚Äôs parameters) as well as all the data onto the GPU. We can easily do this with a Subscriber:\n\nmove the model (and all it‚Äôs trainable parameters) to the device before fit\nmove each batch to the device before batch\n\n\n #| export\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \nclass DeviceS(Subscriber):\n    \n    def __init__(self, device):\n        self.device = device\n    \n    def before_fit(self, learn):\n        learn.model.to(self.device)\n    \n    def before_batch(self, learn):\n        learn.batch = [x.to(self.device) for x in learn.batch]\n\n\ndevice_s = DeviceS(device)\n\nl = Learner(get_model(), dls, F.cross_entropy, torch.optim.SGD, lr, [metrics_s, device_s])\nl.fit(1)\n\n{'epoch': 0, 'mode': 'train', 'loss': '2.209', 'accuracy': '0.256'}\n{'epoch': 0, 'mode': 'eval', 'loss': '2.115', 'accuracy': '0.306'}"
  },
  {
    "objectID": "posts/10_nntrain_learner/index.html#learning-rate-finder",
    "href": "posts/10_nntrain_learner/index.html#learning-rate-finder",
    "title": "nntrain (2/n): Learner",
    "section": "Learning Rate Finder",
    "text": "Learning Rate Finder\nThe learning rate finder is very simple technique that can be used to find a good learning rate for training a network. It works like this:\n\nStart with a very small learning rate\nDo a forward pass through the network of a single batch of data and record the loss\nIncrease the learning rate with constant factor\nDo another forward pass through the network of a single batch and record the loss\nContinue to do this until at some point the loss ‚Äúexplodes‚Äù: for example because the current loss is 3 times as large as the minimum loss recorded so far\n\nAfter this, we plot the learning rate vs the recorded losses and look for a learning rate at which the loss is decreasing the most (i.e.¬†the point where the loss has the smallest derivative).\n\n #| export\nclass LRFindS(Subscriber):\n    \n    def __init__(self, mult=1.25):\n        self.mult = mult\n        self.min = math.inf\n        \n    def before_epoch(self, learn):\n        if not learn.model.training: raise CancelFitException\n        self.losses = []\n        self.lrs = []\n    \n    def after_loss(self, learn):\n        lr = learn.opt.param_groups[0]['lr']\n        self.lrs.append(lr)\n        loss = learn.loss.detach().cpu()\n        self.losses.append(loss)\n        if loss &lt; self.min: self.min = loss\n        if loss &gt; self.min*3: raise CancelFitException()\n        for g in learn.opt.param_groups: g['lr'] = lr * self.mult\n        \n    def plot(self):\n        plt.plot(self.lrs, self.losses)\n        plt.xscale('log')\n\n\nl = Learner(get_model(), dls, F.cross_entropy, torch.optim.SGD, lr, [metrics_s, device_s])\n\nlrfind_s = LRFindS()\nl.fit(5, lr=1e-4, subs=[lrfind_s])\n\n\nlrfind_s.plot()\n\n\n\n\nFrom which we see that a learning rate of around 0.2 would be best"
  },
  {
    "objectID": "posts/10_nntrain_learner/index.html#momentumlearner",
    "href": "posts/10_nntrain_learner/index.html#momentumlearner",
    "title": "nntrain (2/n): Learner",
    "section": "MomentumLearner",
    "text": "MomentumLearner\nAdditionally, we can easily subclass Learner, and implement custom functionality into any of its 5 main functionalities:\n\nprediction: self.predict()\nloss: self.get_loss()\nbackward pass: self.backward()\nstepping of weights: self.step()\nzeroing of gradients: self.zero_grad()\n\nFor example we can create a MomentumLearner which doesn‚Äôt just use the gradient of the last backward pass, but uses an exponentially weighted average of all previously computed gradients. We can do this by not zeroing out the gradients, but just reduce them by a factor between 0 and 1 (the momentum parameter). This way the ‚Äúgradient with momentum‚Äù at time \\(t\\) (\\(m_t\\)), will be a function of the normal gradient (\\(g_t\\)):\n\\[\nm_t = g_t + c \\cdot g_{t-1} + c^2 \\cdot g_{t-2} + ...\n\\]\nThis is called momentum and the idea is to add a sense of ‚Äúinertia‚Äù to the gradients, i.e.¬†if in one step we are moving through the loss manifold in a certain direction, then in the next step we want to keep moving somewhat in that direction irrespective of the gradient of the current step.\n\n #| export\nclass MomentumLearner(Learner):\n    \n    def __init__(self, model, dls, loss_fn, optim_class, lr, subs, mom=0.85):\n        self.mom = mom\n        super().__init__(model, dls, loss_fn, optim_class, lr, subs)\n        \n    def zero_grad(self):\n        with torch.no_grad():\n            for p in self.model.parameters(): p.grad *= self.mom\n\n\nl = MomentumLearner(get_model(), dls, F.cross_entropy, torch.optim.SGD, lr, [metrics_s, device_s])\nl.fit(5)\n\n{'epoch': 0, 'mode': 'train', 'loss': '1.657', 'accuracy': '0.481'}\n{'epoch': 0, 'mode': 'eval', 'loss': '1.105', 'accuracy': '0.647'}\n{'epoch': 1, 'mode': 'train', 'loss': '0.925', 'accuracy': '0.685'}\n{'epoch': 1, 'mode': 'eval', 'loss': '0.831', 'accuracy': '0.695'}\n{'epoch': 2, 'mode': 'train', 'loss': '0.763', 'accuracy': '0.732'}\n{'epoch': 2, 'mode': 'eval', 'loss': '0.736', 'accuracy': '0.739'}\n{'epoch': 3, 'mode': 'train', 'loss': '0.686', 'accuracy': '0.764'}\n{'epoch': 3, 'mode': 'eval', 'loss': '0.675', 'accuracy': '0.766'}\n{'epoch': 4, 'mode': 'train', 'loss': '0.635', 'accuracy': '0.784'}\n{'epoch': 4, 'mode': 'eval', 'loss': '0.634', 'accuracy': '0.779'}\n\n\nAnd this simple technique has a pretty good effect on training our model: the accuracy on the validation set is increasing (even with this simple linear model) from 66% with normal SGD to 78% with momentum."
  },
  {
    "objectID": "posts/10_nntrain_learner/index.html#closing-remarks",
    "href": "posts/10_nntrain_learner/index.html#closing-remarks",
    "title": "nntrain (2/n): Learner",
    "section": "Closing remarks",
    "text": "Closing remarks\nIn this post we have again covered a lot of ground. We have created a very flexible Learner framework, making heavy use of a pubsub system to customize the training loop. As examples we have seen a Subscriber that enables training on the GPU, and another one that takes care of tracking the loss and the metrics while we are training. Additionally we have implement the learning rate finder as a Subscriber, and last but not least we have seen how we can subclass the Learner class to create custom learners that for example implement momentum. Below I have added one additional Subscriber that displays the progress of the loss a bit nicer in a graph, as well as puts the outputs in a table. It has been copied from the miniai library as is (with some minor changes to make it work for nntrain).\n\n #| export\n\nclass ProgressS(Subscriber):\n    order = MetricsS.order+1\n    def __init__(self, plot=False): self.plot = plot\n    def before_fit(self, learn):\n        learn.epochs = self.mbar = master_bar(learn.epochs)\n        self.first = True\n        if hasattr(learn, 'metrics'): learn.metrics.output = self.output\n        self.losses = []\n        self.val_losses = []\n\n    def output(self, d):\n        if self.first:\n            self.mbar.write(list(d), table=True)\n            self.first = False\n        self.mbar.write(list(d.values()), table=True)\n\n    def before_epoch(self, learn): learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)\n    \n    def after_batch(self, learn):\n        learn.dl.comment = f'{learn.loss:.3f}'\n        if self.plot and hasattr(learn, 'metrics') and learn.model.training:\n            self.losses.append(learn.loss.item())\n            if self.val_losses: self.mbar.update_graph([[fc.L.range(self.losses), self.losses],[fc.L.range(learn.epoch).map(lambda x: (x+1)*len(learn.dls.train)), self.val_losses]])\n    \n    def after_epoch(self, learn): \n        if not learn.model.training:\n            if self.plot and hasattr(learn, 'metrics'): \n                self.val_losses.append(learn.metrics.loss.compute())\n                self.mbar.update_graph([[fc.L.range(self.losses), self.losses],[fc.L.range(learn.epoch+1).map(lambda x: (x+1)*len(learn.dls.train)), self.val_losses]])\n\n\nprogress_s = ProgressS(True)\n\nl = MomentumLearner(get_model(), dls, F.cross_entropy, torch.optim.SGD, lr, [metrics_s, device_s, progress_s])\nl.fit(5)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\naccuracy\n\n\n\n\n0\ntrain\n1.818\n0.425\n\n\n0\neval\n1.192\n0.640\n\n\n1\ntrain\n0.965\n0.676\n\n\n1\neval\n0.845\n0.693\n\n\n2\ntrain\n0.772\n0.730\n\n\n2\neval\n0.740\n0.737\n\n\n3\ntrain\n0.690\n0.762\n\n\n3\neval\n0.676\n0.765\n\n\n4\ntrain\n0.636\n0.784\n\n\n4\neval\n0.634\n0.780"
  },
  {
    "objectID": "posts/05_crossentropy/index.html",
    "href": "posts/05_crossentropy/index.html",
    "title": "Cross entropy any which way",
    "section": "",
    "text": "Cross entropy is one of the most commonly used loss functions. In this post, we will have a look at how it works, and compute it in a couple of different ways.\nConsider a network that is build for image classification. During the forward pass, images are passed into the network and the network processes the data layer by layer, until evenually some final activations are being returned by the model. These final activations are called ‚Äúlogits‚Äù and represent the unnormalized predictions of our model.\nSince we generally use mini-batches during training, these logits are of shape [bs, num_classes]\nimport torch\nimport torch.nn.functional as F\n\ng = torch.manual_seed(42) # use a generator for reproducability\n\nbs = 32 # batch size of 32\nnum_classes = 3 # image classification with 3 different classes\n\nlogits = torch.randn(size=(bs, num_classes), generator=g) # size: [32,3]\n\nlogits[0:4] # show the logits for the first couple of samples\n\ntensor([[ 1.9269,  1.4873,  0.9007],\n        [-2.1055,  0.6784, -1.2345],\n        [-0.0431, -1.6047, -0.7521],\n        [ 1.6487, -0.3925, -1.4036]])\nEach row of this tensor represents the unnormalized predictions for each of our samples in the batch. We can normalize these predictions by applying a softmax. The softmax function does two things:\nThis makes sure that we can treat the output of this as probabilities, because:\nSpecifically:\n# Unnormalized predictions for our first sample (3 classes)\nlogits[0]\n\ntensor([1.9269, 1.4873, 0.9007])\n# Exponentiated predictions, making them all positive\nexp_logits = logits[0].exp()\nexp_logits\n\ntensor([6.8683, 4.4251, 2.4614])\n# Turn these values into probabilities by dividing by the sum\nprobs = exp_logits / exp_logits.sum()\n\n# verify that the sum of the probabilities sum to 1\nassert torch.allclose(probs.sum(), torch.tensor(1.))\n\nprobs\n\ntensor([0.4993, 0.3217, 0.1789])\nSo, let‚Äôs create a softmax function that does this for a whole batch:\ndef softmax(logits):\n    exp_logits = logits.exp() # shape: [32, 3]\n    exp_logits_sum = exp_logits.sum(dim=1, keepdim=True) # shape: [32, 1]\n    \n    # Note: this get's correctly broadcasted, since the exp_logits_sum will \n    # expand to [32, 3], so each value in exp_logits gets divided by the sum over its row\n    probs = exp_logits / exp_logits_sum # shape: [32, 3]\n    \n    return probs \n\nprobs = softmax(logits)\nprobs[0:4]\n\ntensor([[0.4993, 0.3217, 0.1789],\n        [0.0511, 0.8268, 0.1221],\n        [0.5876, 0.1233, 0.2891],\n        [0.8495, 0.1103, 0.0401]])\nNext, we want to compute the loss for which also need our labels. These labels represent the ground truth class for each of our samples in the batch. Since we have 3 classes they will be between 0 and 3 (e.g.¬†either 0, 1 or 2)\ng = torch.manual_seed(42) # use a generator for reproducability\n\nlabels = torch.randint(low=0, high=3, size=(32,), generator=g)\nlabels\n\ntensor([0, 2, 1, 1, 0, 2, 1, 2, 1, 2, 1, 1, 2, 0, 0, 1, 2, 1, 0, 1, 1, 2, 1, 2,\n        2, 1, 2, 0, 1, 1, 0, 0])\nFor classification we use the Negative Log Likelihood loss function, which is defined as such:\n\\[\n\\textrm{NLL} = - \\sum_{i}{q_i * \\log(p_i)}\n\\]\nwith \\(i\\) being the index that moves along the classes (3 in our example) and \\(q_i\\) being the probability that the ground truth label is class \\(i\\) (this is a somewhat strange formulation, since this probability is either 1 (for the correct class) or 0 (for all the non-correct classes)). Finally, \\(p_i\\) is the probability that the model associated to class \\(i\\).\nFor the very first row of our probs ([0.4993, 0.3217, 0.1789]) and our first label (0) we thus get:\n\\[\\begin{align}\n\\textrm{NLL} &= - ( (1 \\cdot \\log(0.4993)) + (0 \\cdot \\log(0.3217)) + (0 \\cdot \\log(0.1789)) ) \\\\\n\\textrm{NLL} &= - ( (1 \\cdot \\log(0.4993)) ) \\\\\n\\textrm{NLL} &= - \\log(0.4993)\n\\end{align}\\]\nFrom which we see that it‚Äôs just the negative log of the probability associated with the ground truth class.\nSince this computes only the NLL per sample, we also need a way to combine the NLL across the samples in our batch. We can do this either by summing or averaging, averaging has the advantage that the size of the loss remains the same when we change the batch-size, so let‚Äôs use that:\ndef nll(probs, labels):\n    # probs: shape [32, 3]\n    # labels: shape [32]\n    \n    # this plucks out the probability of the ground truth label per sample, \n    # it uses \"numpy's integer array indexing\":\n    # https://numpy.org/doc/stable/user/basics.indexing.html#integer-array-indexing\n    probs_ground_truth_class = probs[range(len(labels)), labels] # shape: [32]\n    \n    nll = -torch.log(probs_ground_truth_class).mean() # shape: []\n    return nll\nnll(probs, labels)\n\ntensor(1.3465)"
  },
  {
    "objectID": "posts/05_crossentropy/index.html#using-pytorch",
    "href": "posts/05_crossentropy/index.html#using-pytorch",
    "title": "Cross entropy any which way",
    "section": "Using PyTorch",
    "text": "Using PyTorch\nInstead of using our custom softmax, we can also use the build-in softmax function from PyTorch:\n\np = F.softmax(logits, dim=1) # dim=1 --&gt; compute the sum across the columns\nnll(p, labels)\n\ntensor(1.3465)\n\n\nInstead of using our custom nll we can also use the build-in version from PyTorch. However, nll_loss expects the log of the softmax (for numerical stability) so instead of softmax we have to use log_softmax:\n\np = F.log_softmax(logits, dim=1)\n\n# Assert that indeed the log_softmax is just the softmax followed by a log\nassert torch.allclose(p, F.softmax(logits, dim=1).log())\n\ntorch.nn.functional.nll_loss(p, labels)\n\ntensor(1.3465)\n\n\nThe combination of softmax and nll is called cross entropy, so we can also use PyTorch‚Äôs build-in version of that:\n\nF.cross_entropy(logits, labels)\n\ntensor(1.3465)\n\n\nInstead of the methods in nn.functional, we can also use classes. For that, we first create an instance of the object, and then ‚Äúcall‚Äù the instance:\n\nce = torch.nn.CrossEntropyLoss() # create a CrossEntropyLoss instance\nce(logits, labels) # calling the instance with the arguments returns the cross entropy\n\ntensor(1.3465)\n\n\nSimilarly, we can use classes for the log_softmax and nll_loss functions\n\nls = torch.nn.LogSoftmax(dim=1)\nnll = torch.nn.NLLLoss()\n\np = ls(logits)\nnll(p, labels)\n\ntensor(1.3465)\n\n\nThis is practical, if we want specify custom behavior of the loss function ahead of time of calling the actual loss function. For example, let‚Äôs say we want to compute the cross entropy loss based on ‚Äòsums‚Äô instead of ‚Äòaverages‚Äô. Then when using the method in F we would do:\n\nF.cross_entropy(logits, labels, reduction='sum')\n\ntensor(43.0866)\n\n\nSo whenever we call the loss, we have to specify the additional reduction argument.\nWhereas when using the loss classes, we can instantiate the class with that reduction argument, and then call the instance as per usual without passing anything but the logits and the labels:\n\n# instantiate \nce = torch.nn.CrossEntropyLoss(reduction='sum')\n\n# at some other point in your code, compute the loss as per default\nce(logits, labels)\n\ntensor(43.0866)\n\n\nThis is practical when the loss function is getting called by another object to which we don‚Äôt have easy access. So that we can‚Äôt easily change the arguments for that call. This is for example the case when using the FastAI Learner class, to which we pass the loss function which then get‚Äôs called by the Learner object with the default arguments (logits and labels). By using the classes, we can specify the reduction argument ahead of time and pass that instance to the Learner class."
  },
  {
    "objectID": "posts/03_aiornot/index.html",
    "href": "posts/03_aiornot/index.html",
    "title": "First competitionüèÖ",
    "section": "",
    "text": "In the past couple of weeks I have participated in the first ever Hugging Face competition: aiornot. And as a matter of fact, it was also my first competition to participate in! The competition consisted of 62060 images (18618 train and 43442 test images) which were either created by an AI or not (binary image classification).\nToday, the competition has finished and the private leaderboard has been made public. I‚Äôm super happy (and proud üòá) that I finished in 15th place (98 participants):"
  },
  {
    "objectID": "posts/03_aiornot/index.html#credit-where-credit-is-due",
    "href": "posts/03_aiornot/index.html#credit-where-credit-is-due",
    "title": "First competitionüèÖ",
    "section": "Credit where credit is due:",
    "text": "Credit where credit is due:\n\nü§ó Hugging Face\nI would like to thank Hugging Face and in particular Abhishek Thakur for organizing this competition. I started looking for a first competition at Kaggle a few weeks back, and was very interested in the RSNA competition but quickly found that it was probably a bit too complicated for my first competition. I then saw a tweet from Abhishek announcing this competition and found it a perfect competition to get started.\n\n\nfastai\nIn the past month I have been following the fastai course and I am extremely grateful to Jeremy Howard and Sylvain Gugger for creating fastai. The book, the course, the videos and the great community they have built is really something special and is perfectly tailored for anybody who wants to get started with Deep Learning. Without fastai I could never have pulled this off üôè."
  },
  {
    "objectID": "posts/03_aiornot/index.html#learnings-and-notes",
    "href": "posts/03_aiornot/index.html#learnings-and-notes",
    "title": "First competitionüèÖ",
    "section": "Learnings and notes",
    "text": "Learnings and notes\n\nI quickly learned that data augmentation didn‚Äôt work well on this data. Initially I was a bit surprised by this, but upon inspection of the images I arrived at the following intuition. Normally we want to classify images by what‚Äôs being displayed in the image. So 2 images of a bike should both be classified as such. However, in this dataset we can have images of the same object but if one is created by an AI, and the other is not then they should be classified differently. So instead of looking at what‚Äôs being displayed, it probably has to learn more about the style or the way the image is built up. I can imagine that data augmentation makes this more difficult, especially warping, affine transformations and brightness, contrast augmentations. I was happily surprised to find that the 2nd and 4th place solutions also didn‚Äôt use these data augmentation!\nTraining on larger images works very well. I got a large performance boost for switching to sizes of 416. Jeremy Howard mentioned that this generally works well, and I think because of the nature of these images it worked especially well. To train large models on large images, I heavily relied on Gradient Accumulation to not have to reduce the batchsize.\nTransformer based models such as SWIN and VIT performed not as good as models based on convolutions, I used the convnext models.\nProgressive resizing didn‚Äôt work for me.\nI tried training on 5 epochs and 10 epochs. 10 epochs never gave me better results.\n\nLast but not least:\nParticipating in competitions is very motivating and rewarding. Working individually through courses, exercises and lecture notes is very interesting, but you don‚Äôt get a lot of feedback to how you are doing. Am I doing well? Should I spend more time on investigations into certain areas? When participating in a real-world competition you have a very clear goal, and you get immediate feedback on how you are doing. This type of project based learning has the advantage that it‚Äôs very clear what you need to focus on: anything that you encounter during the project.\nIt‚Äôs also great that it has a finite timeline, so that afterwards you can have a sense of achievement which motivates a lot. The Germans have a very nice word for this: Erfolgserlebnis.\n\n\n\nImage for Stable Diffusion prompt: ‚ÄúSense of achievement when finishing my first ever machine learning competition‚Äù"
  },
  {
    "objectID": "posts/15_rnn/index.html",
    "href": "posts/15_rnn/index.html",
    "title": "Recurrent Neural Networks",
    "section": "",
    "text": "In the previous post we went through a simple MLP language model. We looked in detail at embeddings, and how they can be used to overcome the curse of dimensionality encountered for n-gram based models.\nIn this post we are going to have a look at recurrent neural networks and how they are different from the MLP developed earlier. As we will see, the concept of an rnn is remarkably similar."
  },
  {
    "objectID": "posts/15_rnn/index.html#data",
    "href": "posts/15_rnn/index.html#data",
    "title": "Recurrent Neural Networks",
    "section": "Data",
    "text": "Data\nEverything starts with training data, for a description see the earlier post\n\n\nCode\nimport random\nfrom functools import reduce, partial\nfrom pathlib import Path\nfrom urllib.request import urlretrieve\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport torcheval.metrics as tem\nimport fastcore.all as fc\n\nfrom nntrain.dataloaders import DataLoaders\nfrom nntrain.learner import *\nfrom nntrain.activations import *\nfrom nntrain.acceleration import *\nfrom nntrain.ngram import *\n\n\n\n########### Load the data ###########\npath = Path('./data')\npath.mkdir(parents=True, exist_ok=True)\npath = path / 'names.txt'\nurl = 'https://raw.githubusercontent.com/karpathy/makemore/master/names.txt'\n\n_ = urlretrieve(url, path)\n\nwith open(path, 'r') as f:\n    lines = f.read().splitlines()\n    \nrandom.seed(42)\nrandom.shuffle(lines)\n\ntrain_size=0.8\nval_size=0.1\n\ntrain_lines = lines[0:int(train_size * len(lines))]\nval_lines = lines[int(train_size * len(lines)): int((train_size + val_size) * len(lines))]\n\n### Create vocabulary and mappings ###\nunique_chars = list(set(\"\".join(lines)))\nunique_chars.sort()\nvocabulary = ['.'] + unique_chars\n\nc2i = {c:i for i, c in enumerate(vocabulary)}\ni2c = {i:c for i, c in enumerate(vocabulary)}\n\ndef get_dls(bs, context_length):\n    train_ds = NgramDataset(train_lines, c2i, n=context_length+1)\n    val_ds = NgramDataset(val_lines, c2i, n=context_length+1)\n    train_loader = torch.utils.data.DataLoader(train_ds, shuffle=True, batch_size=bs, num_workers=4)\n    val_loader = torch.utils.data.DataLoader(val_ds, batch_size=bs*2, num_workers=4)\n    dls = DataLoaders(train_loader, val_loader)\n    return dls"
  },
  {
    "objectID": "posts/15_rnn/index.html#from-mlp-to-rnn-a-small-change",
    "href": "posts/15_rnn/index.html#from-mlp-to-rnn-a-small-change",
    "title": "Recurrent Neural Networks",
    "section": "From MLP to RNN, a small change",
    "text": "From MLP to RNN, a small change\nThe MLP model encountered in the last post, used an embedding layer to turn the tokens from the context (in the image below of length 3) into vectors. These vectors then got concatenated, and passed through a hidden layer. After that, the activations were mapped to the size of the vocabulary in the output layer:\n\n\n\nMLP architecture\n\n\nFor an RNN, we also start-off by turning our integers into vectors making use of an embedding layer. However, instead of concatenating the result and passing it through a hidden layer, we iteratively feed the embedding activations through a single hidden layer. Each time moving on to (the embedding activations of) the next token and adding the hidden activations obtained from the previous iteration. Once all the context has been processed in this way, we finalize with an output layer similar to what we did for the MLP model.\n\n\n\nRNN architecture\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe first time I learned about the summing of embedding activations and hidden activations, I was pretty surprised: how can you expect to get anything meaningful by simple adding activations from two layers together? As it turns out, in neural network literature it happens quite often that the combination / merging of activations simply happens by element-wise addition. In a section below, we will also shortly discuss another way of combining information: by concatenation.\n\n\nLet‚Äôs put this RNN architecture into code:\n\nclass FixedContextRNN(nn.Module):\n    \n    def __init__(self, c2i, hidden_size=10):\n        super().__init__()\n        self.c2i = c2i\n        self.input2hidden   = nn.Embedding(len(c2i), hidden_size)\n        self.hidden2hidden  = nn.Linear(hidden_size, hidden_size)\n        self.hidden2out     = nn.Linear(hidden_size, len(c2i))\n        \n    def forward(self, x):\n        input1 = F.relu(self.input2hidden(x[:,0]))\n        input2 = F.relu(self.input2hidden(x[:,1]))\n        input3 = F.relu(self.input2hidden(x[:,2]))\n        \n        out = F.relu(self.hidden2hidden(input1))\n        out = F.relu(self.hidden2hidden(out + input2))\n        out = F.relu(self.hidden2hidden(out + input3))\n        \n        return self.hidden2out(out)\n\nObserve that both the embedding dimension and the size of the hidden layer need to be equal (hidden_size) because the activations from both these layers are added together and thus need to be of equal shape. This also means that there is just one hyperparameter influencing the amount of parameters in this network.\nWe can generalize this model, so that it accepts data of arbitrary context length. Also, we will make use of a dummy hidden-layer activation tensor (initialized to zeros) so that we can do the addition for all iterations (and not just for the second iteration onward):\n\nclass VariableContextRNN(nn.Module):\n    \n    def __init__(self, c2i, hidden_size):\n        super().__init__()\n        self.c2i = c2i\n        self.hidden_size = hidden_size\n        self.input2hidden   = nn.Embedding(len(c2i), hidden_size)\n        self.hidden2hidden  = nn.Linear(hidden_size, hidden_size)\n        self.hidden2out     = nn.Linear(hidden_size, len(c2i))\n        \n    def forward(self, x):\n        inputs = self.input2hidden(x)           # [bs, context_length, hidden_size]\n\n        # create dummy hidden-layer activations for the very first token \n        h = torch.zeros((x.shape[0], self.hidden_size)).to(device)\n        \n        for i in range(x.shape[1]):             # iterate through the tokens (context length)\n            h = F.relu(self.hidden2hidden(h + inputs[:,i,:])) # [bs, hidden_size]\n            \n        return self.hidden2out(h)               # [bs, classes]\n\nLet‚Äôs see how this model performs with a similar configuration to the first MLP we build in the previous post (context length of 3 and a hidden size of 50):\n\n# Train parameters\nepochs = 5\nlr     = 1e-2\nbs     = 1000\n\n# Hyperparameters\ncontext_length = 3\nn_h            = 50\n\ndls = get_dls(bs, context_length)\nrnn = VariableContextRNN(c2i, n_h)\n\nsubs = [ProgressS(True),\n        MetricsS(),\n        DeviceS(device)]\n\nscheduler = partial(torch.optim.lr_scheduler.OneCycleLR, max_lr=lr, epochs=epochs, steps_per_epoch=len(dls.train))\n\nl = Learner(rnn, dls, F.cross_entropy, torch.optim.Adam, None, subs=subs + [SchedulerS(scheduler)])\nl.fit(epochs, lr=lr)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\n\n\n\n\n0\ntrain\n2.634\n\n\n0\neval\n2.303\n\n\n1\ntrain\n2.275\n\n\n1\neval\n2.244\n\n\n2\ntrain\n2.231\n\n\n2\neval\n2.213\n\n\n3\ntrain\n2.200\n\n\n3\neval\n2.194\n\n\n4\ntrain\n2.178\n\n\n4\neval\n2.184\n\n\n\n\n\n\n\n\nSo it‚Äôs doing a bit better (2.280 for the MLP, 2.178 for the RNN)"
  },
  {
    "objectID": "posts/15_rnn/index.html#combing-data-addition-or-concatenation",
    "href": "posts/15_rnn/index.html#combing-data-addition-or-concatenation",
    "title": "Recurrent Neural Networks",
    "section": "Combing data: addition or concatenation",
    "text": "Combing data: addition or concatenation\nAs mentioned, the first time I learned about the fact that we are simply adding activations from the hidden layer and the embedding layer, I was pretty surprised. Instead of simple addition of the activations we could also concatenate these activations instead. This means that our hidden2hidden layer shape needs to be updated:\n\nclass VariableContextRNNConcat(nn.Module):\n    \n    def __init__(self, c2i, hidden_size):\n        super().__init__()\n        self.c2i = c2i\n        self.hidden_size = hidden_size\n        self.input2hidden   = nn.Embedding(len(c2i), hidden_size)\n        self.hidden2hidden  = nn.Linear(2*hidden_size, hidden_size)\n        self.hidden2out     = nn.Linear(hidden_size, len(c2i))\n        \n    def forward(self, x):\n        inputs = self.input2hidden(x)                # [bs, context_length, hidden_size]\n        \n        h = torch.zeros((x.shape[0], self.hidden_size)).to(device)\n        \n        for i in range(x.shape[1]):\n            h = torch.cat([h, inputs[:,i,:]], dim=1) # [bs, 2 x hidden_size]\n            h = F.relu(self.hidden2hidden(h))        # [bs, hidden_size]\n        \n        return self.hidden2out(h)                    # [bs, classes]\n\n\nrnn = VariableContextRNNConcat(c2i, n_h)\n\nl = Learner(rnn, dls, F.cross_entropy, torch.optim.Adam, None, subs=subs + [SchedulerS(scheduler)])\nl.fit(epochs, lr=lr)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\n\n\n\n\n0\ntrain\n2.621\n\n\n0\neval\n2.285\n\n\n1\ntrain\n2.262\n\n\n1\neval\n2.230\n\n\n2\ntrain\n2.216\n\n\n2\neval\n2.196\n\n\n3\ntrain\n2.180\n\n\n3\neval\n2.173\n\n\n4\ntrain\n2.156\n\n\n4\neval\n2.163\n\n\n\n\n\n\n\n\nIn this configuration, this seems to give a slightly better performance"
  },
  {
    "objectID": "posts/15_rnn/index.html#pytorch-rnn",
    "href": "posts/15_rnn/index.html#pytorch-rnn",
    "title": "Recurrent Neural Networks",
    "section": "PyTorch RNN",
    "text": "PyTorch RNN\nLet‚Äôs use PyTorch‚Äôs nn.RNN, in the PyTorchRNN class defined below, we can see that this module replaces our hidden2hidden layer, and also takes care of the loop over the tokens in the context in our forward pass.\nSome notes:\n\nnn.RNN uses a tanh activation by default, we can use a relu activation instead by setting the nonlinearity argument\nthe (dummy) activations that we add for the processing of the first token need to have an additional empty dimension in first position (for the non bidirectional case) according to the documentation\nwhen forwarding data through the rnn module, it expects the input data to have the context_length as first dimension. We can alter this by setting batch_first=True\nnn.RNN returns two parameters. The first parameter is a tensor containing the hidden activation from each iteration, the second parameter has the final hidden activation. For the moment we are only interested in the last hidden activations and pass it to the output layer but we will use the aggregated hidden activations in a section below\n\n\nclass PyTorchRNN(nn.Module):\n    \n    def __init__(self, c2i, context_length, hidden_size):\n        super().__init__()\n        self.c2i = c2i\n        self.hidden_size = hidden_size\n        self.input2hidden   = nn.Embedding(len(c2i), hidden_size)\n        self.rnn            = nn.RNN(hidden_size, hidden_size, nonlinearity='relu', batch_first=True)\n        self.hidden2out     = nn.Linear(hidden_size, len(c2i))\n        \n    def forward(self, x):\n        inputs = self.input2hidden(x) # [bs, context_length, hidden_size]\n        \n        h = torch.zeros((1, x.shape[0], self.hidden_size)).to(device)\n        \n        hs, h = self.rnn(inputs, h)\n        # hs : [bs, context_length, hidden_size] -&gt; all hidden states\n        # h  : [1, bs, hidden_size]              -&gt; final hidden state\n\n        return self.hidden2out(h.squeeze(0)) # squeeze out the first empty dimension\n\n\nrnn = PyTorchRNN(c2i, context_length, n_h)\n\nl = Learner(rnn, dls, F.cross_entropy, torch.optim.Adam, None, subs=subs + [SchedulerS(scheduler)])\nl.fit(epochs, lr=lr)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\n\n\n\n\n0\ntrain\n2.570\n\n\n0\neval\n2.285\n\n\n1\ntrain\n2.261\n\n\n1\neval\n2.240\n\n\n2\ntrain\n2.217\n\n\n2\neval\n2.200\n\n\n3\ntrain\n2.180\n\n\n3\neval\n2.176\n\n\n4\ntrain\n2.156\n\n\n4\neval\n2.166"
  },
  {
    "objectID": "posts/15_rnn/index.html#optimizing-dataloading-and-model-for-sequential-data-bptt",
    "href": "posts/15_rnn/index.html#optimizing-dataloading-and-model-for-sequential-data-bptt",
    "title": "Recurrent Neural Networks",
    "section": "Optimizing dataloading and model for sequential data (BPTT)",
    "text": "Optimizing dataloading and model for sequential data (BPTT)\nAlthough the previous RNN‚Äôs are perfectly valid models, there is a pretty big improvement we can make for the training of these models. To realize that, let‚Äôs have a look at the samples for the first name Yuheng in our dataset:\n\nxb, yb = dls.train.dataset[0:6]\nfor x,y in zip(xb, yb):\n    print(list(i2c[i.item()] for i in x), '--&gt;' ,i2c[y.item()])\n\n['.', '.', '.'] --&gt; y\n['.', '.', 'y'] --&gt; u\n['.', 'y', 'u'] --&gt; h\n['y', 'u', 'h'] --&gt; e\n['u', 'h', 'e'] --&gt; n\n['h', 'e', 'n'] --&gt; g\n\n\nConsider what happens when we process this data in order with a batch size of 1. In the very first batch (forward pass) we start with an empty hidden state and compute the final hidden state \\(h_0\\) from processing the three '.' tokens sequentially. These final activations are mapped to output activations and the loss is computed by comparing to the label 'y'.\nWhen moving to the second batch we start again with an empty hidden state, and since we have moved one letter forward we will process 2 '.' tokens and the 'y token sequentially to get to the final hidden activation \\(h_1\\) of this sample. But this is a bit silly, we could have just remembered our hidden activations \\(h_0\\) from the first batch and process the next token ('y') to get to the same result!\nIn other words: for each forward pass, we reinitialize the dummy hidden layer activations, and the first two tokens we process are in fact already processed in the previous sample. We could be more efficient by just using the hidden activations we computed on the previous sample and have one iteration over the new token in the context:\n\n\n\nMore efficient data-processing for sequential data\n\n\nLet‚Äôs simplify things and make the architecture symmetrical by not treating the very first sample in a different way. This also means we can get rid of the triple start symbol (...) and replace it with a single start symbol (.).\n\n\n\nSymmetric architecture for efficient data-processing\n\n\nWith these changes, there are two practical problems we need to address:\n\nHow are we going to change the dataloader and dataset to feed this into a neural network?\nFeeding the hidden activations into the processing of the next token means that the computational graph of the loss is getting more and more complex. If we do this for the entirety of our dataset, the backward pass will become intractable. We thus have to cut it off at some point.\n\nWe need to go from samples (combinations of X and y) that look like this (and don‚Äôt pass over the hidden state from sample to sample):\n\n\nCode\nxb, yb = dls.train.dataset[0:7]\nfor x,y in zip(xb, yb):\n    print(list(i2c[i.item()] for i in x), '--&gt;' ,i2c[y.item()])\n\n\n['.', '.', '.'] --&gt; y\n['.', '.', 'y'] --&gt; u\n['.', 'y', 'u'] --&gt; h\n['y', 'u', 'h'] --&gt; e\n['u', 'h', 'e'] --&gt; n\n['h', 'e', 'n'] --&gt; g\n['e', 'n', 'g'] --&gt; .\n\n\nto samples that simply look like this (and do pass the hidden state from sample to sample):\n\n\nCode\nname = '.yuheng.'\nfor i in range(len(name)-1):\n    print(f\"['{name[i]}'] --&gt; {name[i+1]}\")\n\n\n['.'] --&gt; y\n['y'] --&gt; u\n['u'] --&gt; h\n['h'] --&gt; e\n['e'] --&gt; n\n['n'] --&gt; g\n['g'] --&gt; .\n\n\nHowever, we want to keep the sequence length dimension since we will use that as the maximal amount of tokens through which we will backpropagate. This addresses the second issue mentioned above. We thus need a dataset that looks like this (for a sequence length of 3):\n\n\nCode\nprint(\"['.', 'y', 'u']\", '--&gt;', \"['y', 'u', 'h']\")\nprint(\"['h', 'e', 'n']\", '--&gt;', \"['e', 'n', 'g']\")\n\n\n['.', 'y', 'u'] --&gt; ['y', 'u', 'h']\n['h', 'e', 'n'] --&gt; ['e', 'n', 'g']\n\n\nFor the above example, we would still pass the hidden state computed in the first sample to the second sample, but only the values are passed and not the computational history. This is essentially what‚Äôs referred to as (truncated) backpropagation through time. Also, note that this means that one single sample (['.', 'y', 'u']) will create 3 outputs (for the 3 labels ['y', 'u', 'h']). This means we will have to update our loss function as well.\nLet‚Äôs create a new dataset for this purpose:\n\nclass SequentialDataset():\n    def __init__(self, lines, c2i, sequence_length):\n        text = \".\" + \".\".join(lines) + \".\"\n        self.x = []\n        self.y = []\n        for i in range(0, len(text) - sequence_length - 1, sequence_length):\n            self.x.append([c2i[xi] for xi in text[i: i+sequence_length]])\n            self.y.append([c2i[yi] for yi in text[i+1: i+sequence_length+1]])\n        self.x = torch.tensor(self.x)\n        self.y = torch.tensor(self.y)\n    \n    def __getitem__(self, i):\n        return self.x[i], self.y[i]\n\n    def __len__(self):\n        return len(self.x)\n    \ntrain_ds = SequentialDataset(train_lines, c2i, 3)\nvalid_ds = SequentialDataset(val_lines, c2i, 3)\n\n\nfor i in range(2):\n    print([i2c[xi.item()] for xi in train_ds[i][0]], '--&gt;', [i2c[xi.item()] for xi in train_ds[i][1]])\n\n['.', 'y', 'u'] --&gt; ['y', 'u', 'h']\n['h', 'e', 'n'] --&gt; ['e', 'n', 'g']\n\n\nFurthermore, we need to order our data in a different way. We need to be very careful with shuffling: we certainly can‚Äôt shuffle individual samples anymore in our Dataloader, since we need to maintain the sequence of our data.\n\n\n\n\n\n\nNote\n\n\n\nWe could however shuffle the order of the complete names, at the beginning of each epoch. But let‚Äôs not be concerned with that for now, and just disable shuffling altogether.\n\n\nAlso, we need to make sure that the stream continues from sample to sample across batches, so that we can pass the (final) hidden states from the samples in a batch to the samples in the next batch. Consider a dataset consisting of 100 sequential samples:\n\nIf we order the data in this way, only the very first batch of each epoch will be using a dummy hidden state equal to zero, all other iterations in all the batches will be using the correct (previously computed and sequentially passed) hidden state.\nTo get these batches into the model during training, we need to order the data vertically, so 1, 21, 41, 61, 81, 2, 22, 42, etc. Let‚Äôs do so with a custom Sampler (for details on samplers, check out this earlier blog post):\n\n\n\n‚ÄúVertical‚Äù sampling order\n\n\n\nclass VerticalSampler():\n    def __init__(self, ds, batch_size):\n        self.batch_size = batch_size\n        self.batches = len(ds) // self.batch_size\n        \n    def __iter__(self):\n        for i in range(self.batches):\n            for j in range(self.batch_size):\n                yield i + self.batches*j\n                \n    def __len__(self):\n        return self.batches * self.batch_size\n\nWith this sampler we can then create a dataloader with the associated batch size to load exactly the batches into the model as described!\n\ntrain_dl = torch.utils.data.DataLoader(train_ds, shuffle=False, sampler=VerticalSampler(train_ds, bs), batch_size=bs)\nvalid_dl = torch.utils.data.DataLoader(valid_ds, shuffle=False, sampler=VerticalSampler(valid_ds, bs), batch_size=bs)\n\ndls = DataLoaders(train_dl, valid_dl)\n\nNow let‚Äôs update our model to pass all the hidden states to the output layer:\n\nclass PyTorchRNNMultiOutput(nn.Module):\n    \n    def __init__(self, c2i, hidden_size, bs):\n        super().__init__()\n        self.c2i = c2i\n        self.bs = bs\n        self.hidden_size = hidden_size\n        self.register_buffer('h', torch.zeros((1, bs, self.hidden_size)))\n        \n        self.input2hidden   = nn.Embedding(len(c2i), hidden_size)\n        self.rnn            = nn.RNN(hidden_size, hidden_size, nonlinearity='relu', batch_first=True)\n        self.hidden2out     = nn.Linear(hidden_size, len(c2i))\n        \n    def forward(self, x):\n        inputs = self.input2hidden(x)\n        hs, h = self.rnn(inputs, self.h)\n        # hs : [bs, context_length, hidden_size]\n        # h  : [1, bs, hidden_size]\n        self.h = h.detach()                      # detach the computational graph\n        return self.hidden2out(hs)               # pass all hidden states to the output layer\n\nAnd finally, we have to create a custom loss function that can deal with the multiple output activations and multiple labels:\n\ndef multi_output_cross_entropy(logits, targets):\n    # logits = [bs, context_length, output_classes]\n    # targets = [bs, context_length]\n    \n    targets = targets.view(-1)\n    \n    bs, context_length, output_classes = logits.shape\n    logits = logits.view(-1, output_classes)\n    \n    return F.cross_entropy(logits, targets)\n\n\ndef get_dls(context_length, batch_size):\n    train_ds = SequentialDataset(train_lines, c2i, context_length)\n    valid_ds = SequentialDataset(val_lines, c2i, context_length)\n        \n    train_dl = torch.utils.data.DataLoader(train_ds, shuffle=False, sampler=VerticalSampler(train_ds, batch_size), batch_size=batch_size)\n    valid_dl = torch.utils.data.DataLoader(valid_ds, shuffle=False, sampler=VerticalSampler(valid_ds, batch_size), batch_size=batch_size)\n\n    return DataLoaders(train_dl, valid_dl)\n\nNote that we drop the batch size to a value of 300 (1/3 of 1000). We do this, to roughly keep the amount of weight updates per batch constant. Previously, we had 1000 sequences in the batch, and from each sequence we would receive one ‚Äúweight update‚Äù. Now, we have 3 times the amount of sequences in a batch, so we have to reduce the batch size by 3 to keep the amount of ‚Äúweight updates‚Äù roughly equal.\n\n# Train parameters\nepochs = 5\nlr     = 1e-2\nbs     = 300\n\n# Hyperparameters\ncontext_length = 5\nn_h            = 100\n\ndls = get_dls(context_length, bs)\nrnn = PyTorchRNNMultiOutput(c2i, n_h, bs)\n\nsubs = [ProgressS(True),\n        MetricsS(),\n        DeviceS(device)]\n\nscheduler = partial(torch.optim.lr_scheduler.OneCycleLR, max_lr=lr, epochs=epochs, steps_per_epoch=len(dls.train))\n\nl = Learner(rnn, dls, multi_output_cross_entropy, torch.optim.Adam, None, subs=subs + [SchedulerS(scheduler)])\nl.fit(epochs, lr=lr)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\n\n\n\n\n0\ntrain\n2.528\n\n\n0\neval\n2.224\n\n\n1\ntrain\n2.176\n\n\n1\neval\n2.174\n\n\n2\ntrain\n2.117\n\n\n2\neval\n2.130\n\n\n3\ntrain\n2.064\n\n\n3\neval\n2.104\n\n\n4\ntrain\n2.026\n\n\n4\neval\n2.093\n\n\n\n\n\n\n\n\nPerformance is pretty good, however what‚Äôs up with the spikes in the loss? These spikes occur at the beginning of every epoch and they happen because moving from the last batch in an epoch, to the first batch in the next epoch is not a sequential step in the data. Looking at the image above in which we laid out the ‚Äúvertical ordering of the data‚Äù, wee see that for the first batch in any epoch &gt; 1, we would feed the hidden state originating from samples (20, 40, 60, 80, 100) into the samples (1, 21, 41, 61, 81), e.g.:\n\n\nCode\nfrm = (20, 40, 60, 80, 100)\nto  = (1, 21, 41, 61, 81)\n\nfor f, t in zip(frm,to):\n    print(f'from: {f:3d} to: {t:2d}')\n\n\nfrom:  20 to:  1\nfrom:  40 to: 21\nfrom:  60 to: 41\nfrom:  80 to: 61\nfrom: 100 to: 81\n\n\nAnd this is obviously not sequential. The easiest solution is to reset the hidden state to zero at the beginning of each epoch, let‚Äôs do this with a very small Subscriber:\n\n\n\n\n\n\nNote\n\n\n\nBased on the observation that we could realign the hidden activations by moving them one row down, I implemented this solution but was surprised to see that this doesn‚Äôt improve things at all. I then found that we generally don‚Äôt have a fully connected sequential dataset that fits exactly into a whole number of batches, we thus lose some data in the end, and the data doesn‚Äôt connect easily by shifting it ‚Äúone row down‚Äù. In terms of the sample ordering diagram above: we generally don‚Äôt have (20, 40, 60, 80, 100) as final batch of an epoch but instead something like (18, 38, 58, 78, 98).\n\n\n\nclass HiddenStateResetterS(Subscriber):\n    def before_epoch(self, learn):\n        learn.model.reset_hidden_state()\n        \n@fc.patch\ndef reset_hidden_state(self:PyTorchRNNMultiOutput):\n    device = self.h.get_device()\n    self.h = torch.zeros_like(self.h).to(device)\n\n\n# Train parameters\nepochs = 5\nlr     = 1e-2\nbs     = 300\n\n# Hyperparameters\ncontext_length = 5\nn_h            = 100\n\ndls = get_dls(context_length, bs)\nrnn = PyTorchRNNMultiOutput(c2i, n_h, bs)\n\nsubs = [ProgressS(True),\n        MetricsS(),\n        DeviceS(device),\n        HiddenStateResetterS()]\n\nscheduler = partial(torch.optim.lr_scheduler.OneCycleLR, max_lr=lr, epochs=epochs, steps_per_epoch=len(dls.train))\n\nl = Learner(rnn, dls, multi_output_cross_entropy, torch.optim.Adam, None, subs=subs + [SchedulerS(scheduler)])\nl.fit(epochs, lr=lr)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\n\n\n\n\n0\ntrain\n2.532\n\n\n0\neval\n2.221\n\n\n1\ntrain\n2.177\n\n\n1\neval\n2.159\n\n\n2\ntrain\n2.114\n\n\n2\neval\n2.118\n\n\n3\ntrain\n2.065\n\n\n3\neval\n2.086\n\n\n4\ntrain\n2.025\n\n\n4\neval\n2.076\n\n\n\n\n\n\n\n\nThe loss improved a bit from this fix, and the spikes are smaller, but still visible. This is not surprising since the very first batch in each epoch still has the difficulty of working with an empty hidden state. This is not a big issue however, since the hidden state is quickly filling up. Accordingly, the following batches show again a loss that is in-line with the loss of the end of the previous epoch.\nLast but not least, PyTorch nn.RNN accepts two parameters, one for the features in the inputs and one for the features in the hidden state. Previously we used one single parameter for this to make sure we can add the activations. Apparently, nn.RNN is doing another multiplication to align them. So let‚Äôs use these two values as separate arguments to see whether we can further improve performance through tweaking them.\n\nclass PyTorchRNNMultiOutput(nn.Module):\n    \n    def __init__(self, c2i, embedding_dim, hidden_size, bs):\n        super().__init__()\n        self.c2i = c2i\n        self.bs = bs\n        self.embedding_dim = embedding_dim\n        self.hidden_size = hidden_size\n        # register as buffer so that its moved to the device by the DeviceS Subscriber\n        self.register_buffer('h', torch.zeros((1, bs, self.hidden_size))) \n        \n        self.input2hidden   = nn.Embedding(len(c2i), embedding_dim)\n        self.rnn            = nn.RNN(embedding_dim, hidden_size, nonlinearity='relu', batch_first=True)\n        self.hidden2out     = nn.Linear(hidden_size, len(c2i))\n        \n    def forward(self, x):\n        inputs = self.input2hidden(x)\n        hs, h = self.rnn(inputs, self.h)\n        # hs : [bs, context_length, hidden_size]\n        # h  : [1, bs, hidden_size]\n        self.h = h.detach()\n        return self.hidden2out(hs)\n    \n    def reset_hidden_state(self):\n        device = self.h.get_device()\n        self.h = torch.zeros_like(self.h).to(device)\n\nTrying for around 10 minutes with different settings shows me that performance can be slightly improved by increasing the size of the hidden state by quite a bit, and keeping the embedding dimension smaller:\n\n# Train parameters\nepochs = 5\nlr     = 1e-2\nbs     = 300\n\n# Hyperparameters\ncontext_length = 5\nn_h            = 1000\nembedding_dim  = 100\n\ndls = get_dls(context_length, bs)\nrnn = PyTorchRNNMultiOutput(c2i, embedding_dim, n_h, bs)\n\nsubs = [ProgressS(True),\n        MetricsS(),\n        DeviceS(device),\n        HiddenStateResetterS()]\n\nscheduler = partial(torch.optim.lr_scheduler.OneCycleLR, max_lr=lr, epochs=epochs, steps_per_epoch=len(dls.train))\n\nl = Learner(rnn, dls, multi_output_cross_entropy, torch.optim.Adam, None, subs=subs + [SchedulerS(scheduler)])\nl.fit(epochs, lr=lr)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\n\n\n\n\n0\ntrain\n2.391\n\n\n0\neval\n2.184\n\n\n1\ntrain\n2.130\n\n\n1\neval\n2.126\n\n\n2\ntrain\n2.042\n\n\n2\neval\n2.055\n\n\n3\ntrain\n1.934\n\n\n3\neval\n2.007\n\n\n4\ntrain\n1.825\n\n\n4\neval\n1.985"
  },
  {
    "objectID": "posts/15_rnn/index.html#multilayer-rnns",
    "href": "posts/15_rnn/index.html#multilayer-rnns",
    "title": "Recurrent Neural Networks",
    "section": "MultiLayer RNN‚Äôs",
    "text": "MultiLayer RNN‚Äôs\nLast but not least, RNN‚Äôs can be stacked on top of eachother. In the diagram below, a 2-layer RNN with a sequence length of 3 is shown. The only difference is that instead of passing the hidden activations of the first layer directly through the hidden-to-out layer to get to our output activations, we pass it through an additional hidden-to-hidden layer (with it‚Äôs own weights).\n\n\n\nMultilayer RNN\n\n\nTwo implement this in our model is extremely easy, the only thing we have to change is adding a layers argument into the model which is passed both to the creation of the dummy activations as well as to the nn.RNN module:\n\nclass MultiLayerRNN(nn.Module):\n    \n    def __init__(self, c2i, embedding_dim, hidden_size, bs, layers=1):\n        super().__init__()\n        self.c2i = c2i\n        self.bs = bs\n        self.embedding_dim = embedding_dim\n        self.hidden_size = hidden_size\n        self.register_buffer('h', torch.zeros((layers, bs, self.hidden_size)))    # &lt;- initialize dummy activations for n layers\n        \n        self.input2hidden   = nn.Embedding(len(c2i), embedding_dim)               # multiple layers in the rnn module ‚Üì \n        self.rnn            = nn.RNN(embedding_dim, hidden_size, nonlinearity='relu', batch_first=True, num_layers=layers)\n        self.hidden2out     = nn.Linear(hidden_size, len(c2i))\n        \n    def forward(self, x):\n        inputs = self.input2hidden(x)\n        hs, h = self.rnn(inputs, self.h)\n        # hs : [bs, context_length, hidden_size]\n        # h  : [layers, bs, hidden_size]\n        self.h = h.detach()\n        return self.hidden2out(hs)\n    \n    def reset_hidden_state(self):\n        device = self.h.get_device()\n        self.h = torch.zeros_like(self.h).to(device)\n\n\n# Train parameters\nepochs = 5\nlr     = 1e-2\nbs     = 300\n\n# Hyperparameters\ncontext_length = 5\nn_h            = 1000\nembedding_dim  = 100\nlayers         = 2\n\ndls = get_dls(context_length, bs)\nrnn = MultiLayerRNN(c2i, embedding_dim, n_h, bs, layers)\n\nsubs = [ProgressS(True),\n        MetricsS(),\n        DeviceS(device),\n        HiddenStateResetterS()]\n\nscheduler = partial(torch.optim.lr_scheduler.OneCycleLR, max_lr=lr, epochs=epochs, steps_per_epoch=len(dls.train))\n\nl = Learner(rnn, dls, multi_output_cross_entropy, torch.optim.Adam, None, subs=subs + [SchedulerS(scheduler)])\nl.fit(epochs, lr=lr)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\n\n\n\n\n0\ntrain\n2.363\n\n\n0\neval\n2.207\n\n\n1\ntrain\n2.160\n\n\n1\neval\n2.135\n\n\n2\ntrain\n2.066\n\n\n2\neval\n2.064\n\n\n3\ntrain\n1.948\n\n\n3\neval\n2.002\n\n\n4\ntrain\n1.827\n\n\n4\neval\n1.976\n\n\n\n\n\n\n\n\nAnd this performs even better then the model we obtained with a single layer!"
  },
  {
    "objectID": "posts/15_rnn/index.html#sampling-names",
    "href": "posts/15_rnn/index.html#sampling-names",
    "title": "Recurrent Neural Networks",
    "section": "Sampling names",
    "text": "Sampling names\nAnd as we did with the MLP, let‚Äôs conclude with some sampled names from this model\n\n@fc.patch\ndef generate(self:MultiLayerRNN, n=10, generator=None):\n    # For unbatched input we need a 2D hidden state tensor of size [1, hidden_size]\n    self.h = torch.zeros((self.h.shape[0], self.hidden_size)).cuda()\n    \n    names = []\n    for i in range(n):\n        name = '.'\n        while True:\n            idx = torch.tensor([c2i[name[-1]]]).cuda()\n            logits = self.forward(idx)\n            s = torch.multinomial(F.softmax(logits, dim=1), 1, generator=generator)\n            c = i2c[s.item()]\n            name += c\n            if c == '.':\n                names.append(name)\n                break\n    return names\n\n\nrnn.generate()\n\n['.erwan.',\n '.isabel.',\n '.jestelyn.',\n '.hubri.',\n '.coltyn.',\n '.hasseem.',\n '.ramarion.',\n '.ariaad.',\n '.rayron.',\n '.ciron.']"
  },
  {
    "objectID": "posts/15_rnn/index.html#outlook",
    "href": "posts/15_rnn/index.html#outlook",
    "title": "Recurrent Neural Networks",
    "section": "Outlook",
    "text": "Outlook\nWe have come quite a long way, and we have seen that an RNN is in essence not very different from the MLP created earlier. The complexity in my opinion only has to do with the different way of processing and loading the data, which greatly reduces the time to train such models. In the next post we are going to have a look at long short-term memory (LSTM) networks and gated recurrent unit networks (with lot‚Äôs of fancy diagrams üòè)."
  },
  {
    "objectID": "posts/12_nntrain_accelerations/index.html",
    "href": "posts/12_nntrain_accelerations/index.html",
    "title": "nntrain (4/n): Accelerated optimization",
    "section": "",
    "text": "In this series, I want to discuss the creation of a small PyTorch based library for training neural networks: nntrain. It‚Äôs based off the excellent part 2 of Practical Deep Learning for Coders by Jeremy Howard, in which from lessons 13 to 18 (roughly) the development of the miniai library is discussed.\nWe‚Äôll try to build everything as much as possible from scratch to understand how things work. Once the main functionality of components is implemented and verified, we can switch over to PyTorch‚Äôs version. This is similar to how things are done in the course. However, this is not just a ‚Äúcopy / paste‚Äù of the course: on many occasions I take a different route, and most of the code is my own. That is not to say that all of this is meant to be extremely innovative, instead I had the following goals:\nnb_dev is another great project from the fastai community, which allows python libraries to be written in jupyter notebooks. This may sound a bit weird and controversial, but it has the advantage that we can create the source code for our library in the very same environment in which we want to experiment and interact with our methods, objects and structure while we are building the library. For more details on why this is a good idea and other nice features of nb_dev, see here.\nSo without further ado, let‚Äôs start with where we left off in the previous post:"
  },
  {
    "objectID": "posts/12_nntrain_accelerations/index.html#end-of-last-post",
    "href": "posts/12_nntrain_accelerations/index.html#end-of-last-post",
    "title": "nntrain (4/n): Accelerated optimization",
    "section": "End of last post:",
    "text": "End of last post:\nIn the last post we build a convolutional neural network, loaded the Fashion-Mnist data using our Dataloaders module, and trained it using the Learner module. We created a module which helped to understand the fine-print of training a model, more specifically by understanding the activations throughout the network during training.\nWe finally reached a performance of around 89% (evaluation) accuracy by training 5 epochs. In this post, we are going to look closely into how a model is learning: the step in which we update the weights after the backward pass in the training loop.\nUpdating the weights takes the form of:\n\\[\nw_t = w_{t-1} - lr \\cdot grad^{w}_{t-1}\n\\] or in more pseudocode style syntax: \\[\nw \\leftarrow w - lr \\cdot grad^{w}\n\\]\nBefore we dive into different versions of this update step and it‚Äôs implications, I would like to have a good look at this remarkebly simple formula. Specifically, it‚Äôs important that the change to the weights from one step to another is composed of the product of the learning rate \\(lr\\) and the gradient of the loss with respect to the weight we are updating: \\(grad^w\\). And since it‚Äôs a product, doubling one and reducing the other by a factor of 2, cancel each other out.\nFor example, consider some training setup with some loss function \\(loss\\) and some learning rate \\(lr\\). If we would change our loss function by adding a factor of 2: \\(loss_1 = 2 \\cdot loss\\) this would have the effect that the gradients would also double. Very similiar to how if\n\\[\\begin{aligned}\nf(x) &= x^2 &&\\rightarrow f'(x) = 2x \\\\\ng(x) &= 2f(x) = 2x^2  &&\\rightarrow g'(x) = 4x = 2f'(x)\n\\end{aligned}\\]\nAnd since learning rate and gradients are multiplied in the weight update, we could offset the doubling of the loss by reducing the learning rate by 2. It‚Äôs thus important to realize how these components interact."
  },
  {
    "objectID": "posts/12_nntrain_accelerations/index.html#weight-decay",
    "href": "posts/12_nntrain_accelerations/index.html#weight-decay",
    "title": "nntrain (4/n): Accelerated optimization",
    "section": "Weight decay",
    "text": "Weight decay\nWeight decay is a very common regularization technique. Regularization is meant to reduce overfitting. When a model is overfitting, it basically has learned the structure of the training set to such a degree (e.g.¬†memorized) that it performs very well on the training set, but performance is degraded on the validation set. This means the model is no longer generalizing well to data it was not trained on. And since we are always interested in using a model for making predictions on data it hasn‚Äôt seen (e.g.¬†inference), this is an extremely important thing to be aware of.\nWeight decay takes the simple form of adding a component to the loss which penalizes large weights:\n\\[\nloss_{wd} = loss + c\\sum{w_i^2}\n\\]\nAnd since this simple sum we are adding has no interaction terms between weights, we simply get:\n\\[\\begin{aligned}\ngrad_{wd} &= grad + 2c\\cdot w \\\\\n&= grad + d \\cdot w\n\\end{aligned}\\]\nAnd then going back to updating the weights:\n\\[\\begin{aligned}\nw_{wd} &\\leftarrow w_{wd} - lr \\cdot grad_{wd}  \\\\\n&= w_{wd} - lr \\cdot (grad + d \\cdot w_{wd}) \\\\\n&= w_{wd} - lr \\cdot grad - lr \\cdot d \\cdot w_{wd} \\\\\n&= w_{wd}(1 - lr \\cdot d) - lr \\cdot grad\n\\end{aligned}\\]\nIf we compare this expression to the regular update step: \\(w \\leftarrow w - lr \\cdot grad^{w}\\), we find that we can simply multiply the weights by \\((1 - lr \\cdot d)\\) before we do the step:\n\n #| export\nclass SGD:\n    def __init__(self, params, lr, wd=0.):\n        self.params = list(params)\n        self.lr = lr\n        self.wd = wd\n        self.i = 0\n\n    def step(self):                    # this is the method that get's called by the Learner\n        with torch.no_grad():\n            for p in self.params:\n                self.reg_step(p)       # first add regularization\n                self.opt_step(p)       # then do the actual step\n        self.i +=1\n\n    def opt_step(self, p):\n        p -= p.grad * self.lr          # regular step\n    \n    def reg_step(self, p):\n        if self.wd != 0:               # only regularize when the weight decay parameter is set\n            p *= 1 - self.lr*self.wd   # update the weights as described above\n\n    def zero_grad(self):\n        for p in self.params:\n            p.grad.data.zero_()\n\nLet‚Äôs try it out:\n\n\nCode\nfrom datasets import load_dataset,load_dataset_builder\n\nfrom nntrain.dataloaders import DataLoaders, hf_ds_collate_fn\nfrom nntrain.learner import *\nfrom nntrain.activations import *\n\n\n\n\nCode\nimport torchvision.transforms.functional as TF\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom operator import attrgetter\nfrom functools import partial\nimport fastcore.all as fc\nimport math\nimport torcheval.metrics as tem\nimport matplotlib.pyplot as plt\nimport random\nimport numpy as np\n\n\n\nname = \"fashion_mnist\"\nds_builder = load_dataset_builder(name)\nhf_dd = load_dataset(name)\nbs = 1024\ncollate = partial(hf_ds_collate_fn, flatten=False)\ndls = DataLoaders.from_hf_dd(hf_dd, batch_size=bs, collate_fn=collate)\nset_seed(1)\n\n# model and initializaton\nmodel = cnn_layers().apply(init_weights)\n\n# normalization\nxb, yb = next(iter(dls.train))\nnorm = NormalizationS(xb.mean(), xb.std())\n\n# subscribers\nsubs = [norm, \n        ProgressS(True),\n        MetricsS(accuracy=tem.MulticlassAccuracy()),\n        DeviceS(device)]\n\n# Remember that the optim_func get's initialized during fit()\n# https://github.com/lucasvw/nntrain/blob/main/nntrain/learners.py#L53C65-L53C65\noptim_func = partial(SGD, wd=0)\nlr = 0.4\n\nl = MomentumLearner(model, dls, F.cross_entropy, optim_func, lr, subs)\nl.fit(5)\n\n\n\n\n\n\n\nDownloading and preparing dataset fashion_mnist/fashion_mnist (download: 29.45 MiB, generated: 34.84 MiB, post-processed: Unknown size, total: 64.29 MiB) to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1...\nDataset fashion_mnist downloaded and prepared to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1. Subsequent calls will reuse this data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\naccuracy\n\n\n\n\n0\ntrain\n0.590\n0.788\n\n\n0\neval\n0.406\n0.846\n\n\n1\ntrain\n0.350\n0.872\n\n\n1\neval\n0.365\n0.862\n\n\n2\ntrain\n0.305\n0.888\n\n\n2\neval\n0.349\n0.871\n\n\n3\ntrain\n0.280\n0.896\n\n\n3\neval\n0.342\n0.876\n\n\n4\ntrain\n0.261\n0.903\n\n\n4\neval\n0.308\n0.889"
  },
  {
    "objectID": "posts/12_nntrain_accelerations/index.html#momentum",
    "href": "posts/12_nntrain_accelerations/index.html#momentum",
    "title": "nntrain (4/n): Accelerated optimization",
    "section": "Momentum",
    "text": "Momentum\nMomentum has to do with smartly navigating the loss surface. The loss surface is a high dimensional plain (manifold) where each dimension is a single parameter in the model. Since humans are not made for thinking beyond 3 dimensions, let‚Äôs thus quickly replace this by a mental image with just 3 dimensions. Let the loss surface have just 2 parameters that are put on the x and y dimensions, and let z represent the value of our loss function.\nTraining a neural network is all about finding a low (the lowest) spot on this surface, e.g.¬†a point in this space in which the parameters (x and y) take on a value so that the loss (z direction) is low.\nTo find such a spot, we have to navigate this surface. Sometimes it might be difficult to find this spot from the point where we are, since the lowest value might lay behind a hill. Realize that we also never ‚Äúsee‚Äù the complete surface, we can only navigate it by computing gradients from the point where we currently are. We can draw an analogy of a astronaut being dropped in a completely alien world, having a need to find water (the lowest point on the surface) as quickly as possible (because of limitted oxygen supply) with a visibility of only 1 meter.\n\n\n\n2D Loss surface: difficult to find the lowest spot from a random position with super short visibility!\n\n\nSo what‚Äôs our strategy to find water asap? With SGD we compute the gradient based on the contributions of each sample in the minibatch. Then we take a small step in this direction (of size: learning rate x gradient). And we keep repeating this. But imagine that each time we compute this gradient, we find that it‚Äôs more or less in the same direction. Would it then not make sense to maybe take a bit of a larger step? This is the intuition behind momentum, don‚Äôt just consider the gradient at the current position, but get some sense of the surrounding surface by inferring from the past couple of computed gradients. The following image shows this also nicely:\n\n\n\nOn the left SGD without momentum: just compute the gradient each batch and follow that. On the right, use momentum to find an average gradient across multiple batches. Note that this cancels the vertical component of the gradient somewhat out, and reinforces the horizontal component of the gradient, increasing the speed in the right direction.\n\n\nTo implement, we simply do:\n\\[\\begin{aligned}\nw &\\leftarrow w - lr \\cdot grad\\_avg\n\\end{aligned}\\]\nThe definition of \\(grad\\_avg\\) takes different forms. In PyTorch SGD it‚Äôs simply:\n\\[\ngrad\\_avg \\leftarrow grad + \\mu \\cdot grad\\_avg\n\\]\nAnd this is also exactly the way we previously implemented in the MomentumLearner. Additionally PyTorch defines a dampening parameter \\(\\tau\\) (defaulting to zero):\n\\[\ngrad\\_avg \\leftarrow (1 - \\tau) grad + \\mu \\cdot grad\\_avg\n\\]\nJeremy Howard from Fastai suggests we take a weighted average between the grad and the running average. This is equivalent to setting \\(\\tau = \\mu\\) (dampening = momentum) in PyTorch:\n\\[\ngrad\\_avg \\leftarrow (1-c) grad + c \\cdot grad\\_avg\n\\]\n\n #| export\nclass Momentum(SGD):\n    def __init__(self, params, lr, wd=0., mom=0.9):\n        super().__init__(params, lr=lr, wd=wd)\n        self.mom=mom\n\n    def opt_step(self, p):\n        if not hasattr(p, 'grad_avg'): p.grad_avg = torch.zeros_like(p.grad)\n        p.grad_avg = p.grad_avg*self.mom + p.grad*(1-self.mom)\n        p -= self.lr * p.grad_avg\n\n\nset_seed(1)\n\nmodel = cnn_layers().apply(init_weights)\noptim_func = Momentum\nlr = 1.5\n\nl = Learner(model, dls, F.cross_entropy, optim_func, lr, subs)\nl.fit(5)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\naccuracy\n\n\n\n\n0\ntrain\n0.606\n0.784\n\n\n0\neval\n0.439\n0.838\n\n\n1\ntrain\n0.359\n0.869\n\n\n1\neval\n0.372\n0.861\n\n\n2\ntrain\n0.322\n0.881\n\n\n2\neval\n0.372\n0.864\n\n\n3\ntrain\n0.293\n0.892\n\n\n3\neval\n0.329\n0.878\n\n\n4\ntrain\n0.271\n0.900\n\n\n4\neval\n0.314\n0.886\n\n\n\n\n\n\n\n\nGeoffrey Hinton in this lecture on Momentum mentions that one problem with using plain SGD with a large learning rate is that you can get divergent oscillations if you try to travel down a long and narrow canyon. This would be similar to what happens on the left hand side of the figure above, but now with a learning rate so big that we actually diverge from the goal. By adding momentum he argues, it‚Äôs easier to find the ‚Äúdominant‚Äù (horizontal) direction and we can thus increase the learning rate without causing divergent oscillations.\nOne issue he describes, is that in the beginning of training when the gradients are still large, the momentum should be kept much smaller since otherwise the updates become just too big. One way to deal with these would be to change the momentum during training (but more about that later when we start looking into schedulers) or by using something like RMSProp:"
  },
  {
    "objectID": "posts/12_nntrain_accelerations/index.html#rmsprop",
    "href": "posts/12_nntrain_accelerations/index.html#rmsprop",
    "title": "nntrain (4/n): Accelerated optimization",
    "section": "RMSProp",
    "text": "RMSProp\nRMSProp is a technique (first?) described by Geoffrey Hinton in an online MOOC. Jeremy Howard describes RMSProp as ‚ÄúMomentum, but with the squares of the gradient‚Äù. I personally think this is a rather bad explanation, since this is not at all what‚Äôs going on. Instead it‚Äôs a technique that is concerned with the differences in magnitude of the gradients of different weights. This leads to weights getting updated at very different ‚Äúspeeds‚Äù. E.g. if you have one weight which gradient is 10 times the gradient of another, that weight get‚Äôs an update that‚Äôs 10 times as large as the other one.\nThis can exist between weights at one single step, but also between gradients of a single weight during different times in training.\nThe reason for this is of course that the learning rate is equal at all times for all parameters. RMSProp is a technique which scales the gradients inversely to the (averaged out) magnitude of the gradient:\n\\[\nw \\leftarrow w - lr \\cdot \\frac{grad}{\\sqrt{acc\\_sqrd\\_grad}}\n\\]\nwhere\n\\[\nacc\\_sqrd\\_grad = c \\cdot acc\\_sqrd\\_grad + (1-c) grad^2\n\\]\nFrom which it should be clear that using RMSprop has a large impact on the weight updates, you will thus have to recalibrate your learning rate.\n\n #| export\nclass RMSProp(SGD):\n    def __init__(self, params, lr, wd=0., sqr_mom=0.99, eps=1e-5):\n        super().__init__(params, lr=lr, wd=wd)\n        self.sqr_mom = sqr_mom\n        self.eps = eps\n\n    def opt_step(self, p):\n        if not hasattr(p, 'sqr_avg'): \n            p.sqr_avg = p.grad**2\n        p.sqr_avg = p.sqr_avg*self.sqr_mom + (1-self.sqr_mom)*p.grad**2\n        p -= self.lr * p.grad/(p.sqr_avg.sqrt() + self.eps)\n\n\nset_seed(1)\n\nmodel = cnn_layers().apply(init_weights)\noptim_func = RMSProp\nlr = 3e-3                        # much smaller learning rate!\n\nl = Learner(model, dls, F.cross_entropy, optim_func, lr, subs)\nl.fit(5)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\naccuracy\n\n\n\n\n0\ntrain\n0.699\n0.754\n\n\n0\neval\n0.472\n0.831\n\n\n1\ntrain\n0.421\n0.847\n\n\n1\neval\n0.452\n0.837\n\n\n2\ntrain\n0.373\n0.862\n\n\n2\neval\n0.387\n0.858\n\n\n3\ntrain\n0.344\n0.874\n\n\n3\neval\n0.378\n0.860\n\n\n4\ntrain\n0.322\n0.881\n\n\n4\neval\n0.352\n0.875"
  },
  {
    "objectID": "posts/12_nntrain_accelerations/index.html#adam",
    "href": "posts/12_nntrain_accelerations/index.html#adam",
    "title": "nntrain (4/n): Accelerated optimization",
    "section": "Adam",
    "text": "Adam\nAdam is probably the most common optimizer used in practice, and is nothing more then the combination of RMSProp and Momentum:\n\\[\nw \\leftarrow w - lr \\cdot \\frac{avg\\_grad}{\\sqrt{acc\\_sqrd\\_grad}}\n\\]\n\n #| export\nclass Adam(SGD):\n    def __init__(self, params, lr, wd=0., beta1=0.9, beta2=0.99, eps=1e-5):\n        super().__init__(params, lr=lr, wd=wd)\n        self.beta1,self.beta2,self.eps = beta1,beta2,eps\n\n    def opt_step(self, p):\n        if not hasattr(p, 'avg'): \n            p.avg = torch.zeros_like(p.grad.data)\n            p.sqr_avg = torch.zeros_like(p.grad.data)\n            \n        p.avg = self.beta1*p.avg + (1-self.beta1)*p.grad\n        unbias_avg = p.avg / (1 - (self.beta1**(self.i+1)))\n        p.sqr_avg = self.beta2*p.sqr_avg + (1-self.beta2)*(p.grad**2)\n        unbias_sqr_avg = p.sqr_avg / (1 - (self.beta2**(self.i+1)))\n        p -= self.lr * unbias_avg / (unbias_sqr_avg + self.eps).sqrt()\n\n\nset_seed(1)\n\nmodel = cnn_layers().apply(init_weights)\noptim_func = Adam\nlr = 6e-3\n\nl = Learner(model, dls, F.cross_entropy, optim_func, lr, subs)\nl.fit(5)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\naccuracy\n\n\n\n\n0\ntrain\n0.609\n0.783\n\n\n0\neval\n0.449\n0.838\n\n\n1\ntrain\n0.366\n0.865\n\n\n1\neval\n0.373\n0.866\n\n\n2\ntrain\n0.320\n0.882\n\n\n2\neval\n0.365\n0.863\n\n\n3\ntrain\n0.290\n0.893\n\n\n3\neval\n0.325\n0.881\n\n\n4\ntrain\n0.269\n0.902\n\n\n4\neval\n0.323\n0.882"
  },
  {
    "objectID": "posts/12_nntrain_accelerations/index.html#learning-rate-schedulers",
    "href": "posts/12_nntrain_accelerations/index.html#learning-rate-schedulers",
    "title": "nntrain (4/n): Accelerated optimization",
    "section": "Learning Rate Schedulers",
    "text": "Learning Rate Schedulers\nNow let‚Äôs look at learning rate schedulers: these are objects that change important parameters such as the learning rate and momentum during training. The OneCycleLR scheduler for example starts with a low learning rate, then increases it to a maximum value and then anneales it back to the lower value.\nThe idea being that initially, when our weights are random, we don‚Äôt want to take too large steps. As the weights are getting somewhat reasonable, we speed up the learning and once we arrive at a better solution because of this, we decrease the learning rate again to make sure we can squeeze out the last bit of performance gain:\n\nopt = torch.optim.SGD(model.parameters(), lr=3e-3)\n\n# Every scheduler has the concept of steps. \n# Either specified as done here or as \"epochs\" and \"steps_per_epoch\" (=# of batches) as two separate args\nsteps = 100   \nsch = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=6e-2, total_steps=steps)\n\n# The scheduler has a convenience method for getting the current learning rate\nlrs = [sch.get_last_lr()]\n# For other parameters such as momentum, we have to look into the optimizer itself\nmoms = [opt.param_groups[0]['momentum']]\nfor i in range(100):\n    opt.step()                 \n    sch.step()                       # calling step() on the scheduler updates the parameters of the optimizer\n    lrs.append(sch.get_last_lr())\n    moms.append(opt.param_groups[0]['momentum'])\n\nfig, axs = plt.subplots(1,2, figsize=(10,5))\naxs[0].plot(lrs);\naxs[0].set_title('learning rate');\naxs[1].plot(moms);\naxs[1].set_title('momentum');\n\n\n\n\nBesides the cycling of the learning rate, the momentum is cycled inversely with the OneCycleLr scheduler\nLet‚Äôs see if we can create a Subscriber that enables the use of Schedulers in our framework:\n\n #| export\nclass SchedulerS(Subscriber):\n    def __init__(self, scheduler_class):\n        self.scheduler_class = scheduler_class\n    \n    # intialize the scheduler instance after the optimizer has been intialized\n    def before_fit(self, learn):\n        self.scheduler = self.scheduler_class(learn.opt) \n        \n    # step the scheduler after the optimizer has stepped\n    def after_step(self, learn):\n        self.scheduler.step()\n\n\nset_seed(1)\n\nepochs = 5\nN = len(dls.train)\n\nmodel = cnn_layers().apply(init_weights)\nlr = 6e-2\nsch = partial(torch.optim.lr_scheduler.OneCycleLR, max_lr=lr, epochs=epochs, steps_per_epoch=N)\nsubs_sch = subs + [SchedulerS(sch)]\n\nl = Learner(model, dls, F.cross_entropy, torch.optim.Adam, lr, subs_sch)\nl.fit(epochs)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\naccuracy\n\n\n\n\n0\ntrain\n0.679\n0.762\n\n\n0\neval\n0.818\n0.763\n\n\n1\ntrain\n0.386\n0.859\n\n\n1\neval\n0.372\n0.860\n\n\n2\ntrain\n0.298\n0.889\n\n\n2\neval\n0.321\n0.880\n\n\n3\ntrain\n0.244\n0.910\n\n\n3\neval\n0.278\n0.894\n\n\n4\ntrain\n0.208\n0.923\n\n\n4\neval\n0.265\n0.902\n\n\n\n\n\n\n\n\nAnd that helps us to reach 90% accuracy! Not bad for a simple convolutional neural network"
  },
  {
    "objectID": "posts/12_nntrain_accelerations/index.html#resnet",
    "href": "posts/12_nntrain_accelerations/index.html#resnet",
    "title": "nntrain (4/n): Accelerated optimization",
    "section": "ResNet",
    "text": "ResNet\nNext, let‚Äôs see if we can increase the depth of the model even more: we can use a reduced stride (=1) in the first convolution so that we don‚Äôt reduce the pixel-grid. We can thus add one more convolution at the end, to end up with a 1x1 pixel grid:\n\ndef cnn_layers_more_depth():\n    return nn.Sequential(                               # 28x28  \n        conv_block(1 , 8, stride=1),                    # 28x28\n        conv_block(8 ,16),                              # 14x14\n        conv_block(16,32),                              # 7x7\n        conv_block(32,64),                              # 4x4\n        conv_block(64,128),                             # 2x2\n        conv_block(128,10, norm=False, act=False),      # 1x1\n        nn.Flatten())\n\nset_seed(1)\n\nmodel = cnn_layers_more_depth().apply(init_weights)\n\nl = Learner(model, dls, F.cross_entropy, torch.optim.Adam, lr, subs_sch)\nl.fit(epochs)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\naccuracy\n\n\n\n\n0\ntrain\n0.571\n0.801\n\n\n0\neval\n0.600\n0.821\n\n\n1\ntrain\n0.325\n0.880\n\n\n1\neval\n0.399\n0.864\n\n\n2\ntrain\n0.248\n0.909\n\n\n2\neval\n0.302\n0.894\n\n\n3\ntrain\n0.192\n0.929\n\n\n3\neval\n0.234\n0.914\n\n\n4\ntrain\n0.143\n0.948\n\n\n4\neval\n0.228\n0.919\n\n\n\n\n\n\n\n\nThe question arises whether we can keep increasing the depth of our model to boost our performance. Since we already exhausted the pixel grid, the only way would be to add more stride 1 convolutions.\nHowever, this doesn‚Äôt improve performance. Kaiming et al found in ‚ÄúDeep Residual Learning for Image Recognition‚Äù that deeper CNN models often perform worse then shallow CNN models. This is surprising since a deeper model can be understood as a superset of a more shallow model. It‚Äôs easy to see this if we consider that a deeper model can just duplicate the shallow model in it‚Äôs initial layers matching the layers of the shallow model, and keep it‚Äôs excess layers as an identity, passing through the data as is. But the observation they made, is that during training deeper model are performing worse and thus this ‚Äúoptimum‚Äù is apparently not found while training the deeper model.\nTo help the training process of deeper models, they came up with an architecture which should be able to more easily replicate the shallow network. And the way they did this, was by adding skip connections:\n\n\n\nResBlock consisting of two paths: a convolutional path, and a path in which the signal is propagated directly\n\n\nSo instead of having a fully sequential model, they added these identity skip connections which connects both the input to and the output of the convolutional path. They argued that by doing so, it would be trivial for the model to forward the data without modifying it. This way, the model could use the convolutional path for the depth of ‚Äúshallow‚Äù model, and use the skip connections for the remaining layers that only the ‚Äúdeep‚Äù model has.\nWhy a double convolutional layer? For that we come back to the idea of adding stride 1 convolution to increase the depth of our model. In the architecture described above, the first convolutional layer is a stride 1 convolution (keeping the pixel grid constant) and increases the number of filters. The second convolution is a stride 2 convolution which reduces the pixel grid by a factor of 2, but keeps the number of filters constant. The first convolution is thus expanding the data, and the second convolution compresses it, this is different from the standard ResBlock implementation in Pytorch and works significantly better in my experiments.\nThe name for the block shown above is a ResBlock, and a model that is using them is called a ResNet. Res is short for residual, and that name becomes apparent when we look at this block in the following way:\n\\[\\begin{aligned}\n& y &&= x + conv_2(conv_1(x)) \\\\\n& y - x &&= conv_2(conv_1(x)) \\\\\n& residual &&= conv_2(conv_1(x))\n\\end{aligned}\\]\nThe term \\(y-x\\) is often called ‚Äúthe residual‚Äù in statistics. Now let‚Äôs see how we can create such a ResBlock, we start with the convolutional path:\n\n #| export\ndef conv_conn(in_c, out_c, kernel_size=3, stride=2):\n    return nn.Sequential(\n        conv_block(in_c, out_c, kernel_size=kernel_size, stride=1, act=True, norm=True),\n        conv_block(out_c, out_c, kernel_size=kernel_size, stride=stride, act=False, norm=True)\n    )\n\nAnd let‚Äôs create the ResBlock with a couple of notes upfront:\n\nWe can only use a ‚Äútrue‚Äù identity connection (y_id = x) under the condition that the convolutions don‚Äôt change the number of filters (in_c = out_c) nor the pixel grid (stride=1). In other words, we can only use a true identity connection if the trailing 3 dimensions in our batch NCHW stay constant. If this is not the case, we have to ‚Äúmatch up‚Äù the identity connection as simple as possible to these new dimensions:\n\nIf the number of filters has changed we thus add a convolution with stride and kernel size equal to 1, only changing the number of channels.\nIf the pixel grid has changed (halved in size due to a stride 2 convolution in the convolutional connection), we reduce the size of pixel grid in the same way, making use of a AvgPool2d layer, effectively averaging with a 2x2 kernel size.\n\nNote also that the second convolution doesn‚Äôt have an activation, this is because we want to apply the activation after summing both the contributions from the identity path and the convolutional path\n\n\n #| export\nclass ResBlock(nn.Module):\n    def __init__(self, in_c, out_c, stride=2):\n        super().__init__()\n        self.in_c = in_c\n        self.out_c = out_c\n        self.stride = stride\n        self.conv_conn = conv_conn(in_c, out_c, stride=stride)\n        self.identity_conn = conv_block(in_c, out_c, kernel_size=1, stride=1, act=False, norm=False)\n        self.pooling = torch.nn.AvgPool2d(2, ceil_mode=True)\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        y_conv = self.conv_conn(x)\n        if self.in_c == self.out_c: y_id = x\n        elif self.stride == 1:\n            y_id = self.identity_conn(x)\n        else:\n            y_id = self.pooling(self.identity_conn(x))\n        return self.relu(y_conv + y_id)\n\nCreating the ResNet is very similar to what we have done before, we basically replace the conv_block with a ResBlock. Additionally we change the head of the model: First we flatten, then we use a linear layer to map to the required dimensions (10 output categories) and we conclude with a final BatchNorm layer.\n\n\n\n\n\n\nNote\n\n\n\nAlthough the final BatchNorm layer has a positive impact on performance, I don‚Äôt have a good intuition as to the reasons why. I would think that the distributions of logits would be not very gaussian in an ideal setting: the correct categories should get much higher outputs then the other ones. Perhaps this works in some second order effect as regularization, or perhaps it has to do with the scaling that the BatchNorm layer does after the normalization. If anybody has a good intuition for this, please let me knowüôè\n\n\n\n #| export\ndef resnet():\n    return nn.Sequential(                             # pixel grid input: 28x28  \n        ResBlock(1 , 8, stride=1),                    # 28x28\n        ResBlock(8 ,16),                              # 14x14\n        ResBlock(16,32),                              # 7x7\n        ResBlock(32,64),                              # 4x4\n        ResBlock(64,128),                             # 2x2\n        ResBlock(128,256),                            # 1x1\n        nn.Flatten(),                                 # flatten to 256 features\n        nn.Linear(256, 10, bias=False),               # linear layer to map to 10 output features\n        nn.BatchNorm1d(10)                            # final batchnorm layer\n    )\n\n\nset_seed(1)\n\nmodel = resnet().apply(init_weights)\n\nlrfind = LRFindS()\n\nlrfind_subs = [norm,\n               DeviceS(device),\n               lrfind]\n\nstart_lr = 1e-5\nl = Learner(model, dls, F.cross_entropy, torch.optim.Adam, start_lr, lrfind_subs)\nl.fit(1)\n\nlrfind.plot()\n\n\n\n\n\nset_seed(1)\n\nmodel = resnet().apply(init_weights)\n\nlr = 1e-2\nsch = partial(torch.optim.lr_scheduler.OneCycleLR, max_lr=lr, epochs=epochs, steps_per_epoch=N)\n\nsubs_sch = subs + [SchedulerS(sch)]\n\nl = Learner(model, dls, F.cross_entropy, torch.optim.Adam, lr, subs_sch)\nl.fit(epochs)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\naccuracy\n\n\n\n\n0\ntrain\n0.815\n0.783\n\n\n0\neval\n0.578\n0.846\n\n\n1\ntrain\n0.399\n0.889\n\n\n1\neval\n0.372\n0.885\n\n\n2\ntrain\n0.282\n0.914\n\n\n2\neval\n0.292\n0.906\n\n\n3\ntrain\n0.212\n0.934\n\n\n3\neval\n0.257\n0.918\n\n\n4\ntrain\n0.159\n0.953\n\n\n4\neval\n0.237\n0.927\n\n\n\n\n\n\n\n\nWhich reaches a score of almost 93% within 5 epochs!"
  },
  {
    "objectID": "posts/12_nntrain_accelerations/index.html#parameters-and-macs",
    "href": "posts/12_nntrain_accelerations/index.html#parameters-and-macs",
    "title": "nntrain (4/n): Accelerated optimization",
    "section": "Parameters and MACs",
    "text": "Parameters and MACs\nNow that we are able to create fairly deep models, it‚Äôs a good idea to be able to track the memory footprint and the amount of compute that is involved in doing a single forward pass. To track the memory, we can look at the amount of parameters of each layer, and for the compute we can try to estimate something equivalent to Multiply‚Äìaccumulate operations (MACs). Let‚Äôs create a small Subscriber that tracks thes stats by running one single batch and then cancels the fit:\n\n #| export\nclass ModelMonitorS(Subscriber):\n    \n    def __init__(self, modules): self.modules = modules\n    \n    def before_fit(self, learn):\n        self.hooks = [Hook(i, module, partial(self.record_stats, learn)) for i, module in enumerate(self.modules)]\n        \n    def record_stats(self, learn, hook, layer, inp, outp):\n        if learn.model.training:\n            hook.nparams = sum(submodule.numel() for submodule in layer.parameters())\n            if isinstance(layer, ResBlock):\n                # K √ó K √ó Cin √ó Hout √ó Wout √ó Cout source=https://machinethink.net/blog/how-fast-is-my-model/\n                mac_conv1 = 9 * layer.in_c * inp[0].shape[2] * inp[0].shape[3] * layer.out_c\n                mac_conv2 = 9 * layer.out_c * outp.shape[2] * outp.shape[3] * layer.out_c    \n                hook.mac = (mac_conv1 + mac_conv2) / 1e6\n                if layer.stride != 1:\n                    # Add identity conv\n                    hook.mac += (layer.in_c * outp.shape[2] * outp.shape[3] * layer.out_c / 1e6)\n            else:\n                hook.mac = hook.nparams / 1e6\n            hook.batch_size = inp[0].shape[0]\n            hook.in_shape = list(inp[0].shape[1:])\n            hook.out_shape = list(outp.shape[1:])\n            \n    def after_batch(self, learn):\n        for h in self.hooks: h.remove()\n        raise CancelFitException                   # Only run this for a single batch, then cancel\n        \n    def __repr__(self):\n        out = f'{\"layer\":&lt;20} : {\"input\":&lt;20} : {\"output\":&lt;20} : {\"# params\":&gt;10} : {\"# MACs\":&gt;10}\\n'\n        total_params = 0\n        total_mac = 0\n        for h in self.hooks:\n            out += f'{h.layer_name:&lt;20} : {str(h.in_shape):&lt;20} : {str(h.out_shape):&lt;20} : {h.nparams:&gt;10d} : {h.mac: 10.1f}\\n'\n            total_params += h.nparams\n            total_mac += h.mac\n        return f'{\"Total parameters:\":&lt;20}{total_params:&gt;10d} \\n{\"Total MACs:\":&lt;20}{total_mac:10.1f} \\n\\n' + out\n\n\nmodel = resnet().apply(init_weights)\n\nmodules = [module for module in model.modules() if isinstance(module, (ResBlock, nn.Linear, torch.nn.BatchNorm1d))]\nmonitor = ModelMonitorS(modules)\n\nl = Learner(model, dls, F.cross_entropy, torch.optim.Adam, lr, [monitor])\nl.fit(1)\n\nmonitor\n\nTotal parameters:      1227900 \nTotal MACs:                8.4 \n\nlayer                : input                : output               :   # params :     # MACs\n0_ResBlock           : [1, 28, 28]          : [8, 28, 28]          :        696 :        0.5\n1_ResBlock           : [8, 28, 28]          : [16, 14, 14]         :       3664 :        1.4\n2_ResBlock           : [16, 14, 14]         : [32, 7, 7]           :      14496 :        1.4\n3_ResBlock           : [32, 7, 7]           : [64, 4, 4]           :      57664 :        1.5\n4_ResBlock           : [64, 4, 4]           : [128, 2, 2]          :     230016 :        1.8\n5_ResBlock           : [128, 2, 2]          : [256, 1, 1]          :     918784 :        1.8\n6_Linear             : [256]                : [10]                 :       2560 :        0.0\n7_BatchNorm1d        : [10]                 : [10]                 :         20 :        0.0\n\n\nFrom which we see how data flows through the model, where most parameters are and where most compute is being spend. Although the MACs computation isn‚Äôt perfectly valid, it gives a good indication. It especially shows:\n\nmost parameters are in later layers, those are the layers that cost most memory. In fact the final ResBlock uses more memory as all earlier ResBlocks combined!\nthis is also where most compute is being used, although the differences between earlier layers is much less pronounced then for the memory footprint, the reason for this is that the pixel grid is reduced a lot in those later layers"
  },
  {
    "objectID": "posts/12_nntrain_accelerations/index.html#augmentation",
    "href": "posts/12_nntrain_accelerations/index.html#augmentation",
    "title": "nntrain (4/n): Accelerated optimization",
    "section": "Augmentation",
    "text": "Augmentation\nWe have finally come to the point of data augmentation. Once you have a good model, you would ideally train for much longer times. When doing so, the problem that arises is that of overfitting. Which we described before as the tendency of the model to start to memorize our training data and no longer generalize well to the validation set.\nTo overcome this, data augmentation is used to increase the variety in the training data. For example we can rotate the images, or (horizontally or vertically) flip the image. For such augmentations the label stays the same, since a rotated image of a shoe is still a shoe, but there exist also augmentations that for example mix images, and thus also the labels are altereed. There are great libraries out there such as Albumentations with a huge variety of transformations that can be applied to images, so we won‚Äôt be going into the full details. But let‚Äôs at least figure out how we can build this into our framework.\n\n #| export\nclass AugmentS(Subscriber):\n    def __init__(self, transform):\n        self.transform = transform\n        \n    def before_batch(self, learn):\n        if learn.model.training:                    # augmentations are only applied to the training data\n            learn.batch[0] = self.transform(learn.batch[0])\n\n\nfrom torchvision import transforms\n\naugs = nn.Sequential(transforms.RandomHorizontalFlip(), \n                     transforms.RandomCrop(size=28, padding=1))\n\n\nset_seed(1)\n\nmodel = resnet().apply(init_weights)\n\nepochs=20\nlr = 1e-2\nsch = partial(torch.optim.lr_scheduler.OneCycleLR, max_lr=lr, epochs=epochs, steps_per_epoch=N)\n\nsubs_sch = subs + [SchedulerS(sch), AugmentS(augs)]\n\nl = Learner(model, dls, F.cross_entropy, torch.optim.Adam, lr, subs_sch)\nl.fit(epochs)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\naccuracy\n\n\n\n\n0\ntrain\n1.052\n0.694\n\n\n0\neval\n0.757\n0.778\n\n\n1\ntrain\n0.641\n0.837\n\n\n1\neval\n0.591\n0.841\n\n\n2\ntrain\n0.492\n0.869\n\n\n2\neval\n0.587\n0.838\n\n\n3\ntrain\n0.401\n0.882\n\n\n3\neval\n0.407\n0.871\n\n\n4\ntrain\n0.340\n0.893\n\n\n4\neval\n0.378\n0.873\n\n\n5\ntrain\n0.305\n0.898\n\n\n5\neval\n0.349\n0.877\n\n\n6\ntrain\n0.283\n0.902\n\n\n6\neval\n0.310\n0.896\n\n\n7\ntrain\n0.251\n0.912\n\n\n7\neval\n0.335\n0.883\n\n\n8\ntrain\n0.239\n0.915\n\n\n8\neval\n0.279\n0.901\n\n\n9\ntrain\n0.219\n0.923\n\n\n9\neval\n0.253\n0.913\n\n\n10\ntrain\n0.210\n0.926\n\n\n10\neval\n0.248\n0.911\n\n\n11\ntrain\n0.192\n0.932\n\n\n11\neval\n0.260\n0.909\n\n\n12\ntrain\n0.179\n0.937\n\n\n12\neval\n0.214\n0.926\n\n\n13\ntrain\n0.167\n0.940\n\n\n13\neval\n0.214\n0.922\n\n\n14\ntrain\n0.157\n0.945\n\n\n14\neval\n0.197\n0.933\n\n\n15\ntrain\n0.142\n0.950\n\n\n15\neval\n0.190\n0.935\n\n\n16\ntrain\n0.133\n0.954\n\n\n16\neval\n0.187\n0.935\n\n\n17\ntrain\n0.123\n0.958\n\n\n17\neval\n0.186\n0.937\n\n\n18\ntrain\n0.120\n0.959\n\n\n18\neval\n0.187\n0.938\n\n\n19\ntrain\n0.117\n0.960\n\n\n19\neval\n0.185\n0.939"
  },
  {
    "objectID": "posts/12_nntrain_accelerations/index.html#final-remarks",
    "href": "posts/12_nntrain_accelerations/index.html#final-remarks",
    "title": "nntrain (4/n): Accelerated optimization",
    "section": "Final Remarks",
    "text": "Final Remarks\nAnd that‚Äôs it. We have created a model that is performing with close to 94% accuracy on the fashion-mnist dataset, according to papers with code, that puts us in the top 10 of papers that are written about this dataset, and we did so with just 20 epochs of training and limitted data augmentation.\nIn the previous posts in this series, we have build up a small library nntrain in which we have build more or less everything from scratch, from data loading, training loop, activation tracking, initialization, convolutions, optimizers, schedulers and finally ResNets and data augmentation. We understand in detail how it all works, and it‚Äôs fairly straight forward to extend the framework to different kind of Machine Learning challenges.\nI can‚Äôt thank Jeremy Howard and the people from FastAI enough for all the wonderful things they are doing for the machine learning community, it truly is spectacular and inspiring ü§ó. To close off, I would like to share a small snippet from the last ‚Äúbase‚Äù lecture in which Jeremy Howard speaks some words about this himself and reinforces the idea that nothing in machine learning is magic, that everything can be understood with common sense and be build from scratch with enough perseverance and tenacity."
  },
  {
    "objectID": "posts/14_mlp/index.html",
    "href": "posts/14_mlp/index.html",
    "title": "Multilayer Perceptron language model",
    "section": "",
    "text": "In the previous post we had a first look at language models and discussed the n-gram model. Specifically, we:\nIn this post we are going to have a look at a neural network that is described in Bengio et al: A Neural Probabilistic Language Model which aims to circumvent this curse of dimensionality by making use of embeddings.\nThis post is inspired by the series from Andrej Karpathy‚Äôs on neural networks."
  },
  {
    "objectID": "posts/14_mlp/index.html#data",
    "href": "posts/14_mlp/index.html#data",
    "title": "Multilayer Perceptron language model",
    "section": "Data",
    "text": "Data\nEverything starts with training data, for a description see the previous post\n\n\nCode\nimport random\nfrom functools import reduce, partial\nfrom pathlib import Path\nfrom urllib.request import urlretrieve\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport torcheval.metrics as tem\nimport fastcore.all as fc\n\nfrom nntrain.dataloaders import DataLoaders\nfrom nntrain.learner import *\nfrom nntrain.activations import *\nfrom nntrain.acceleration import *\nfrom nntrain.ngram import *\n\n\n\n########### Load the data ###########\npath = Path('./data')\npath.mkdir(parents=True, exist_ok=True)\npath = path / 'names.txt'\nurl = 'https://raw.githubusercontent.com/karpathy/makemore/master/names.txt'\n\n_ = urlretrieve(url, path)\n\nwith open(path, 'r') as f:\n    lines = f.read().splitlines()\n    \nrandom.seed(42)\nrandom.shuffle(lines)\n\n### Create vocabulary and mappings ###\nunique_chars = list(set(\"\".join(lines)))\nunique_chars.sort()\nvocabulary = ['.'] + unique_chars\n\nc2i = {c:i for i, c in enumerate(vocabulary)}\ni2c = {i:c for i, c in enumerate(vocabulary)}"
  },
  {
    "objectID": "posts/14_mlp/index.html#embeddings-relation-to-integer-and-one-hot-encoding",
    "href": "posts/14_mlp/index.html#embeddings-relation-to-integer-and-one-hot-encoding",
    "title": "Multilayer Perceptron language model",
    "section": "Embeddings: relation to integer and one-hot encoding",
    "text": "Embeddings: relation to integer and one-hot encoding\nIn the previous post we introduced embeddings in the following way:\n\nAn embedding layer is a layer that is used to ‚Äúencode‚Äù our data, which roughly translates to the way we input our data into a neural network. We already saw above that we numericalized our characters (a-z) as integers. We will have to do something similar for our neural network, since we can‚Äôt input characters into a neural network. However, instead of feeding integers into our network, we can also employ an embedding layer, which will basically create a vector out of our integers. Each integer value will be mapped to it‚Äôs own vector, and the values contained in this vector will be learned during training. The advantage of this, is that the model can easily learn different ‚Äúattributes‚Äù that make up the individual tokens. For example, it could use the first dimension in the vector to denote whether the token is a vowel (a, e, i, o, u) and the second dimension to represent the likelihood of starting a sentence. The emphasis in the last sentence is on could, since these things have to be learned by the network itself during the training-process.\n\n\nInteger encoding\nThis is all well and good, but let‚Äôs add some more detail and intuition. Let‚Äôs go back to ultimate basics and see what happens if we feed numericalized tokens (integers) directly into a model using context length of 1, and a first linear layer consisting of 4 neurons (for simplicity we ignore the bias term). Since we have a context length of 1, one single sample consists of one value (the integer encoded value of our token) and is depicted in blue. We have 4 neurons, which weights are also just a single value, depicted in green. The activations of the 4 neurons are depicted in yellow.\n\n\n\nMatrix multiplication of one sample consisting of 1 token (blue matrix) with a 4 neuron linear layer (green matrix). On the bottom: the simple multiplication associated with the activation of the first neuron\n\n\nAs you can see, the integer values are just being multiplied by the weights of the linear layer. This establishes an implicit relation between our inputs (blue matrix) in terms of the activation (yellow matrix). Consider a context e (numericalized 4) and another context a (numericalized 1). Because of the linearity, the activations of the former will always be 4 times as large as the activations of the latter. No matter how we change the weights in our network, this implicit relation will always be there, and there is no good reason why this should be the case. Moreover, the factor (4) is a result from the ordering of our vocabulary. If instead we would have \\(a\\) in 10th position, and \\(e\\) in 20th position, the difference would have been a factor of 2. It should be clear that these kind of implicit linear relations don‚Äôt fit to our (nominal) data, and should only be used for ratio, interval and possibly ordinal data.\n\n\nOne-hot encoding\nSo how then should we treat these variables? If you are familiar with linear regression you have probably heard of one hot encoding. This is a technique in which we replace integer values with a vector. This vector will be of length equal to the amount of possible values the integer can take on and consists of zeros in all but one position, in that nonzero position the value will be 1. Each integer will have this 1 in a different position. For a vocabulary of length 4, the 4 vectors will be:\n\nfor i in range(4):\n    print(F.one_hot(torch.tensor(i), 4))\n\ntensor([1, 0, 0, 0])\ntensor([0, 1, 0, 0])\ntensor([0, 0, 1, 0])\ntensor([0, 0, 0, 1])\n\n\nSo we have turned our single integer variable, into 4 distinct (binary) variables. Each of these 4 variables will have it‚Äôs own associated weight (parameter) in linear regression and the contributions to the output are thus totally independant of another. To see this, let‚Äôs assume we fit a linear function: \\(y = b \\cdot x\\) where \\(x\\) is an integer variable taking on 4 values and is transformed into 4 binary variables as shown above \\(z_1, .. , z_4\\). We then have:\n\\[\\begin{align}\ny &= b_1 z_1 + b_2 z_2 + b_3 z_3 + b_4 z_4  \\\\\ny &= \\left.\n  \\begin{cases}\n    b_1, & \\text{for } x = 1 (z_1 = 1, z_2 = 0, z_3 = 0, z_4 = 0) \\\\\n    b_2, & \\text{for } x = 2 (z_1 = 0, z_2 = 1, z_3 = 0, z_4 = 0) \\\\\n    b_3, & \\text{for } x = 3 (z_1 = 0, z_2 = 0, z_3 = 1, z_4 = 0) \\\\\n    b_4, & \\text{for } x = 4 (z_1 = 0, z_2 = 0, z_3 = 0, z_4 = 1) \\\\\n  \\end{cases}\n  \\right\\}\n\\end{align}\\]\nSince \\(b_1\\) through \\(b_4\\) can take on any value during fitting, we got rid of our implicit relation between our tokens!\n\n\nEmbedding encoding\nFor a neural network, something very similar happens. Here an example when we feed these 4 vectors into a linear layer consisting of 2 neurons:\n\n\nCode\ninteger_value = 0\none_hot = F.one_hot(torch.tensor(integer_value), 4).to(torch.float32)\nlinear = torch.rand(4,2)\n\nprint(f'{\"integer value\":&lt;25}: {integer_value}')\nprint(f'{\"one hot representation\":&lt;25}: {one_hot.tolist()}')\nprint('weight matrix of linear layer:')\nprint(linear)\nprint('activations of one hot encoded vector of integer \"0\" with linear layer:')\nprint(one_hot@linear)\n\n\ninteger value            : 0\none hot representation   : [1.0, 0.0, 0.0, 0.0]\nweight matrix of linear layer:\ntensor([[0.1354, 0.9329],\n        [0.0975, 0.1335],\n        [0.4640, 0.6912],\n        [0.3926, 0.5246]])\nactivations of one hot encoded vector of integer \"0\" with linear layer:\ntensor([0.1354, 0.9329])\n\n\nSo the activations are just the weights associated with the first input to both neurons! If you are familiar with the drawing of neural networks as connected neurons, the activations are equal to the highlighted weights in the drawing below:\n\n\n\nThe highlighted weights that get pulled out of the weight matrix for a one-hot representation of integer 0\n\n\nWe conclude: multiplying a one-hot encoded vector with a weight matrix, simply returns the weights of the associated row of that weight matrix.\nMathematically this is thus equivalent to indexing into the weight matrix at the row equal to the integer value:\n\nlinear[integer_value]\n\ntensor([0.1354, 0.9329])\n\n\nIf we compare now the inputs (integer values) with the outputs (vector of floats), we have verified the statement we made earlier: However, instead of feeding integers into our network, we can also employ an embedding layer, which will basically create a vector out of our integers.\nAnd now let‚Äôs also do this explicitly with an nn.Embedding layer:\n\nemb = nn.Embedding(4, 2)\nprint('weight matrix of embedding layer:')\nprint(emb.weight.data); print('\\n')\nprint('activations of embedding layer of integer \"0\":')\nprint(emb(torch.tensor(integer_value)).data)\n\nweight matrix of embedding layer:\ntensor([[-0.5065, -0.5940],\n        [-0.9815, -0.3719],\n        [-1.2932, -1.1999],\n        [ 0.3102,  0.2803]])\n\n\nactivations of embedding layer of integer \"0\":\ntensor([-0.5065, -0.5940])\n\n\nSince the parameters of this layer are learned through backpropagation, Bengio et al refer to this as learning a distributed representation for words (in our case we use it for characters instead of words, but the idea is the same). The distributed representation (in the example above a vector of length 2), is what we previously referred to as the different attributes the model can learn that are associated with the tokens."
  },
  {
    "objectID": "posts/14_mlp/index.html#embeddings-mapping-inputs-into-n-dimensional-space",
    "href": "posts/14_mlp/index.html#embeddings-mapping-inputs-into-n-dimensional-space",
    "title": "Multilayer Perceptron language model",
    "section": "Embeddings: mapping inputs into n-dimensional space",
    "text": "Embeddings: mapping inputs into n-dimensional space\nThese different attributes is also what we refer to when saying that embedding layers map inputs into an n-dimensional space. With n being the amount of attributes, generally smaller then the size of the vocabulary. For example, if we would create an embedding layer with depth of (number of attributes) 4 we would have:\n\ninteger encoded tokens: 1D (27 possible values)\none-hot encoding: 27D (binary values: either zero or one)\nembedding: 4D (real / float valued)\n\nAnd because the weights are learned, the model can place our inputs into this 4D space in positions it finds useful (for learning the task at hand, that is). Inputs that are similar can be placed closely together, and input that are very different can be placed further apart. Moreover, since this space can (and generally is) high dimensional, it can place inputs close together on certain dimensions, but far apart in other dimensions. This allows for a differentiated representation of our inputs, where on some attributes inputs might be similar, but on other attributes very different."
  },
  {
    "objectID": "posts/14_mlp/index.html#embeddings-the-key-to-generalization",
    "href": "posts/14_mlp/index.html#embeddings-the-key-to-generalization",
    "title": "Multilayer Perceptron language model",
    "section": "Embeddings: the key to generalization",
    "text": "Embeddings: the key to generalization\nA final intuition I would like to share, has to do with being ‚Äúout of distribution‚Äù. Let‚Äôs say during training you have encountered the sentences (for this example we switch to sentences and words, instead of names and characters):\n\na dog is walking through the kitchen\na dog is walking through the living room\na dog is walking through the garden\n\nBut never a similar sentence for a cat. If an n-gram (\\(n &gt;= 6\\)) model has to fill in the next word in the sequence: ‚Äúa cat is walking through the ‚Ä¶‚Äù it wouldn‚Äôt know what to reply, because we are out of distribution: during training we never encountered such a context so it has nothing to go on. The learned probability distribution of an n-gram model is only possible when the model has seen examples of the exact context (possibly with different labels, which are the basis for the probability distribution).\nHowever, for a model using a learned distributed representation, we ideally would like the model to be able to use it‚Äôs knowledge of cats being similar to dogs, and thus use the training data it has seen on dogs to be able to answer: kitchen, living room or garden. This kind of generalization becomes possible with embeddings since the model can learn that the vector for cats is similar to the vector for dogs.\nThis similarity in vector space, will lead to similar outputs. In the words of Bengio et al:\n‚ÄúIn the proposed model, it will so generalize because ‚Äúsimilar‚Äù words are expected to have a similar feature vector, and because the probability function is a smooth function of these feature values, a small change in the features will induce a small change in the probability. Therefore, the presence of only one of the above sentences in the training data will increase the probability, not only of that sentence, but also of its combinatorial number of ‚Äúneighbors‚Äù in sentence space (as represented by sequences of feature vectors)‚Äú."
  },
  {
    "objectID": "posts/14_mlp/index.html#using-embeddings-effectively",
    "href": "posts/14_mlp/index.html#using-embeddings-effectively",
    "title": "Multilayer Perceptron language model",
    "section": "Using Embeddings effectively",
    "text": "Using Embeddings effectively\nNow that we have a better understanding of embeddings both mathematically and conceptually, let‚Äôs turn to the paper of Bengio et al.¬†Where they make smart use of embeddings to circumvent the problems with huge weight matrices for n-grams with large n.\nThe following (simplified) architecture diagram is from that paper and describes pretty well what‚Äôs going on.\nOn the bottom we see three inputs, so we are using a context of length 3. These inputs are feeded into the same embedding layer C. Also, this embedding layer as a depth smaller then the size of the vocabulary\n\n\n\n\n\n\nNote\n\n\n\nFor the n-gram neural network model we actually also used an embedding layer, but it has a depth equal to the size of the vocabulary. We were thus mapping our integers into a 27D space, you can imagine that with just 27 integers that space was super sparse!\n\n\nFor an n-gram model we would have already \\(27^3\\) parameters to accomodate all the different possibilities in the context. However, here we just have vocabulary length x embedding depth parameters. Since all the elements of the context are feed through the same embedding layer, it‚Äôs not dependant on the context length, which allows us to increase the context length without having a penalty on the amount of parameters. This implicitly means, that the position of the token is ignored as far as the embedding layer is concerned. That is, a letter e in first or second position will give the same embedding activation.\nNext, the three embedding vectors are concatenated and passed through a hidden linear layer with a tanh activation function. This is thus where the positional information is dealt with in this model.\nFinally the activations get passed through another linear layer, which maps the activations to the correct amount of classes (i.e.¬†the number of tokens in the vocabulary).\n\nLet‚Äôs build this model and train it with nntrain:\n\n# Datasets and loaders\n\ntrain_size=0.8\nval_size=0.1\n\ntrain_lines = lines[0:int(train_size * len(lines))]\nval_lines = lines[int(train_size * len(lines)): int((train_size + val_size) * len(lines))]\n\ntrain_ds = NgramDataset(train_lines, c2i, n=4)\nval_ds = NgramDataset(val_lines, c2i, n=4)\n\nbs = 1000\ntrain_loader = torch.utils.data.DataLoader(train_ds, shuffle=True, batch_size=bs, num_workers=4)\nval_loader = torch.utils.data.DataLoader(val_ds, batch_size=bs*2, num_workers=4)\ndls = DataLoaders(train_loader, val_loader)\n\nAs a reminder, let‚Äôs have a look at the samples for the first name in the dataset (‚ÄúYuheng‚Äù)\n\nxb, yb = train_ds[0:6]\nfor x,y in zip(xb, yb):\n    print(list(i2c[i.item()] for i in x), '--&gt;' ,i2c[y.item()])\n\n['.', '.', '.'] --&gt; y\n['.', '.', 'y'] --&gt; u\n['.', 'y', 'u'] --&gt; h\n['y', 'u', 'h'] --&gt; e\n['u', 'h', 'e'] --&gt; n\n['h', 'e', 'n'] --&gt; g\n\n\n\nclass MLP(nn.Module):\n    def __init__(self, c2i, emb_dim, ctx_len, n_hidden):\n        super().__init__()\n        self.ctx_len = ctx_len\n        self.emb    = nn.Embedding(len(c2i), emb_dim)\n        self.hidden = nn.Linear(emb_dim*ctx_len, n_hidden)\n        self.tanh   = nn.Tanh()\n        self.out    = nn.Linear(n_hidden, len(c2i))\n        \n    def forward(self, x):\n        out = self.emb(x)\n        out = out.view(out.shape[0], -1)\n        out = self.hidden(out)\n        out = self.tanh(out)\n        out = self.out(out)\n        return out\n\nLet‚Äôs find a good learning rate:\n\nmlp = MLP(c2i, emb_dim=2, ctx_len=3, n_hidden=50)\n\nsubs = [DeviceS(device)]\n\nlrfind = LRFindS()\nl = Learner(mlp, dls, F.cross_entropy, torch.optim.Adam, None, subs=subs + [lrfind])\nl.fit(5, lr=1e-4)\nlrfind.plot()\n\n\n\n\nLet‚Äôs take a learning rate of 3e-2, and use the OneCycleLR scheduler together with an Adam optimizer:\n\nepochs = 5\nN = len(train_loader)\nlr = 3e-2\n\nmlp = MLP(c2i, emb_dim=2, ctx_len=3, n_hidden=50)\n\nsch = partial(torch.optim.lr_scheduler.OneCycleLR, max_lr=lr, epochs=epochs, steps_per_epoch=N)\n\nsubs = [ProgressS(True),\n        MetricsS(),\n        DeviceS(device)]\n\nl = Learner(mlp, dls, F.cross_entropy, torch.optim.Adam, None, subs=subs + [SchedulerS(sch)])\nl.fit(epochs, lr=lr)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\n\n\n\n\n0\ntrain\n2.697\n\n\n0\neval\n2.444\n\n\n1\ntrain\n2.395\n\n\n1\neval\n2.358\n\n\n2\ntrain\n2.336\n\n\n2\neval\n2.312\n\n\n3\ntrain\n2.304\n\n\n3\neval\n2.288\n\n\n4\ntrain\n2.285\n\n\n4\neval\n2.280\n\n\n\n\n\n\n\n\nNext, let‚Äôs see if we can tweak the hyperparameters to get the best performance. Below I show the result of trying for around 10 minutes to get the best results:\n\ndef get_dls(bs, context_length):\n    train_ds = NgramDataset(train_lines, c2i, n=context_length+1)\n    val_ds = NgramDataset(val_lines, c2i, n=context_length+1)\n    train_loader = torch.utils.data.DataLoader(train_ds, shuffle=True, batch_size=bs, num_workers=4)\n    val_loader = torch.utils.data.DataLoader(val_ds, batch_size=bs*2, num_workers=4)\n    dls = DataLoaders(train_loader, val_loader)\n    return dls\n\n\n# Train parameters\nepochs = 5\nlr     = 3e-2\nbs     = 1000\n\n# Hyperparameters\ncontext_length = 15\nnh             = 350\nemb_dim        = 20\n\ndls = get_dls(bs, context_length)\nmlp = MLP(c2i, emb_dim, context_length, nh)\nscheduler = partial(torch.optim.lr_scheduler.OneCycleLR, max_lr=lr, epochs=epochs, steps_per_epoch=len(dls.train))\n\nl = Learner(mlp, dls, F.cross_entropy, torch.optim.Adam, None, subs=subs + [SchedulerS(scheduler)])\nl.fit(epochs, lr=lr)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\n\n\n\n\n0\ntrain\n2.405\n\n\n0\neval\n2.284\n\n\n1\ntrain\n2.254\n\n\n1\neval\n2.238\n\n\n2\ntrain\n2.150\n\n\n2\neval\n2.104\n\n\n3\ntrain\n2.011\n\n\n3\neval\n2.022\n\n\n4\ntrain\n1.890\n\n\n4\neval\n1.990\n\n\n\n\n\n\n\n\nAnd this is looking pretty good. The loss we obtained from n-gram models are around 2.139, with this model we are around 2."
  },
  {
    "objectID": "posts/14_mlp/index.html#tanh-activation-function",
    "href": "posts/14_mlp/index.html#tanh-activation-function",
    "title": "Multilayer Perceptron language model",
    "section": "Tanh activation function",
    "text": "Tanh activation function\nThe MLP model we have used, is employing a tanh activation function which we haven‚Äôt encountered so far. So let‚Äôs have a quick look at it:\n\nx = np.linspace(-5,5,100)\ny = np.tanh(x)\n\nfig, axs = plt.subplots(1,1, figsize=(10,5))\naxs.plot(x,y);\naxs.grid();\naxs.axhline(0, c='black');\naxs.axvline(0, c='black');\n\n\n\n\nObserve that the tanh activation function has y limits at -1 and 1: however large or small the input, the output is always between -1 and 1. These asymptotes are very flat, meaning that the derivatives in these areas are extremely small. Thinking back at the learning mechanism of neural networks, and remembering that gradients are flowing back through the network by making use of the chain rule: having very small local gradients of a tanh activation function leads to the cancelling-out of whatever gradient has been accumulated so far. This means that the weight updates of all upstream weights is impacted. Previously, this behavior was mentioned in terms of the ReLU activation function, see here.\nThis becomes a serious problem, if for all our data (in a batch, or even worse in an epoch) this is the case. So let‚Äôs check how we are doing by creating a ActivationStatsS subclass tailored towards tracking stats for the tanh activation.\n\nclass TanhActivationS(ActivationStatsS):\n    \n    def record_stats(self, learn, hook, layer, inp, outp):\n        if learn.model.training:\n            if not hasattr(hook, 'stats'): hook.stats = ([], [])\n            acts = outp.detach().cpu()\n            hook.stats[0].append(acts.histc(100,-1,1))           # get the histogram counts with 100 bins in the range (-1,1)\n            \n            # computation of the not_firing_rate_per_act\n            N = acts.shape[0]                 \n            flat = acts.view(N, -1)                              # flatten the activations: matrix of [samples, activations]\n            nf_rate_p_act = (flat.abs() &gt; 0.99).sum(dim=0) / N   # compute not firing rate per activations (so across the samples)\n            hook.stats[1].append(nf_rate_p_act)   \n\n\n# Train parameters\nepochs = 5\nlr     = 3e-2\nbs     = 1000\n\n# Hyperparameters\ncontext_length = 15\nnh             = 350\nemb_dim        = 20\n\ndls = get_dls(bs, context_length)\nmlp = MLP(c2i, emb_dim, context_length, nh)\nscheduler = partial(torch.optim.lr_scheduler.OneCycleLR, max_lr=lr, epochs=epochs, steps_per_epoch=len(dls.train))\ntanhS = TanhActivationS([m for m in mlp.modules() if isinstance(m, nn.Tanh)])\n\nl = Learner(mlp, dls, F.cross_entropy, torch.optim.Adam, None, subs=subs + [SchedulerS(scheduler)] + [tanhS])\nl.fit(epochs, lr=lr)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\n\n\n\n\n0\ntrain\n2.383\n\n\n0\neval\n2.262\n\n\n1\ntrain\n2.257\n\n\n1\neval\n2.232\n\n\n2\ntrain\n2.150\n\n\n2\neval\n2.126\n\n\n3\ntrain\n2.015\n\n\n3\neval\n2.035\n\n\n4\ntrain\n1.902\n\n\n4\neval\n1.997\n\n\n\n\n\n\n\n\nAnd, let‚Äôs now plot all histograms of the activations during training as a heatmap:\n\nfig, ax = plt.subplots(2,1, figsize=(15,10))\n\nhist = torch.stack(tanhS.hooks[0].stats[0]).T\nax[0].imshow(hist, cmap='Blues', origin='lower', aspect='auto')\nax[0].set_yticks(np.arange(-0.5, 100.5, 10), [round(i, 1) for i in np.linspace(-1, 1, 11)]);\nax[1].imshow(hist[:,:20], cmap='Blues', origin='lower', aspect='auto')\nax[1].set_yticks(np.arange(-0.5, 100.5, 10), [round(i, 1) for i in np.linspace(-1, 1, 11)]);\n\n\n\n\nOn the top, the histograms are shown for all of training (800+ batches), and on the bottom the histograms for the first 20 batches are shown. We observe that during the beginning of training, we don‚Äôt have over-saturated tanh activations, but as training progresses the activations become more and more saturated. We can also look at the dead rate per neuron: which is the amount of activations with absolute value &gt; 0.99 across a batch per neuron:\n\nfig, ax = plt.subplots(1,1, figsize=(15,5))\n\ndead_rate = torch.stack(tanhS.hooks[0].stats[1]).T\nax.imshow(dead_rate, cmap='Greys', origin='lower', aspect='auto');\n\n\n\n\nHowever, this doesn‚Äôt seem to be problematic. Foremost, we need to make sure that we don‚Äôt initialize in a way that causes over-saturated tanh activations. And as mentioned, during the beginning of training, this is not the case. Also, when looking at a histogram of the dead rate per neuron across the very last training batch we see that most neurons have a low dead-rate, and we don‚Äôt have any neuron for which the dead-rate is higher then 75%:\n\nplt.hist(dead_rate[:,-1], bins=20);"
  },
  {
    "objectID": "posts/14_mlp/index.html#sampling-names",
    "href": "posts/14_mlp/index.html#sampling-names",
    "title": "Multilayer Perceptron language model",
    "section": "Sampling names",
    "text": "Sampling names\nTo conclude, let‚Äôs sample from this model:\n\n@fc.patch\ndef generate(self:MLP, n=10, generator=None):\n    names = []\n    for i in range(n):\n        name = '.' * (self.ctx_len)\n        while True:\n            idx = torch.tensor([[c2i[i] for i in name[-(self.ctx_len):]]]).cuda()\n            logits = self.forward(idx)\n            s = torch.multinomial(F.softmax(logits, dim=1), 1, generator=generator)\n            c = i2c[s.item()]\n            name += c\n            if c == '.':\n                names.append(name)\n                break\n    return names\n\n\nnames = mlp.generate()\n[name[15:-1] for name in names]\n\n['xenleigh',\n 'samyah',\n 'milena',\n 'camrihan',\n 'nafraya',\n 'aris',\n 'marcely',\n 'zopierah',\n 'serg',\n 'oshem']\n\n\nAnd these neural utterings are starting to sound pretty name-like!"
  },
  {
    "objectID": "posts/16_lstm/index.html",
    "href": "posts/16_lstm/index.html",
    "title": "Long short-term memory",
    "section": "",
    "text": "In the previous post we went in detail through Recurrent Neural Networks (RNN‚Äôs). We looked at how they are similar to MLPs and how they allow for a large improvement in processing data.\nOne main problem with RNN‚Äôs is that it‚Äôs still relatively difficult for the model to maintain information on something that has happened a long time ago. If in one sentence the subject turns out to be a male, in the next sentence the model should still remember this, and use ‚Äúhe‚Äù as a pronoun when referring to this person. One way to do this, is by extending the sequence length to large values, but this brings its own problems: exploding and or vanishing gradients because of the increased depth of the model.\nAlthough there are multiple ways of dealing with this problem such as normalization and initialization, the Long short-term memory (LSTM) layer is another way to deal with this. Instead of increasing the sequence length (and thus increasing the depth of the model), it aims to have a better technique for remembering the past."
  },
  {
    "objectID": "posts/16_lstm/index.html#architecture-and-intuition",
    "href": "posts/16_lstm/index.html#architecture-and-intuition",
    "title": "Long short-term memory",
    "section": "Architecture and Intuition",
    "text": "Architecture and Intuition\nTo discuss the LSTM architecture, let‚Äôs start by looking back at RNN‚Äôs. In the previous post, we ended up with the following diagram displaying an (unrolled, single layer) RNN with a sequence length of 3:\n\n\n\nUnrolled 1 layer RNN processes a single batch with a sequence length of 3\n\n\nIf we zoom in on the processing of one token (timestep) we can generalize this to the image below. In the middle, we have the RNN Cell which in terms of the image above, is the combination of the addition and the blue arrow (the hidden to hidden linear layer). On the left we see the previous hidden state entering the RNN cell, and on the bottom we have the embedding activations of the input entering the cell (this is what comes out of the red arrow in the image above. The computation of the new hidden state (on the right), is simply the addition of both activations and a linear layer followed by a Relu activation. This hidden state is passed to the next token as well as to the output layer (the yellow arrow in the previous diagrams).\n\n\n\nDetails of the RNN cell\n\n\nWith this representation, it becomes easy to understand the LSTM. Since the only difference is the content of this RNN cell!\nOn the bottom left we see the (embedding activations of the) inputs \\(x\\) and the hidden state \\(h\\), similar to what we have seen in RNN‚Äôs.\nThe first thing that‚Äôs different from an RNN, is that the LSTM architecture uses a second (cell) state \\(c\\) (top left) besides the hidden state \\(h\\). Whereas in the RNN the hidden state is responsible for both keeping a memory of everything that has happened in the past, as well as having information for predicting the next token. These tasks are split in the LSTM, the cell state is responsible for keeping a memory and the hidden state is concerned with the next token.\nFurthermore, the cell state update is not going through a neural network layer (orange circles), instead the LSTM can update this state by two element-wise operations (blue circles). This allows it to keep information (remember) for a long time.\nThe cell state is first multiplied by the output of the forget gate. These outputs first go through a sigmoid non-linearity and are thus between 0 and 1. Multiplications with 1‚Äôs mean that (parts of the) cell state is kept whereas values of 0 mean that (parts of the) cell state is removed (forgotton). The inputs to the forget gate are the concatenation of the previous hidden state and the (embedding activations) of the current input. However, there exist other LSTM‚Äôs where the previous cell state is concatenated with the hidden state and inputs (for example in the Peephole LSTM, in which all the gates get an input from the cell state).\n\n\n\nDetails of the LSTM cell\n\n\nIn the two gates that follow (input gate and cell gate), information is prepared to be added to the cell state. The input gate is outputting activations between 0 and 1 and is responsible for determining which cell state to update, and the cell gate is responsible for creating new candidate values. These two are multiplied together so that the outputs of the input gate filters the candidate values created by the cell gate.\nFinally, the new hidden state gets computed and is a filtered version of the (updated) cell state. The cell state is first put through a tanh activation (values between -1 and 1) and then multiplied by sigmoid layer (values between 0 and 1). This sigmoid layer is essentially deciding which values of the cell state it wants to use for predicting the next token. The new hidden state is both outputted to the right (for the processing of the next token / timestep) as well as to the top (to be passed to the output layer similarly as in an RNN).\nNow let‚Äôs discuss this process and the concepts in terms of a continuous text generation task. The cell state represents all the information about what has been generated previously: the subjects that the text is talking about, what is going on with the subjects, the style the text is written in (present tense, past tense etc). The multiplication of this cell state with the outputs of the forget gate, allow elements of the cell state to be forgotten. For example, if the current token (word) indicates that we are switching from present tense to past tense, the forget gate‚Äôs output should let the cell state know to drop it‚Äôs representation of the currently active (present) tense. The next two gates are responsible for adding new information into the cell state: e.g.¬†preparing a representation of the past tense. Finally, the output gate should prepare a filter that the next word (let‚Äôs say a verb) should be created in the current style (which has now switched to past tense).\nAlthough I have read about this intuition in multiple places, I am not sure whether it‚Äôs exactly correct. Nor whether it has been proven that an LSTM is doing things exactly in this way. My (current) belief is that researchers often not exactly know whether a certain intuition is correct or not. But that we simply try something, in which we can imagine that a neural network could learn something in a certain way. Whether the network is actually doing that thing, is not always clear. But I guess as long as it‚Äôs beneficial to performance, that‚Äôs the first thing to be happy about.\nNow that we understand the LSTM, let‚Äôs code it up!"
  },
  {
    "objectID": "posts/16_lstm/index.html#data",
    "href": "posts/16_lstm/index.html#data",
    "title": "Long short-term memory",
    "section": "Data",
    "text": "Data\nEverything starts with training data, for a description see an earlier post\n\n\nCode\nimport random\nfrom functools import reduce, partial\nfrom pathlib import Path\nfrom urllib.request import urlretrieve\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport torcheval.metrics as tem\nimport fastcore.all as fc\n\nfrom nntrain.dataloaders import DataLoaders\nfrom nntrain.learner import *\nfrom nntrain.activations import *\nfrom nntrain.acceleration import *\nfrom nntrain.rnn import *\n########### Load the data ###########\npath = Path('./data')\npath.mkdir(parents=True, exist_ok=True)\npath = path / 'names.txt'\nurl = 'https://raw.githubusercontent.com/karpathy/makemore/master/names.txt'\n\n_ = urlretrieve(url, path)\n\nwith open(path, 'r') as f:\n    lines = f.read().splitlines()\n    \nrandom.seed(42)\nrandom.shuffle(lines)\n\ntrain_size=0.8\nval_size=0.1\n\ntrain_lines = lines[0:int(train_size * len(lines))]\nval_lines = lines[int(train_size * len(lines)): int((train_size + val_size) * len(lines))]\n\n### Create vocabulary and mappings ###\nunique_chars = list(set(\"\".join(lines)))\nunique_chars.sort()\nvocabulary = ['.'] + unique_chars\n\nc2i = {c:i for i, c in enumerate(vocabulary)}\ni2c = {i:c for i, c in enumerate(vocabulary)}\n\n\nLast post the SequentialDataset and the VerticalSampler were introduced to load data in a way that fits RNN‚Äôs (and thus LSTM‚Äôs):\n\ndef get_dls(context_length, batch_size):\n    train_ds = SequentialDataset(train_lines, c2i, context_length)\n    valid_ds = SequentialDataset(val_lines, c2i, context_length)\n        \n    train_dl = torch.utils.data.DataLoader(train_ds, shuffle=False, sampler=VerticalSampler(train_ds, batch_size), batch_size=batch_size)\n    valid_dl = torch.utils.data.DataLoader(valid_ds, shuffle=False, sampler=VerticalSampler(valid_ds, batch_size), batch_size=batch_size)\n\n    return DataLoaders(train_dl, valid_dl)"
  },
  {
    "objectID": "posts/16_lstm/index.html#manual-lstm",
    "href": "posts/16_lstm/index.html#manual-lstm",
    "title": "Long short-term memory",
    "section": "Manual LSTM",
    "text": "Manual LSTM\nThe creation of an LSTM cell is relatively straight-forward, by making use of the diagram we can simply put the arithmetic in a class:\n\nclass LSTMCell(nn.Module):\n    def __init__(self, ni, nh):\n        super().__init__()        \n        self.forget_gate = nn.Linear(ni + nh, nh)\n        self.input_gate  = nn.Linear(ni + nh, nh)\n        self.cell_gate   = nn.Linear(ni + nh, nh)\n        self.output_gate = nn.Linear(ni + nh, nh)\n        \n    def forward(self, inp, h, c):\n        # inp  [bs, ni]\n        # h [bs, nh]\n        # c [bs, nh]\n        \n        h = torch.cat([h, inp], dim=1)              # [bs, ni+nh]\n        \n        forget = torch.sigmoid(self.forget_gate(h)) # [bs, nh]\n        c = forget * c                              # [bs, nh]   \n        \n        inp = torch.sigmoid(self.input_gate(h))     # [bs, nh]\n        cell = torch.tanh(self.cell_gate(h))        # [bs, nh]\n        \n        inp = inp * cell                            # [bs, nh]\n        c = c + inp                                 # [bs, nh]\n        \n        h = torch.sigmoid(self.output_gate(h))      # [bs, nh]\n        h = h * torch.tanh(c)                       # [bs, nh]\n        \n        return h, c\n\nNow let‚Äôs create an LSTMNet that uses this LSTMCell. This is a very similar network as the RNN we created in the previous post, we just have to iterate manually through the sequence length and sequentially call the inputs on the cell passing in the hidden and cell state:\n\nclass LSTMNet(nn.Module):\n    \n    def __init__(self, c2i, embedding_dim, hidden_size, bs):\n        super().__init__()\n        self.c2i            = c2i\n        self.bs             = bs\n        self.embedding_dim  = embedding_dim\n        self.hidden_size    = hidden_size\n        # register as buffer so that its moved to the device by the DeviceS Subscriber\n        self.register_buffer('h', torch.zeros((bs, self.hidden_size)))\n        self.register_buffer('c', torch.zeros((bs, self.hidden_size)))\n        \n        self.input2hidden   = nn.Embedding(len(c2i), embedding_dim)\n        self.lstm           = LSTMCell(embedding_dim, hidden_size)\n        self.hidden2out     = nn.Linear(hidden_size, len(c2i))\n        \n    def forward(self, x):\n        inputs = self.input2hidden(x)\n        outputs = []\n        for i in range(inputs.shape[1]):\n            self.h, self.c = self.lstm(inputs[:,i,:], self.h, self.c)\n            outputs += [self.h]\n        self.h = self.h.detach()\n        self.c = self.c.detach()\n        return self.hidden2out(torch.stack(outputs, dim=1))\n    \n    def reset_hidden_state(self):\n        device = self.h.get_device()\n        self.h = torch.zeros_like(self.h).to(device)\n        self.c = torch.zeros_like(self.c).to(device)\n\nAnd let‚Äôs see how it performs:\n\n# Train parameters\nepochs = 5\nlr     = 1e-2\nbs     = 300\n\n# Hyperparameters\ncontext_length = 5\nn_h            = 1000\nembedding_dim  = 100\n\ndls = get_dls(context_length, bs)\nlstm = LSTMNet(c2i, embedding_dim, n_h, bs)\n\nsubs = [ProgressS(True),\n        MetricsS(),\n        DeviceS(device),\n        HiddenStateResetterS()]\n\nscheduler = partial(torch.optim.lr_scheduler.OneCycleLR, max_lr=lr, epochs=epochs, steps_per_epoch=len(dls.train))\n\nl = Learner(lstm, dls, multi_output_cross_entropy, torch.optim.Adam, None, subs=subs + [SchedulerS(scheduler)])\nl.fit(epochs, lr=lr)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\n\n\n\n\n0\ntrain\n2.413\n\n\n0\neval\n2.166\n\n\n1\ntrain\n2.089\n\n\n1\neval\n2.066\n\n\n2\ntrain\n1.974\n\n\n2\neval\n2.003\n\n\n3\ntrain\n1.846\n\n\n3\neval\n1.963\n\n\n4\ntrain\n1.711\n\n\n4\neval\n1.950\n\n\n\n\n\n\n\n\nWith the 2 layer RNN from the previous post, we reached an evaluation loss of 1.976, this is thus a nice improvement!"
  },
  {
    "objectID": "posts/16_lstm/index.html#pytorch-lstm",
    "href": "posts/16_lstm/index.html#pytorch-lstm",
    "title": "Long short-term memory",
    "section": "PyTorch LSTM",
    "text": "PyTorch LSTM\nWhen we use the default PyTorch LSTM, the network is practically identical to the RNN we created in the previous post:\n\nclass PyTorchLSTM(nn.Module):\n    def __init__(self, c2i, embedding_dim, hidden_size, bs, layers=1):\n        super().__init__()\n        self.c2i = c2i\n        self.bs = bs\n        self.embedding_dim = embedding_dim\n        self.hidden_size = hidden_size\n        self.register_buffer('h', torch.zeros((layers, bs, self.hidden_size)))\n        self.register_buffer('c', torch.zeros((layers, bs, self.hidden_size)))\n        \n        self.input2hidden   = nn.Embedding(len(c2i), embedding_dim)               \n        self.lstm = nn.LSTM(embedding_dim, hidden_size, layers, batch_first=True)\n        self.hidden2out     = nn.Linear(hidden_size, len(c2i))\n        \n    def forward(self, x):\n        inputs = self.input2hidden(x)\n        hs, state = self.lstm(inputs, (self.h, self.c))\n        self.h = state[0].detach()\n        self.c = state[1].detach()\n        return self.hidden2out(hs)\n    \n    def reset_hidden_state(self):\n        device = self.h.get_device()\n        self.h = torch.zeros_like(self.h).to(device)\n        self.c = torch.zeros_like(self.c).to(device)\n    \n\n\n# Train parameters\nepochs = 5\nlr     = 1e-2\nbs     = 300\n\n# Hyperparameters\ncontext_length = 5\nn_h            = 1000\nembedding_dim  = 100\nlayers         = 1\n\ndls = get_dls(context_length, bs)\nlstm = PyTorchLSTM(c2i, embedding_dim, n_h, bs, layers=layers)\n\nsubs = [ProgressS(True),\n        MetricsS(),\n        DeviceS(device),\n        HiddenStateResetterS()]\n\nscheduler = partial(torch.optim.lr_scheduler.OneCycleLR, max_lr=lr, epochs=epochs, steps_per_epoch=len(dls.train))\n\nl = Learner(lstm, dls, multi_output_cross_entropy, torch.optim.Adam, None, subs=subs + [SchedulerS(scheduler)])\nl.fit(epochs, lr=lr)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\n\n\n\n\n0\ntrain\n2.419\n\n\n0\neval\n2.163\n\n\n1\ntrain\n2.087\n\n\n1\neval\n2.064\n\n\n2\ntrain\n1.972\n\n\n2\neval\n2.004\n\n\n3\ntrain\n1.846\n\n\n3\neval\n1.966\n\n\n4\ntrain\n1.713\n\n\n4\neval\n1.952\n\n\n\n\n\n\n\n\nThe result is similar to the manual LSTM."
  },
  {
    "objectID": "posts/16_lstm/index.html#sampling-names",
    "href": "posts/16_lstm/index.html#sampling-names",
    "title": "Long short-term memory",
    "section": "Sampling names",
    "text": "Sampling names\nLast but not least, let‚Äôs sample some names with our LSTM:\n\n@fc.patch\ndef generate(self:PyTorchLSTM, n=10, generator=None):\n    # For unbatched input we need a 2D hidden state tensor of size [1, hidden_size]\n    self.h = torch.zeros((self.h.shape[0], self.hidden_size)).cuda()\n    self.c = torch.zeros((self.h.shape[0], self.hidden_size)).cuda()\n    \n    names = []\n    for i in range(n):\n        name = '.'\n        while True:\n            idx = torch.tensor([c2i[name[-1]]]).cuda()\n            logits = self.forward(idx)\n            s = torch.multinomial(F.softmax(logits, dim=1), 1, generator=generator)\n            c = i2c[s.item()]\n            name += c\n            if c == '.':\n                names.append(name)\n                break\n    return names\n\n\nlstm.generate()\n\n['.misdell.',\n '.treniya.',\n '.malu.',\n '.jamem.',\n '.diln.',\n '.savina.',\n '.nayelly.',\n '.genise.',\n '.zia.',\n '.wrenson.']"
  },
  {
    "objectID": "posts/11_nntrain_activations/index.html",
    "href": "posts/11_nntrain_activations/index.html",
    "title": "nntrain (3/n): Activations, Initialization and Normalization",
    "section": "",
    "text": "In this series, I want to discuss the creation of a small PyTorch based library for training neural networks: nntrain. It‚Äôs based off the excellent part 2 of Practical Deep Learning for Coders by Jeremy Howard, in which from lessons 13 to 18 (roughly) the development of the miniai library is discussed.\nWe‚Äôll try to build everything as much as possible from scratch to understand how things work. Once the main functionality of components is implemented and verified, we can switch over to PyTorch‚Äôs version. This is similar to how things are done in the course. However, this is not just a ‚Äúcopy / paste‚Äù of the course: on many occasions I take a different route, and most of the code is my own. That is not to say that all of this is meant to be extremely innovative, instead I had the following goals:\nnb_dev is another great project from the fastai community, which allows python libraries to be written in jupyter notebooks. This may sound a bit weird and controversial, but it has the advantage that we can create the source code for our library in the very same environment in which we want to experiment and interact with our methods, objects and structure while we are building the library. For more details on why this is a good idea and other nice features of nb_dev, see here.\nSo without further ado, let‚Äôs start with where we left off in the previous post:"
  },
  {
    "objectID": "posts/11_nntrain_activations/index.html#end-of-last-post",
    "href": "posts/11_nntrain_activations/index.html#end-of-last-post",
    "title": "nntrain (3/n): Activations, Initialization and Normalization",
    "section": "End of last post:",
    "text": "End of last post:\nIn the last two posts we build up the learner and dataloader module. In this post, we are going to use everything we have build so far to build and train a new model. From the naive MLP model we have been using so far, we will switch to a convolutional neural network (CNN). We will investigate performance and go into the fine print of making sure the networks trains well. We will do this by looking at the activations throughout the network.\n\nfrom datasets import load_dataset,load_dataset_builder\n\nfrom nntrain.dataloaders import DataLoaders, hf_ds_collate_fn\nfrom nntrain.learner import *\n\n\n #| export\nimport torchvision.transforms.functional as TF\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom operator import attrgetter\nfrom functools import partial\nimport fastcore.all as fc\nimport math\nimport torcheval.metrics as tem\nimport matplotlib.pyplot as plt\nimport random\nimport numpy as np\n\n\n #| export\ndef set_seed(seed, deterministic=False):\n    torch.use_deterministic_algorithms(deterministic)\n    torch.manual_seed(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n\nSince a CNN operates on a pixel grid instead of a flat array of pixel values, we have to load the data differently. For this we can use the Dataloaders module we have created in an earlier post. By using the hf_ds_collate_fn with flatten=False, we keep the pixel grid and end up with data have a shape of [batch, channels, height, width]:\n\nname = \"fashion_mnist\"\nds_builder = load_dataset_builder(name)\nhf_dd = load_dataset(name)\n\nbs = 1024\n\ncollate = partial(hf_ds_collate_fn, flatten=False)\n\ndls = DataLoaders.from_hf_dd(hf_dd, batch_size=bs, collate_fn=collate)\n\nxb, yb = next(iter(dls.train))\nxb.shape\n\n\n\n\n\n\n\nDownloading and preparing dataset fashion_mnist/fashion_mnist (download: 29.45 MiB, generated: 34.84 MiB, post-processed: Unknown size, total: 64.29 MiB) to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1...\nDataset fashion_mnist downloaded and prepared to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1. Subsequent calls will reuse this data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.Size([1024, 1, 28, 28])"
  },
  {
    "objectID": "posts/11_nntrain_activations/index.html#convolutional-neural-network",
    "href": "posts/11_nntrain_activations/index.html#convolutional-neural-network",
    "title": "nntrain (3/n): Activations, Initialization and Normalization",
    "section": "Convolutional Neural Network",
    "text": "Convolutional Neural Network\nNext, let‚Äôs create a simple CNN consisting of a couple of convolutions with ReLU activations in between:\n\ndef conv_block(in_c, out_c, kernel_size=3, act=True):\n    padding = kernel_size // 2    # don't lose pixels of the edges\n    stride = 2                    # reduce the image size by factor of 2 in x and y directions\n    conv = torch.nn.Conv2d(in_c, out_c, kernel_size, stride, padding)\n    if act: return nn.Sequential(conv, torch.nn.ReLU())\n    else: return conv\n\nThe model will receive images of shape 28x28 and since we use a stride of 2 in each convolution, the pixel grid will be reduced by a factor of 2 in both the x and y direction (a factor of 4 in total). At the same time we increase the number of filters by a factor of 2 so that the overall data-reduction of a single convolution is roughly a factor of 2.\nIn the very first convolution we go from 1 input channel to 8 output channels. To do that, we increase the kernel size from 3 to 5. For kernel size 3, we would have 3x3 pixels (x 1 input channel) that would map to single position in the output grid. The output grid has 8 channels, so we would go from 9 (3x3x1) to 8 (1x1x8) values. Going from 9 to 8 values is practically no reduction, which doesn‚Äôt allow the convolution to learn anything. So instead we take a kernel of size 5 so that we go from 25 (5x5x1) to 9 values and have roughly a 2x reduction.\n\ndef cnn_layers():\n    return nn.Sequential(                  # input image grid=28x28\n        conv_block(1 , 8, kernel_size=5),  # pixel grid: 14x14\n        conv_block(8 ,16),                 #             7x7\n        conv_block(16,32),                 #             4x4\n        conv_block(32,64),                 #             2x2\n        conv_block(64,10, act=False),      #             1x1\n        nn.Flatten())\ncnn_layers()\n\nSequential(\n  (0): Sequential(\n    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n    (1): ReLU()\n  )\n  (1): Sequential(\n    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n  )\n  (2): Sequential(\n    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n  )\n  (3): Sequential(\n    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n  )\n  (4): Conv2d(64, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (5): Flatten(start_dim=1, end_dim=-1)\n)\n\n\nLet‚Äôs see how this trains:\n\nset_seed(1)\n\nsubs = [ProgressS(True),\n        MetricsS(accuracy=tem.MulticlassAccuracy()),\n        DeviceS(device)]\n\nl = MomentumLearner(cnn_layers(), dls, F.cross_entropy, torch.optim.SGD, 0.1, subs)\nl.fit(5)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\naccuracy\n\n\n\n\n0\ntrain\n2.278\n0.154\n\n\n0\neval\n2.302\n0.100\n\n\n1\ntrain\n2.212\n0.192\n\n\n1\neval\n2.002\n0.288\n\n\n2\ntrain\n0.958\n0.653\n\n\n2\neval\n0.701\n0.740\n\n\n3\ntrain\n0.593\n0.783\n\n\n3\neval\n0.574\n0.797\n\n\n4\ntrain\n0.516\n0.816\n\n\n4\neval\n0.508\n0.823\n\n\n\n\n\n\n\n\nAlthough the accuracy is better then what we had with the MLP model, this doesn‚Äôt look good. The loss is going down and then spikes up to a large value. This happens twice before finally the loss is going down in a more stable manner. To understand what‚Äôs going on, we have to understand what‚Äôs happening to the activations throughout the network while we are training."
  },
  {
    "objectID": "posts/11_nntrain_activations/index.html#iterative-matrix-multiplications",
    "href": "posts/11_nntrain_activations/index.html#iterative-matrix-multiplications",
    "title": "nntrain (3/n): Activations, Initialization and Normalization",
    "section": "Iterative matrix multiplications",
    "text": "Iterative matrix multiplications\nBut before we do, it‚Äôs important to realize that in the forward pass of neural networks we iteratively multiply the inputs to the model many times with (different) weight matrices. Let‚Äôs see how that works by using some artificial data, we generate 1000 random samples of data each with 10 features taken from a unit gaussian (mean=0 and standard deviation=1)\n\nx = torch.randn(10000, 10) \nprint(f'{x.mean()=:.3f}, {x.std()=:.3f}')\n\nx.mean()=-0.003, x.std()=1.000\n\n\nNow let‚Äôs iteratively multiply these inputs with a unit gausian weight matrix mapping from 10 input features to 10 output features and record the mean and standard deviation (std) after each iteration. Theoretically the mean of the outputs should remain 0, since \\(\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y] = 0 * 0 = 0\\) if \\(X\\) and \\(Y\\) are independant. But what happens to the std?\n\nfor i in range(10):\n    print(f'\"x\" multiplied {i:02d} times: mean={x.mean().item():10.3f},   std={x.std().item():15.3f}')\n    w = torch.randn(10, 10)\n    x = x@w\n\n\"x\" multiplied 00 times: mean=    -0.003,   std=          1.000\n\"x\" multiplied 01 times: mean=     0.013,   std=          3.317\n\"x\" multiplied 02 times: mean=    -0.013,   std=          9.330\n\"x\" multiplied 03 times: mean=    -0.009,   std=         32.389\n\"x\" multiplied 04 times: mean=     0.484,   std=        103.563\n\"x\" multiplied 05 times: mean=     2.104,   std=        299.863\n\"x\" multiplied 06 times: mean=     4.431,   std=        948.056\n\"x\" multiplied 07 times: mean=    -4.135,   std=       3229.121\n\"x\" multiplied 08 times: mean=   -74.124,   std=      11527.234\n\"x\" multiplied 09 times: mean=   -38.169,   std=      35325.461\n\n\nWe observe some pretty unstable activations:\n\nthe standard deviation grows exponentially\ninitially the mean remains around 0, but eventually it starts to deviate. Probably because the standard deviation is getting larger and larger\n\nThis is a big problem for the training of neural networks as input data is passing through the network. When activations are ever increasing so are the gradients which causes the updates to the weights to explode.\nLet‚Äôs try the same with a weight matrix that has a smaller standard deviation:\n\nx = torch.randn(10000, 10) \n\nfor i in range(10):    \n    print(f'\"x\" multiplied {i:02d} times: mean={x.mean().item():10.3f},   std={x.std().item():15.3f}')\n    w = torch.randn(10, 10) * 0.1 # reduce the standard deviation to 0.1\n    x = x@w\n\n\"x\" multiplied 00 times: mean=     0.004,   std=          1.001\n\"x\" multiplied 01 times: mean=    -0.000,   std=          0.279\n\"x\" multiplied 02 times: mean=    -0.000,   std=          0.072\n\"x\" multiplied 03 times: mean=    -0.000,   std=          0.027\n\"x\" multiplied 04 times: mean=    -0.000,   std=          0.011\n\"x\" multiplied 05 times: mean=     0.000,   std=          0.003\n\"x\" multiplied 06 times: mean=    -0.000,   std=          0.001\n\"x\" multiplied 07 times: mean=     0.000,   std=          0.000\n\"x\" multiplied 08 times: mean=    -0.000,   std=          0.000\n\"x\" multiplied 09 times: mean=     0.000,   std=          0.000\n\n\nThis is not any better, all activations are about the same, and they are all zero! If this happens in a neural network, the network is not learning at all, since the activations and gradients will all be zero.\n\n\n\n\n\n\nNote\n\n\n\nYou might wonder whether this analysis still holds if we are having a network which consists of convolutional layers. Since the matrix multiplications just shown ofcourse resemble the linear layer style straight up matrix multiplications and not any convolutional arithmetic. And indeed this analysis still holds, because in fact convolutional arithmetic can be rewritten in a form in which we:\n\nflatten out the CHW dimensions into a flat array\nmultiply with a weight matrix constructed out of the weights in the kernels, configured in a special way\n\nAnd thus resembles a special form of linear layer matrix multiplications. See for example here"
  },
  {
    "objectID": "posts/11_nntrain_activations/index.html#activations",
    "href": "posts/11_nntrain_activations/index.html#activations",
    "title": "nntrain (3/n): Activations, Initialization and Normalization",
    "section": "Activations",
    "text": "Activations\nFrom the above it follows that it‚Äôs important to keep track of the activations as data flows through the network. Let‚Äôs try to build this into our framework.\nTo track the activations we can make use of PyTorch hooks: a function that you can attach to any nn.Module and which will be called after the module is being called either during the forward (register_forward_hook()) or backward pass (register_backward_hook()). Let‚Äôs create a small Hook class that wraps this logic and which can remove the hook after we are done with it. We will also store the tracked metrics as attributes on this class.\n\n #| export\nclass Hook():\n    def __init__(self, nr, layer, func):\n        wrapped_func = partial(func, self) # pass the Hook object into the function\n        self.hook = layer.register_forward_hook(wrapped_func)\n        self.layer_name = f'{nr}_{layer.__class__.__name__}'\n        \n    def remove(self):\n        self.hook.remove()\n\nAnd let‚Äôs create a Subscriber, that creates and removes the hooks and keeps track of the statistics:\n\nWe will keep track of mean and std as the main metrics we have also been looking at above\nkeep track of the histogram counts for additional visibility into the activations\nkeep track of the average firing rate per activation over the batch, but more about that later\n\n\n #| export\nclass ActivationStatsS(Subscriber):\n    \n    def __init__(self, modules):\n        self.modules = modules\n    \n    def before_fit(self, learn):\n        self.hooks = [Hook(i, module, partial(self.record_stats, learn)) for i, module in enumerate(self.modules)]\n        \n    def record_stats(self, learn, hook, layer, inp, outp):\n        if learn.model.training:\n            if not hasattr(hook, 'stats'): hook.stats = ([], [], [], [])\n            acts = outp.detach().cpu()\n            hook.stats[0].append(acts.mean())              # get the means over all activations\n            hook.stats[1].append(acts.std())               # get the stds over all activations\n            hook.stats[2].append(acts.histc(20,-10,10))    # get the histogram counts with 20 bins (-10,10)\n            \n            # computation of the not_firing_rate_per_activation\n            N = acts.shape[0]                 \n            flat = acts.view(N, -1)                        # flatten the activations: matrix of [samples, activations]\n            nf_rate_p_act = (flat == 0.0).sum(dim=0) / N   # compute not firing rate per activations (so across the samples)\n            hook.stats[3].append(nf_rate_p_act)   \n\n    def after_fit(self, learn):\n        for h in self.hooks: h.remove()\n\n\n\nCode\n# This code is folded by default to not clutter the blog\n@fc.patch()\ndef plot(self:ActivationStatsS, figsize=(15,4), average_firing_rate=False):\n    plots = 3 if average_firing_rate else 2\n    fig,axs = plt.subplots(1,plots, figsize=figsize)\n    legend = []\n    for h in self.hooks:\n        axs[0].plot(h.stats[0])\n        axs[0].set_title('mean')\n        axs[1].plot(h.stats[1])\n        axs[1].set_title('std')\n        if average_firing_rate:\n            axs[2].plot(1-torch.stack(h.stats[3]).T.mean(dim=0))\n            axs[2].set_title('average firing rate')\n            axs[2].set_ylim(0,1)\n        legend.append(h.layer_name)\n    plt.legend(legend);\n\n@fc.patch()\ndef plot_hist(self:ActivationStatsS, figsize=None, log=True):\n    if figsize is None: figsize = (15, len(self.hooks))\n    fig,axs = plt.subplots(math.ceil(len(self.hooks)/2), 2, figsize=figsize)\n    axs = axs.flat\n    for i, hook in enumerate(self.hooks):\n        d = torch.stack(hook.stats[2]).T\n        if log: d = d.log1p()\n        axs[i].imshow(d, cmap='Blues', origin='lower', aspect='auto')\n        axs[i].set_title(hook.layer_name)\n        axs[i].set_yticks(np.arange(0, 20, 2), np.arange(-10, 10, 2))\n\n@fc.patch()\ndef plot_dead(self:ActivationStatsS, binary=False, figsize=None):\n    if figsize is None: figsize = (15, len(self.hooks))\n    fig,axs = plt.subplots(math.ceil(len(self.hooks)/2), 2, figsize=figsize)\n    axs = axs.flat\n    for i, hook in enumerate(self.hooks):\n        d = torch.stack(hook.stats[3]).T\n        if binary: d = d == 1.0\n        axs[i].imshow(d, cmap='Greys', origin='lower', aspect='auto')\n        axs[i].set_title(hook.layer_name)\n\n\n\nset_seed(1)\n\nmodel = cnn_layers()\n\n# show activation stats on all ReLU layers\nmodules = [module for module in model.modules() if isinstance(module, nn.ReLU)]\nact_stats = ActivationStatsS(modules)\n\nsubs = [act_stats,\n        DeviceS(device)]\n\nl = MomentumLearner(model, dls, F.cross_entropy, torch.optim.SGD, 0.1, subs)\nl.fit(5)\n\nLet‚Äôs have a look at the mean and std‚Äôs of the layers as we progressively train the model. On the horizontal axis the number of batches are displayed and the coloured lines depict the mean and std respectively of the activations in the layers we are tracking (all the ReLU layers in our model):\n\nact_stats.plot()\n\n\n\n\nThese plots show a similar problem to what we saw in the loss plots: two large spikes during training. In the beginning the means are nice and small (around 0), but the std‚Äôs are way too small (also around 0). There is thus very little variation in our activations. The std then increases exponentially (exactly as we have seen in the iterative matrix multiplication example above!) and crashes back to zero. This patterns repeats once again, and then finally the std‚Äôs stabilize around a value somewhat in the range of 1.\nLet‚Äôs also have a look at the histogram plots, these plots show a single histogram vertically. On the horizontal axis we have again number of batches. Vertically we display a histogram as a heatmap (to help with the colorscale, we actually display the log of the histogram counts): high counts in a bin correspond to a dark blue color. The histogram records values on the vertical axis from -10 to 10. Since we are tracking the stats of ReLU layers there are no counts below zero.\n\nact_stats.plot_hist()\n\n\n\n\nFinally, we can also have a look at the percentage of ‚Äúdead activations‚Äù of our ReLU neurons. Remember that a ReLU neuron passes the data directly through if the data is larger than 0, and outputs 0 whenever the data is below 0. Whenever the data is clipped at zero, it‚Äôs gradient will also be zero (since the derivative of a horizontal line is zero). For backpropagation this means that all the upstream gradient components that flow through this neuron will all be zero, which translates to no updates.\nIn principle it‚Äôs not a problem when for some samples the ReLU output is zero, this is actually totally normal and part of what the ReLY should do. Wowever when this happens for all batches in a the (training) epoch, we have a neuron which never activates for any of our data-points, and thus never passes any gradient to upstream components. This is what Andrej Karpathy calls a ‚Äúdead neuron‚Äù, and signals some kind of ‚Äúpermanent brain damage of a neural net‚Äù.\nWe are tracking this by computing the ‚Äúnone-firing-rate‚Äù per ReLU activation over the batch: if none of the samples in a batch have a positive ReLU output we record a value of 1.0, if for 50% of the samples the ReLU output is positive we record a value of 0.5.\nWith the following plots, we display those neurons that don‚Äôt fire a single time in a minibatch (black) vs those neurons that fire at least for one sample in the batch (white). Horizontally the batches, vertically the activations per layer (CxHxW)\n\nact_stats.plot_dead(binary=True)\n\n\n\n\nAs expected, we are seeing a very large number of black clusters. Especially starting from the two spikes we identified above, we see many neurons being totally thrown of and never recover anymore from it. This is a sign of severe training problems."
  },
  {
    "objectID": "posts/11_nntrain_activations/index.html#initialization",
    "href": "posts/11_nntrain_activations/index.html#initialization",
    "title": "nntrain (3/n): Activations, Initialization and Normalization",
    "section": "Initialization",
    "text": "Initialization\nWe have seen the importance of keeping the activations stable throughout training, and we have seen how we can monitor these activations.\nA first effort at stabilizing the activations, is by taking care of the initialization of the network, which is the process of setting the weights before they are trained. Above we showed that if the std of the weights is too large, our activations explode over time and if the std is too small the activations vanish. Can we set the std to a value that is just right, and makes sure the std of the activations stays roughly 1?\nWith Xavier initialization, the weight matrices are initialized in such a way that activations taken from a unit gaussian don‚Äôt explode or vanish as we have seen above. It turns out, that we have to scale the standard deviation by \\(1/\\sqrt{n_{in}}\\)\n\nx = torch.randn(10000, 10) \n\n# reduce the standard deviation by a factor of 1/sqrt(10)\nw3 = torch.randn(10, 10) * (1/math.sqrt(10))\n\nfor i in range(10):\n    x = x@w3\nprint(f'\"x\" multiplied {i:02d} times: mean={x.mean().item():10.3f},   std={x.std().item():15.3f}')\n\n\"x\" multiplied 09 times: mean=    -0.002,   std=          0.882\n\n\nHowever, this doesn‚Äôt cover the full story. During training of a neural network we also have activation functions sitting in between our matrix mulitplies. And activations typically squash the activations coming out of the (linear, convolutional..) layer. See for example what happens to our activations, after multiplying with Xavier initialized weights and adding a ReLU non-linearity:\n\nx = torch.randn(10000, 10) \n\n# reduce the standard deviation by a factor of 1/sqrt(10)\nw4 = torch.randn(10, 10) * (1/math.sqrt(10))\n\nfor i in range(10):\n    x = (x@w3).relu()\nprint(f'\"x\" multiplied {i:02d} times: mean={x.mean().item():10.3f},   std={x.std().item():15.3f}')\n\n\"x\" multiplied 09 times: mean=     0.004,   std=          0.009\n\n\nAnd as expected, everyhing has imploded to zero. It turns out that we can correct for this squashing by adding a gain. This is called Kaiming initialization. For example the gain for ReLU is \\(\\sqrt{2}\\):\n\nx = torch.randn(10000, 10)\n\n# add a gain of sqrt(2)\nw4 = torch.randn(10, 10) * math.sqrt(2/10)\n\nfor i in range(10):\n    x = torch.nn.functional.relu((x@w4))\nprint(f'\"x\" multiplied {i:02d} times: mean={x.mean().item():10.3f},   std={x.std().item():15.3f}')\n\n\"x\" multiplied 09 times: mean=     0.332,   std=          0.688\n\n\nSo let‚Äôs apply Kaiming initialization to our model, and see how it performs:\n\n #| export\ndef init_weights(m):\n    if isinstance(m, nn.Conv2d): torch.nn.init.kaiming_normal_(m.weight)\n\n\nset_seed(1)\n\nmodel = cnn_layers().apply(init_weights)\n\nmodules = [module for module in model.modules() if isinstance(module, nn.ReLU)]\nact_stats = ActivationStatsS(modules)\n\nsubs = [act_stats,\n        ProgressS(True),\n        MetricsS(accuracy=tem.MulticlassAccuracy()),\n        DeviceS(device)]\n\nl = MomentumLearner(model, dls, F.cross_entropy, torch.optim.SGD, 0.1, subs)\nl.fit(5)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\naccuracy\n\n\n\n\n0\ntrain\n1.400\n0.563\n\n\n0\neval\n0.622\n0.768\n\n\n1\ntrain\n0.507\n0.815\n\n\n1\neval\n0.457\n0.834\n\n\n2\ntrain\n0.413\n0.851\n\n\n2\neval\n0.426\n0.845\n\n\n3\ntrain\n0.376\n0.864\n\n\n3\neval\n0.389\n0.861\n\n\n4\ntrain\n0.353\n0.872\n\n\n4\neval\n0.379\n0.863\n\n\n\n\n\n\n\n\nPropper initialization increases the performance from 82% to around 87%, also the loss graph looks a bit better. Let‚Äôs have a look at our activation plots:\n\nact_stats.plot()\n\n\n\n\n\nact_stats.plot_hist()\n\n\n\n\n\nact_stats.plot_dead(binary=True)\n\n\n\n\nA lot better indeed, but we still see spikes and ‚Äúbrain damage‚Äù occuring after the spikes."
  },
  {
    "objectID": "posts/11_nntrain_activations/index.html#normalisation",
    "href": "posts/11_nntrain_activations/index.html#normalisation",
    "title": "nntrain (3/n): Activations, Initialization and Normalization",
    "section": "Normalisation",
    "text": "Normalisation\nBesides initializing the weight matrices properly, we can also normalize the data itself. Since the batch size is quite large (1024) let‚Äôs do so by taking the statistics of the first batch:\n\nxb_mean = xb.mean()\nxb_std = xb.std()\n\nCreate a small Subscriber that normalizes our inputs before a batch:\n\n #| export\nclass NormalizationS(Subscriber):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n        \n    def before_batch(self, learn):\n        learn.batch = [(learn.batch[0] - self.mean) / self.std, learn.batch[1]]\n\n\nset_seed(1)\n\nmodel = cnn_layers().apply(init_weights)\n\nmodules = [module for module in model.modules() if isinstance(module, nn.ReLU)]\nact_stats = ActivationStatsS(modules)\n\nnorm = NormalizationS(xb_mean, xb_std)\n\nsubs = [norm, \n        act_stats,\n        ProgressS(True),\n        MetricsS(accuracy=tem.MulticlassAccuracy()),\n        DeviceS(device)]\n\nl = MomentumLearner(model, dls, F.cross_entropy, torch.optim.SGD, 0.1, subs)\nl.fit(5)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\naccuracy\n\n\n\n\n0\ntrain\n0.905\n0.692\n\n\n0\neval\n0.507\n0.813\n\n\n1\ntrain\n0.429\n0.844\n\n\n1\neval\n0.407\n0.852\n\n\n2\ntrain\n0.377\n0.863\n\n\n2\neval\n0.391\n0.860\n\n\n3\ntrain\n0.341\n0.876\n\n\n3\neval\n0.364\n0.868\n\n\n4\ntrain\n0.325\n0.881\n\n\n4\neval\n0.366\n0.866\n\n\n\n\n\n\n\n\n\nact_stats.plot()\n\n\n\n\n\nact_stats.plot_hist()\n\n\n\n\n\nact_stats.plot_dead(binary=True)\n\n\n\n\nThese are all good improvements. The histogram plots start to look a lot better and the amount of dead neurons is greatly reduced. We still have problems though in the beginning of training."
  },
  {
    "objectID": "posts/11_nntrain_activations/index.html#batch-normalization",
    "href": "posts/11_nntrain_activations/index.html#batch-normalization",
    "title": "nntrain (3/n): Activations, Initialization and Normalization",
    "section": "Batch Normalization",
    "text": "Batch Normalization\nSo far we have tried to tackle the problems around activations from two sides: weight initialization and normalization of the input data. These are effective measures, but don‚Äôt cover the full problem: the weights are updated during training after which they are no longer normalized. Additionally, after we normalize our data we just ‚Äúhave to send it through the network‚Äù and see what happens with the activations.\nThe idea behind batch normalization is remarkably simple: if we know that we need unit gaussian activations throughout the network, let‚Äôs just make them unit gaussianü§ì. This might sound a bit weird, but in fact the normalization operation is perfectly differentiable, and thus the gradients can be backpropagated through a normalization operation. Batch normalization takes the form of a layer and normalizes each batch during training.\nLet‚Äôs start with the basic functionality, a layer that normalizes a batch of data. Note that Batchnorm normalizes the batch across the batch, height and width but not across the channels. So when passing RGB images through your network, the mean and std would have 3 values each (for a Batchnorm layer that would act directly upon the inputs)\n\nclass BatchNorm(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n    def forward(self, xb):\n        mean = xb.mean(dim=(0,2,3), keepdim=True)\n        std = xb.std(dim=(0,2,3), keepdim=True)\n        return (xb - mean) / std\n\n\nxb.mean(dim=(0,2,3)), xb.std(dim=(0,2,3))\n\n(tensor([0.2859]), tensor([0.3539]))\n\n\n\nout = BatchNorm().forward(xb)\nout.mean(dim=(0,2,3)), out.std(dim=(0,2,3))\n\n(tensor([8.2565e-08]), tensor([1.]))\n\n\nAn interesting and somewhat remarkable consequence of this layer, is that the activations of a single sample in the batch are getting coupled to the activations of the other samples in the batch, since the mean and std of the batch are computed across all the samples. This leads to all sort of strange behavior and after batch normalization other normalization layers have been developed which don‚Äôt have this property: such as layer, group or instance normalization. But as it turns out, this coupling also has a regularizing effect. Since the coupling of samples acts somewhat similarly to data augmentation.\nAdditionally, batchnorm defines two parameters mult and add by which the outputs are multiplied by and added to. These parameters are initialized at 1 and 0, meaning that at the very start of training these layers are in fact normalizing the data. However, they are learnable parameters, so during training these values can be changed if the model sees fit. This means that a Batchnorm layer can in fact do something totally different then normalizing the data!\n\nclass BatchNorm(nn.Module):\n    def __init__(self, nf):\n        super().__init__()\n        self.mult = torch.nn.Parameter(torch.ones(nf, 1, 1)) # also computed per channel\n        self.add = torch.nn.Parameter(torch.zeros(nf, 1, 1)) # also computed per channel\n        \n    def forward(self, xb):\n        mean = xb.mean(dim=(0,2,3), keepdim=True)\n        std = xb.std(dim=(0,2,3), keepdim=True)\n        return self.mult * (xb - mean) / std + self.add\n\nOne problem with this, is that during inference we would like to be able to pass in just a single sample. But because of the batchnorm layer which is expecting a full batch of data, it‚Äôs no longer clear how to get sensible predictions out of the model. One way to solve this, is to keep running statistics of the mean and std during training and just use these when performing inference. Let‚Äôs add that as well:\n\nclass BatchNorm(nn.Module):\n    def __init__(self, nf, mom=0.1):\n        super().__init__()\n        self.mom = mom\n        self.mult = torch.nn.Parameter(torch.ones(nf, 1, 1))\n        self.add = torch.nn.Parameter(torch.zeros(nf, 1, 1))\n        self.register_buffer('var',  torch.ones(1,nf,1,1))    # make sure they are stored during export\n        self.register_buffer('mean', torch.zeros(1,nf,1,1))   # make sure they are stored during export\n        \n    def update_stats(self, xb):\n        mean = xb.mean(dim=(0,2,3), keepdim=True)\n        var = xb.var(dim=(0,2,3), keepdim=True)\n        self.mean.lerp_(mean, self.mom)                        # take a weighted average (in place) between self.mean and mean\n        self.var.lerp_(var, self.mom)                          # dito with variance\n        \n    def forward(self, xb):\n        if self.training:\n            with torch.no_grad(): self.update_stats(xb)\n        return self.mult * ((xb - self.mean) / (self.var + 1e-5).sqrt()) + self.add\n\nTo add this to our model we have to redefine some functions:\n\n #| export\ndef conv_block(in_c, out_c, kernel_size=3, act=True, norm=True):\n    padding = kernel_size // 2\n    stride = 2\n    layers = [torch.nn.Conv2d(in_c, out_c, kernel_size, stride, padding, bias=not norm)]\n    if norm: layers.append(torch.nn.BatchNorm2d(out_c))\n    if act: layers.append(torch.nn.ReLU())\n    return nn.Sequential(*layers) if len(layers)&gt;1 else layers[0]\n\n\n #| export\ndef cnn_layers(act=True):\n    return nn.Sequential(                  \n        conv_block(1 , 8, kernel_size=5),\n        conv_block(8 ,16),\n        conv_block(16,32),\n        conv_block(32,64),\n        conv_block(64,10, norm=False, act=False),\n        nn.Flatten())\n\n\nset_seed(1)\n\nmodel = cnn_layers().apply(init_weights)\n\nmodules = [module for module in model.modules() if isinstance(module, nn.ReLU)]\nact_stats = ActivationStatsS(modules)\n\nnorm = NormalizationS(xb_mean, xb_std)\n\nsubs = [norm, \n        act_stats,\n        ProgressS(True),\n        MetricsS(accuracy=tem.MulticlassAccuracy()),\n        DeviceS(device)]\n\nl = MomentumLearner(model, dls, F.cross_entropy, torch.optim.SGD, 0.4, subs)\nl.fit(5)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\naccuracy\n\n\n\n\n0\ntrain\n0.572\n0.795\n\n\n0\neval\n0.413\n0.845\n\n\n1\ntrain\n0.339\n0.877\n\n\n1\neval\n0.352\n0.871\n\n\n2\ntrain\n0.305\n0.888\n\n\n2\neval\n0.345\n0.871\n\n\n3\ntrain\n0.277\n0.898\n\n\n3\neval\n0.340\n0.875\n\n\n4\ntrain\n0.262\n0.903\n\n\n4\neval\n0.300\n0.890\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nI must be missing something in the Batchnorm layer defined above, because it doesn‚Äôt train as well as the Batchnorm2d layer from PyTorch. Let me know if anybody knows what I‚Äôm missing\n\n\n\nact_stats.plot()\n\n\n\n\n\nact_stats.plot_hist()\n\n\n\n\n\nact_stats.plot_dead(binary=True)\n\n\n\n\nAnd this is all looking very good, no more spikes in the loss, histograms are looking good and no or very little permanently dead neurons.\nLet‚Äôs have a final look at the ‚Äúdead plot‚Äù. Without the binary=True it displays the average dead rate across the samples in in the minibatch.\nAdditionally we can plot the average firing rate across all neurons:\n\nact_stats.plot_dead()\n\n\n\n\n\nact_stats.plot(average_firing_rate=True)\n\n\n\n\nFrom which we see that on average all ReLU neurons fire for around 50% of the samples in a minibatch. Some fire a bit less (darker) some fire a bit more (brighter), but we have very little neurons that never fire (black)."
  },
  {
    "objectID": "posts/11_nntrain_activations/index.html#final-remarks",
    "href": "posts/11_nntrain_activations/index.html#final-remarks",
    "title": "nntrain (3/n): Activations, Initialization and Normalization",
    "section": "Final remarks",
    "text": "Final remarks\nWe have again come a long way, and have seen how we can make sure to train a neural network properly. We have learned to look at activations from many angles, and improved our model up to around 89% accuracy by careful initialization and normalization."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Another Machine Learning Blog",
    "section": "",
    "text": "Long short-term memory\n\n\n\n\n\n\n\nnlp\n\n\nrnn\n\n\nlstm\n\n\ngenerative\n\n\n\n\n\n\n\n\n\n\n\nOct 9, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nRecurrent Neural Networks\n\n\n\n\n\n\n\nnlp\n\n\nrnn\n\n\nbptt\n\n\ngenerative\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nMultilayer Perceptron language model\n\n\n\n\n\n\n\nembeddings\n\n\nmlp\n\n\nnlp\n\n\ngenerative\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nN-gram language models\n\n\n\n\n\n\n\nn-gram\n\n\nnlp\n\n\nembeddings\n\n\ngenerative\n\n\n\n\n\n\n\n\n\n\n\nSep 6, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nnntrain (4/n): Accelerated optimization\n\n\n\n\n\n\n\nweight decay\n\n\nmomentum\n\n\nRMSProp\n\n\nAdam\n\n\nResnet\n\n\nlearning rate scheduler\n\n\ndata augmentation\n\n\ncomputer vision\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nnntrain (3/n): Activations, Initialization and Normalization\n\n\n\n\n\n\n\nconvolutions\n\n\nactivations\n\n\ninitialization\n\n\nbatch normalization\n\n\ncomputer vision\n\n\n\n\n\n\n\n\n\n\n\nAug 20, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nnntrain (2/n): Learner\n\n\n\n\n\n\n\ntraining\n\n\nmomentum\n\n\nsubscribers\n\n\nlearning rate finder\n\n\ncomputer vision\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nnntrain (1/n): Datasets and Dataloaders\n\n\n\n\n\n\n\ndataloading\n\n\ntraining\n\n\ncollation\n\n\nsampler\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nnntrain (0/n): Preliminaries\n\n\n\n\n\n\n\nfoundations\n\n\nPyTorch\n\n\nnn.Module\n\n\n\n\n\n\n\n\n\n\n\nAug 9, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Stable Diffusion - Code\n\n\n\n\n\n\n\ngenerative\n\n\nstable diffusion\n\n\ndiffusers\n\n\ncomputer vision\n\n\n\n\n\n\n\n\n\n\n\n\nJul 3, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Stable Diffusion - Concepts\n\n\n\n\n\n\n\ngenerative\n\n\nstable diffusion\n\n\ndiffusers\n\n\ncomputer vision\n\n\n\n\n\n\n\n\n\n\n\n\nMar 18, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nCross entropy any which way\n\n\n\n\n\n\n\nloss functions\n\n\nsoftmax\n\n\nnll\n\n\ncross entropy\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nFast matrix multiplications\n\n\n\n\n\n\n\nfoundations\n\n\nmath\n\n\nvectorization\n\n\nlinear algebra\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nFirst competitionüèÖ\n\n\n\n\n\n\n\ncompetition\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nPaperspace setup\n\n\n\n\n\n\n\nsetup\n\n\nmlops\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nBlog setup\n\n\n\n\n\n\n\nsetup\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/02_paperspace_setup/index.html",
    "href": "posts/02_paperspace_setup/index.html",
    "title": "Paperspace setup",
    "section": "",
    "text": "Most people don‚Äôt have a GPU installed in their working machine that is suited for Deep Learning, and in fact you don‚Äôt need to. It‚Äôs quite easy to setup a remote GPU server nowadays, and in this blog I will explain how to do so with Paperspace Gradient.\nI started using Paperspace because of a recommendation from Jeremy Howard in his Live Coding Videos. If you haven‚Äôt seen these lectures, I can highly recommend them. They are a great resource on many things related to getting started with Deep Learning. Jeremy shows a lot of productivity hacks and practical tips on getting a good setup.\nHowever, the Paperspace setup explanations are a bit out-dated which can lead to confusion when following along with the video‚Äôs. Also, after the recording of the videos Jeremy created some nice scripts which simplify the setup. This blog will hopefully help others to navigate this and quickly set-up a remote GPU server. I would advice anybody who wants to try Paperspace, to first watch the videos from Jeremy to have a general idea of how it works, and then follow these steps to quickly get set-up.\nOnce you have signed up to Paperspace, go to their Gradient service and create a new project. Paperspace has a free tier, as well as a pro- ($8/month) and growth-plan ($39/month). I personally signed up for the pro-plan, which has a very good value for money. You get 15Gb persistent storage and free Mid instance types. If available, I use the A4000, which is the fastest and comes with 16GB of GPU memory.\nWith the pro-plan you can create up to 3 servers, or ‚ÄúNotebooks‚Äù as they are called by Paperspace (throughout this blog I‚Äôll refer to them as Notebook Servers). So let‚Äôs create one:"
  },
  {
    "objectID": "posts/02_paperspace_setup/index.html#first-look-at-our-notebook-server",
    "href": "posts/02_paperspace_setup/index.html#first-look-at-our-notebook-server",
    "title": "Paperspace setup",
    "section": "First look at our Notebook Server",
    "text": "First look at our Notebook Server\nNext, let‚Äôs open a terminal and get familiar with our Server\n\n\nTerminal\n\n&gt; which python\n/usr/local/bin/python\n\n&gt; python --version\nPython 3.9.13\n\nAnd let‚Äôs also check the PATH variable:\n\n\nTerminal\n\n&gt; echo $PATH\n/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin: /usr/sbin:/usr/bin:/sbin:/bin:/root/mambaforge/bin\n\nThe python command is thus pointing to the system Python installation. However, on the PATH variable we are also seeing an entry at the end mentioning mambaforge.\nAnd indeed we can execute:\n\n\nTerminal\n\n&gt; mamba list | grep python\n\nipython                   8.5.0              pyh41d4057_1    conda-forge\nipython_genutils          0.2.0                      py_1    conda-forge\npython                    3.10.6          h582c2e5_0_cpython    conda-forge\npython-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge\npython-fastjsonschema     2.16.2             pyhd8ed1ab_0    conda-forge\npython_abi                3.10                    2_cp310    conda-forge\n\nSo we are having both a mamba based Python 3.10.6 and a system installation of Python 3.9.13.\nLet‚Äôs open a Jupyter Notebook and see which Python version is running:\n\n\nUntitled.ipynb\n\nimport sys\nsys.version\n\nWhich returns: '3.9.13 (main, May 23 2022, 22:01:06) \\n[GCC 9.4.0]'. Jupyter is thus running the system Python installation.\n\n\n\n\n\n\nNote\n\n\n\nIn the videos Jeremy mentions that we should never use the system Python but instead always create a Mamba installation. However, since we are working here on a virtual machine that is only used for running Python, this shouldn‚Äôt be a problem. Just be aware that we are using the system Python which is totally separate from the Mamba setup.\n\n\nSince we are running the system Python version, we can inspect all the packages that are installed:\n\n\nTerminal\n\n&gt; pip list\n\n...\nfastai                            2.7.10\nfastapi                           0.92.0\nfastbook                          0.0.28\nfastcore                          1.5.27\nfastdownload                      0.0.7\nfastjsonschema                    2.15.3\nfastprogress                      1.0.3\n...\ntorch                             1.12.0+cu116\ntorchaudio                        0.12.0+cu116\ntorchvision                       0.13.0+cu116\n..."
  },
  {
    "objectID": "posts/02_paperspace_setup/index.html#persisted-storage-at-paperspace",
    "href": "posts/02_paperspace_setup/index.html#persisted-storage-at-paperspace",
    "title": "Paperspace setup",
    "section": "Persisted Storage at Paperspace",
    "text": "Persisted Storage at Paperspace\nIn general, things are not persisted on Paperspace. So anything we store during a session, will be gone when we restart our Notebook Server. However, Paperspace comes with two special folders that are persisted. It‚Äôs important to understand how these folder works since we obviously need to persist our work. Not only that, but we also need to persist our configuration files from services lik GitHub, Kaggle and HuggingFace and potentially any other config files for tools or services we are using.\nThe persisted folders are called /storage and /notebooks. Anything in our /storage is shared among all the Notebook Servers we are running, whereas anything that is stored in the /notebooks folder is only persisted on that specific Notebook Server."
  },
  {
    "objectID": "posts/02_paperspace_setup/index.html#set-up",
    "href": "posts/02_paperspace_setup/index.html#set-up",
    "title": "Paperspace setup",
    "section": "Set up",
    "text": "Set up\nIn the first few videos, Jeremy shows a lot of tricks on how to install new packages and set up things like Git and GitHub. After the recording of these videos, he made a GitHub repo which facilitates this setup greatly and makes most of the steps from the videos unnecessary. So let‚Äôs use that:\n\n\nTerminal\n\n&gt; git clone https://github.com/fastai/paperspace-setup.git\n&gt; cd paperspace-setup\n&gt; ./setup.sh\n\nTo understand what this does, let‚Äôs have a look at setup.sh:\n\n\nsetup.py\n\n#!/usr/bin/env bash\n\nmkdir /storage/cfg\ncp pre-run.sh /storage/\ncp .bash.local /storage/\necho install complete. please start a new instance\n\nFirst it‚Äôs creating a new directory inside of our /storage folder called cfg. As we will see, this is where we will store all our configuration files and folders.\nNext, the script copies 2 files to our storage folder. Let‚Äôs have a closer look at those\n\npre-run.sh\nDuring startup of a Notebook Server (upon creation or restart), Paperspace automatically executes the script it finds at /storage/pre-run.sh. This is really neat, since we can create a script at this location to automate our setup!\nFor the full script, click here, and let‚Äôs have a closer look at this first snippet:\n\n\npre-run.sh (snippet)\n\nfor p in .local .ssh .config .ipython .fastai .jupyter .conda .kaggle\ndo\n        if [ ! -e /storage/cfg/$p ]; then\n                mkdir /storage/cfg/$p\n        fi\n        rm -rf ~/$p\n        ln -s /storage/cfg/$p ~/\ndone\n\nSo we are iterating through a list of folder names (.local .ssh ...) on line 1, and for each one we create a directory inside of /storage/cfg on line 4. We only do this if the directory doesn‚Äôt already exist on line 3. Next, each of these folders is symlinked to the home directory (~/) on line 7.\nThis means that:\n\nWhen we store something in any of these symlinked folders (e.g.¬†~/.local), it‚Äôs actually being written to the associated storage folder (e.g.¬†/storage/cfg/.local) because of the symlink.\nWhenever we restart our Notebook Server, all the stuff that has previously been persisted (e.g.¬†in /storage/cfg/.local) are made available again in the home directory (e.g.¬†~/.local).\n\nThis is very nice, because as it turns out: many tools keep their configuration files in this home folder. So by persisting this data, they will keep working across restarts of our Notebook servers.\nLet‚Äôs a closer look at the folders we are persisting:\n\n.local\nWe saw before that the FastAI runtime comes with a number of installed Python packages. If we want to install additional packages, we could do: pip install &lt;package&gt;. However, pip installs the packages in /usr/local/lib, and are thus not persisted. To make sure our packages are persisted, we can instead install with pip install --user &lt;package&gt;. This --user flag, tells pip to install the package only for the current user, and so it installs into the ~/.local directory. So by persisting this folder, we make sure that we our custom installed python packages are persisted, awesome!\n\n\n.ssh\nTo authenticate with GitHub without using passwords, we use ssh keys. To create a pair of keys, we run: ssh-keygen. This creates the private key (id_rsa) and the public key (id_rsa.pub) to the ~/.ssh folder. Once we upload the public key to GitHub we can authenticate with GitHub, and by persisting this folder we can authenticate upon restart!\nBy now you probably get the idea, any of these folders represent a certain configuration we want to persist:\n\n.conda: contains conda/mamba installed packages\n.kaggle: contains a kaggle.json authentication file\n.fastai: contains downloaded datasets and some other configuration\n.config, .ipython and .jupyter: contain config files for various pieces of software such as matplotlib, ipython and jupyter.\n\nI personally also added .huggingface to this list, to make sure my HuggingFace credentials are also persisted. See here for the PR back into the main repo.\nIn the second part of the script we do exactly the same thing, but for a number of files instead of directories.\n\n\npre-run.sh (snippet)\n\nfor p in .git-credentials .gitconfig .bash_history\ndo\n        if [ ! -e /storage/cfg/$p ]; then\n                touch /storage/cfg/$p\n        fi\n        rm -rf ~/$p\n        ln -s /storage/cfg/$p ~/\ndone\n\nNow that we understand pre-run.sh, let‚Äôs have a look at the second file we store in our /storage folder:\n\n\n\n.bash.local\n\n\n.bash.local\n\n#!/usr/bin/env bash\n\nalias mambai='mamba install -p ~/.conda '\nalias pipi='pip install --user '\n\nexport PATH=~/.local/bin:~/.conda/bin/:$PATH\n\nPaperspace runs this script whenever we open a terminal. As you can see it defines two aliases to easily install things persistently with either mamba (mambai) or pip (pipi).\nAny binaries that are installed this way, are installed in ~/.local/bin (through pip) and to ~/.conda/bin/ (through mamba). We need to add these paths to the PATH variable, to make sure we can call them from the command line.\n\n\nNote on Mamba\nAt this point you might wonder why we have the Mamba installation at all, since we have seen that the system Python is used. In fact, our Mamba environment is totally decoupled from what we are using in our Jupyter notebook, and installing packages through mamba will not make them available in Jupyter. Instead, we should install Python packages through pip.\nSo what do we need Mamba for? I guess Jeremy has done this to be able to install binaries that he wants to use from the Terminal. For example, in the videos he talks about ctags which he installs through mamba. Since installing none-Python specific binaries through pip can be complicated, we can use Mamba instead. In other words, we can use it as a general package manager, somewhat similar to apt-get.\n\n\nFinal words\nIn my opinion Paperspace offers a great product for very fair money, especially if combined with the setup described in this blog!"
  },
  {
    "objectID": "posts/08_nntrain_setup/index.html",
    "href": "posts/08_nntrain_setup/index.html",
    "title": "nntrain (0/n): Preliminaries",
    "section": "",
    "text": "In this series, I want to discuss the creation of a small PyTorch based library for training neural networks: nntrain. It‚Äôs based off the excellent part 2 of Practical Deep Learning for Coders by Jeremy Howard, in which from lessons 13 to 18 (roughly) the development of the miniai library is discussed.\nWe‚Äôll try to build everything as much as possible from scratch to understand how things work. Once the main functionality of components is implemented and verified, we can switch over to PyTorch‚Äôs version. This is similar to how things are done in the course. However, this is not just a ‚Äúcopy / paste‚Äù of the course: on many occasions I take a different route, and most of the code is my own. That is not to say that all of this is meant to be extremely innovative, instead I had the following goals:\nnb_dev is another great project from the fastai community, which allows python libraries to be written in jupyter notebooks. This may sound a bit weird and controversial, but it has the advantage that we can create the source code for our library in the very same environment in which we want to experiment and interact with our methods, objects and structure while we are building the library. For more details on why this is a good idea and other nice features of nb_dev, see here.\nSo without further ado, let‚Äôs start with some data!"
  },
  {
    "objectID": "posts/08_nntrain_setup/index.html#data",
    "href": "posts/08_nntrain_setup/index.html#data",
    "title": "nntrain (0/n): Preliminaries",
    "section": "Data",
    "text": "Data\nTo keep things simple, let‚Äôs use the fashion-mnist dataset. We can get the data from the huggingface datasets library:\n\nfrom datasets import load_dataset,load_dataset_builder\n\nname = \"fashion_mnist\"\nds_builder = load_dataset_builder(name)\nprint(ds_builder.info.description)\n\n\n\n\n\n\n\nFashion-MNIST is a dataset of Zalando's article images‚Äîconsisting of a training set of\n60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image,\nassociated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in\nreplacement for the original MNIST dataset for benchmarking machine learning algorithms.\nIt shares the same image size and structure of training and testing splits.\n\n\n\n\nds = load_dataset(name, split='train')\n\nDownloading and preparing dataset fashion_mnist/fashion_mnist (download: 29.45 MiB, generated: 34.84 MiB, post-processed: Unknown size, total: 64.29 MiB) to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1...\nDataset fashion_mnist downloaded and prepared to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1. Subsequent calls will reuse this data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nds is a Dataset object. These kind of objects appear in many Deep Learning libraries and have two main functionalities: you can index into them and they have a length:\n\nds[0]\n\n{'image': &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=28x28&gt;,\n 'label': 9}\n\n\n\nlen(ds)\n\n60000\n\n\n\nds\n\nDataset({\n    features: ['image', 'label'],\n    num_rows: 60000\n})\n\n\nHugginface datasets (as opposed to PyTorch datasets) also have some properties, in this case num_rows, which is the length of the dataset (60000) and features, a dictionary giving metadata on what is returned when we index into the dataset:\n\nds.features\n\n{'image': Image(decode=True, id=None),\n 'label': ClassLabel(num_classes=10, names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)}\n\n\nLet‚Äôs visualize one single item:\n\nimport matplotlib.pyplot as plt\n\nimage = ds[0]['image']\nlabel = ds[0]['label']\n\nfigure, axs = plt.subplots()\n\naxs.imshow(ds[0]['image'], cmap='Greys')\naxs.set_title(f'Image of the first item in the dataset: label={label} -&gt; \"{ds.features[\"label\"].int2str(label)}\"');\naxs.axis('off');\n\n\n\n\nSince we want to start simple, and only later get to Datsets and Dataloaders: let‚Äôs pull out the data into a tensor so we can build simple linear layers.\n\nimport torchvision.transforms.functional as TF   # to transform from PIL to tensor\nimport torch\n\nx_train = [TF.to_tensor(i).view(-1) for i in ds['image']]\ny_train = [torch.tensor(i) for i in ds['label']]\n\nlen(x_train), len(y_train), len(x_train[0])\n\n(60000, 60000, 784)\n\n\nSo x_train and y_train are both lists of length 60000, and an element in x_train has length 784 (28x28 pixels)."
  },
  {
    "objectID": "posts/08_nntrain_setup/index.html#linear-layers",
    "href": "posts/08_nntrain_setup/index.html#linear-layers",
    "title": "nntrain (0/n): Preliminaries",
    "section": "Linear layers",
    "text": "Linear layers\nNow that we have the data, let‚Äôs create our very first network operation: a linear layer which takes the 784 long flattened out image vector, and maps it to an output vector of length 10\n\nimport torch\n\ndef lin(x, a, b):\n    return x@a + b\n\na = torch.randn(784, 10)\nb = torch.randn(10)\n\nout = lin(x_train[0], a, b)\nout.shape\n\ntorch.Size([10])\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor details on matrix multiplications, check out this post I wrote earlier.\n\n\nLet‚Äôs do the same for all our training data at once:\n\nx_train = torch.stack(x_train)\nout = lin(x_train, a,b)\nout.shape\n\ntorch.Size([60000, 10])\n\n\nNice, that‚Äôs basically a forward pass through our model on all our training data!\nNow if we want to increase the depth of our network by adding an additional layer, we need to add a non-linearity in the middle. Why? See for example the first paragraphs of this answer.\nLet‚Äôs add a ReLu nonlinearity:\n\ndef relu(x):\n    return x.clamp_min(0.0)\n\nAnd let‚Äôs combine these into our first ‚Äúmodel‚Äù, consisting of two linear layers and a relu nonlinearity in the middle:\n\nn_in = 784 # number of input units (28x28)\nn_h = 50   # number of hidden units\nn_out = 10 # number of output units\n\nw1 = torch.randn(n_in, n_h)\nb1 = torch.zeros(n_h)\nw2 = torch.randn(n_h, n_out)\nb2 = torch.zeros(n_out)\n\ndef model(x):\n    a1 = lin(x, w1, b1)\n    z1 = relu(a1)\n    return lin(z1, w2, b2)\n\n\nout = model(x_train)\n\n\nout.shape\n\ntorch.Size([60000, 10])\n\n\nOur ‚Äúmodel‚Äù currently only does a forward pass through the network. And as a matter of fact, it‚Äôs doing a forward pass with random weights. When training a neural network, we want to change these parameters in a way that the outputs of the network align with the outputs (y_train). I will not go into the details of this, but here is a great video by Andrej Karpathy which in my opinion gives one of the best explanations into how this works.\nBefore doing a backward pass, we first have to calculate the loss. Since the outputs represent any of the 10 classes the image corresponds with, cross entropy is a straight forward loss function. Some details about cross entropy loss can be found in a post I wrote earlier. However, since we want to add the backpropagation ourselves and I don‚Äôt know how to backpropagate through cross entropy (and I don‚Äôt feel like spending a lot of time on it), let‚Äôs use a much easier loss function for now: mean squared error (MSE). This obviously doesn‚Äôt make any sense in the context of our data, but mathematically it‚Äôs possible. We just have to end up with a single activation of our model instead of 10:\n\nn_out = 1  # number of output units changed to 1\n\nw2 = torch.randn(n_h, n_out)\nb2 = torch.zeros(n_out)\n\ndef model(x):\n    a1 = lin(x, w1, b1)\n    z1 = relu(a1)\n    return lin(z1, w2, b2)\n\nout = model(x_train)\n\n\nout.shape\n\ntorch.Size([60000, 1])\n\n\nFrom which we see that the outputs have an empty trailing dimension. y_train doesn‚Äôt have this, so we have to squeeze out this empty dimension when computing the MSE:\n\ndef mse(pred, targ): \n    return (pred.squeeze(-1)-targ).pow(2).mean() \n\ny_train = torch.stack(y_train)\nmse(out, y_train)\n\ntensor(3015.2351)\n\n\nThe next step will be to add the backward pass. But let‚Äôs refactor our code to put things into classes, that way the backward pass can be added more easily:\n\nclass Linear():\n    def __init__(self, n_in, n_out):\n        self.w = torch.randn(n_in, n_out)\n        self.b = torch.zeros(n_out)\n    \n    def __call__(self, x):\n        self.inp = x                      # storing this for the backward pass\n        self.out = x@self.w + self.b      # storing this for the backward pass\n        return self.out\n    \nclass Relu():\n    def __call__(self, x):\n        self.inp = x                      # storing this for the backward pass\n        self.out = x.clamp_min(0.)        # storing this for the backward pass\n        return self.out\n    \nclass MSE():\n    def __call__(self, pred, targ):\n        self.pred = pred                   # storing this for the backward pass\n        self.targ = targ                   # storing this for the backward pass\n        self.out = (pred.squeeze(-1)-targ).pow(2).mean()\n        return self.out\n    \nclass Model():\n    def __init__(self, n_in, n_h, n_out):\n        self.layers = [Linear(n_in, n_h), Relu(), Linear(n_h, n_out)]\n        self.loss = MSE()\n        \n    def __call__(self, x, y):\n        for l in self.layers: x = l(x)\n        return self.loss(x, y)\n\n\nx_train.shape\n\ntorch.Size([60000, 784])\n\n\n\nm = Model(n_in, n_h, n_out)\nl = m(x_train, y_train)\n\nTo add in the functionality for the backward pass, redefining the whole class is a nuisance. So instead we‚Äôll patch the classes. We can do this very easily by using the fastcore library. Let‚Äôs see a small example:\n\nimport fastcore.all as fc\n\nclass A():\n    def hi(self): print('hello üòé')\n    \na = A()\na.hi()\n\n@fc.patch\ndef hi(self:A): print('howdy ü§†')\n\na.hi()\n\nhello üòé\nhowdy ü§†\n\n\nSo with fc.patch we can extend or change the behavior of Classes that have been defined elsewhere, even on instances of the objects that are already created. Nice!\n\n@fc.patch\ndef backward(self: Linear):\n    self.inp.g = self.out.g @ self.w.t()\n    self.w.g = self.inp.t() @ self.out.g\n    self.b.g = self.out.g.sum(0)\n    \n@fc.patch\ndef backward(self: Relu):\n    self.inp.g = (self.inp&gt;0).float() * self.out.g\n    \n@fc.patch\ndef backward(self: MSE):\n    self.pred.g = 2. * (self.pred.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]\n    \n@fc.patch\ndef backward(self: Model):\n    self.loss.backward()\n    for l in reversed(self.layers): l.backward()\n\n\nm = Model(n_in, n_h, n_out)\nl = m(x_train, y_train)\nm.backward()\n\nNow the actual operations in the backward methods you will just have to take for granted as I am not going to derive them. If you want, you can have some fun (?) to try and derive it yourself. What I think is most important about these formulas:\n\nNotice that each layer has a reference to it‚Äôs inputs and it‚Äôs outputs\nDuring the backward pass, each layer uses the gradient from the outputs and uses it to set the gradient on the inputs\nThe inputs from layer \\(n\\) are the outputs from layer \\(n-1\\), so when the gradients are being set on the inputs from layer \\(n\\), this means that layer \\(n-1\\) it‚Äôs outputs are being set at the same time\nThis is the fundamental point about backpropagation of the gradient: in reverse order, layer by layer the gradients are being propagated back through the network using the chain rule\nAlthough we don‚Äôt derive the operations, we can see that that there exist operations that do this. These operations are not magical, they are just the result of calculus: not very different from the fact that if \\(f(x) = x^2\\) then \\(f'(x) = 2x\\) and if \\(h(x) = f(g(x))\\) then \\(h'(x) = f'(g(x)) * g'(x)\\)"
  },
  {
    "objectID": "posts/08_nntrain_setup/index.html#first-refactor-module-baseclass-and-training-loop",
    "href": "posts/08_nntrain_setup/index.html#first-refactor-module-baseclass-and-training-loop",
    "title": "nntrain (0/n): Preliminaries",
    "section": "First refactor: Module baseclass and training loop",
    "text": "First refactor: Module baseclass and training loop\nNow let‚Äôs see how we can make this a little better. One thing that seems a bit silly is that in each of the Linear, MSE and Relu classes, we are storing explicitly the inputs and outputs when doing a forward call. As mentioned, we need this to backpropagate the gradients. However, we rather not store that explicitly all the time when creating a new layer.\nSo let‚Äôs create a base class that takes care of this:\n\nPack the forward functionality of each layer in a dedicated forward method\nlet the storing of inputs and ouputs be done in the __call__ method of the baseclass, and call the self.forward method in between.\n\nThis works, but there is one caveat: most layers just have one input when they are called (x), but the loss has 2 (pred and targ). To make this storing of the inputs generic we can store them as an array on the base class, and also pass them as positional arguments to _backward. This way, forward and _backward have the same arguments.\n\nclass Module():\n    def __call__(self, *args):\n        self.args = args\n        self.out = self.forward(*args)\n        return self.out\n    \n    def backward(self): self._backward(*self.args)\n\n    \nclass Linear(Module):\n    def __init__(self, n_in, n_out):\n        self.w = torch.randn(n_in, n_out)\n        self.b = torch.zeros(n_out)\n    \n    def forward(self, x):\n        return x@self.w + self.b\n    \n    def _backward(self, inp):\n        inp.g = self.out.g @ self.w.t()\n        self.w.g = inp.t() @ self.out.g\n        self.b.g = self.out.g.sum(0)\n    \n    \nclass Relu(Module):\n    def forward(self, x):\n        return x.clamp_min(0.)\n    \n    def _backward(self, inp):\n        inp.g = (inp&gt;0).float() * self.out.g\n\n    \nclass MSE(Module):\n    def forward(self, pred, targ):\n        return (pred.squeeze(-1)-targ).pow(2).mean()\n    \n    def _backward(self, pred, targ):\n        pred.g = 2. * (pred.squeeze() - targ).unsqueeze(-1) / targ.shape[0]\n    \n    \nclass Model(Module):\n    def __init__(self, n_in, n_h, n_out):\n        self.layers = [Linear(n_in, n_h), Relu(), Linear(n_h, n_out)]\n        self.loss = MSE()\n        \n    def forward(self, x, y):\n        for l in self.layers: x = l(x)\n        return self.loss(x, y)\n    \n    def backward(self):\n        self.loss.backward()\n        for l in reversed(self.layers): l.backward()\n\nWith these objects, let‚Äôs create our first training loop:\n\nepochs = 5                              # train for nr of epochs\nbs     = 1024                           # batch-size\nlr     = 0.01                           # learning rate\nm = Model(n_in, n_h, n_out)             # instantiate our model\n\nfor epoch in range(epochs):             # iterate through epochs\n    for i in range(0,len(x_train), bs): # iterate through the batches\n        xb = x_train[i:i+bs]            # get minibatch \n        yb = y_train[i:i+bs]\n        \n        loss = m(xb, yb)                # forward pass\n        m.backward()                    # backward pass\n        \n        for l in m.layers:              # iterate through the layers\n            if isinstance(l, Linear):   # only update the linear layers\n                l.w += - lr * l.w.g     # update the weights\n                l.b += - lr * l.b.g     # update the bias\n\n                l.w.g = None            # reset the gradients\n                l.b.g = None\n    print(f'{epoch=} | {loss=:.1f}')\n\nepoch=0 | loss=14242.1\nepoch=1 | loss=1329.6\nepoch=2 | loss=135.2\nepoch=3 | loss=21.2\nepoch=4 | loss=9.7\n\n\nAwesome, the loss is decreasing i.e.¬†the model is training!"
  },
  {
    "objectID": "posts/08_nntrain_setup/index.html#second-refactor-simplify-the-weight-update",
    "href": "posts/08_nntrain_setup/index.html#second-refactor-simplify-the-weight-update",
    "title": "nntrain (0/n): Preliminaries",
    "section": "Second refactor: simplify the weight update",
    "text": "Second refactor: simplify the weight update\nLet‚Äôs try to simplify our training loop, and make it more generic. By adding functionality to our Module class so that it has a reference to it‚Äôs trainable parameters, we can update the weights as shown below.\n\ndef fit(epochs):\n    for epoch in range(epochs):\n        for i in range(0,len(x_train), bs):\n            xb = x_train[i:i+bs]\n            yb = y_train[i:i+bs]\n\n            loss = m(xb, yb)\n            m.backward()\n\n            for p in m.parameters():    # model has a reference to the trainable parameters\n                p -= lr * p.g           \n            m.zero_grad()               # model can reset the gradients\n        print(f'{epoch=} | {loss=:.1f}')\n\nTo do so, we will create a new baseclass (NNModule), from which our model and all the layers will inherit. We have the following conditions and properties:\n\nThe class will hold a dictionary _named_args, in which all the named arguments are stored that are set on the Module.\nThis is done by defining a __setattr__ method, which stores any named argument that doesn‚Äôt start with an _ in this dictionary\nFor the Linear, these named arguments will be the parameters w and b\nFor the Model, these named arguments will be layers (an array containing the layer objects) and loss containing the MSE object.\nBecause we want to get the parameters directly out of a layer, as well as out of the model, we need to implement some logic in _parameters() to iterate through the lowest ‚Äúlevel‚Äù and get the actual parameters out\nLast but not least we have to implement a zero_grad() method to zero the gradients on the parameters\n\n\nclass NNModule:\n    def __init__(self):\n        self._named_args = {}                           # [1]\n        \n    def __setattr__(self, name, value):                 # [2]\n        if not name.startswith(\"_\"): self._named_args[name] = value\n        super().__setattr__(name, value)\n        \n    def _parameters(self, obj):                         # [5]\n        for i in obj:\n            if isinstance(i, torch.Tensor): yield i\n            if isinstance(i, NNModule):\n                yield from iter(self._parameters(i._named_args.values()))\n            if isinstance(i, list):\n                yield from iter(self._parameters(i))\n        \n    def parameters(self):\n        return list(self._parameters(self._named_args.values()))\n    \n    def zero_grad(self):\n        for p in self.parameters():\n            p.g = None                                   # [6]\n        \n    def __call__(self, *args):\n        self._args = args                                # NOT stored under _named_args as \\\n        self._out = self.forward(*args)                  # it starts with \"_\"\n        return self._out\n    \n    def backward(self): self._backward(*self._args)\n\n\nclass Linear(NNModule):\n    def __init__(self, n_in, n_out):\n        super().__init__()\n        self.w = torch.randn(n_in, n_out)               # [3] stored under _named_args \n        self.b = torch.zeros(n_out)                     # [3] stored under _named_args\n    \n    def forward(self, x):\n        return x@self.w + self.b\n    \n    def _backward(self, inp):\n        inp.g = self._out.g @ self.w.t()\n        self.w.g = inp.t() @ self._out.g\n        self.b.g = self._out.g.sum(0)\n        \n        \nclass Relu(NNModule):\n    def forward(self, x):\n        return x.clamp_min(0.)\n    \n    def _backward(self, inp):\n        inp.g = (inp&gt;0).float() * self._out.g\n\n    \nclass MSE(NNModule):\n    def forward(self, pred, targ):\n        return (pred.squeeze(-1)-targ).pow(2).mean()\n    \n    def _backward(self, pred, targ):\n        pred.g = 2. * (pred.squeeze() - targ).unsqueeze(-1) / targ.shape[0]\n        \n        \nclass Model(NNModule):\n    def __init__(self, n_in, n_h, n_out):\n        super().__init__()\n        self.layers = [Linear(n_in, n_h), Relu(), Linear(n_h, n_out)]\n        self.loss = MSE()                              # [4] &lt; and ^ are stored under _named_args\n        \n    def forward(self, x, y):\n        for l in self.layers: x = l(x)\n        return self.loss(x, y)\n    \n    def backward(self):\n        self.loss.backward()\n        for l in reversed(self.layers): l.backward()\n\nAnd now we can indeed call parameters on both the model as well as on individual layers:\n\nm = Model(n_in, n_h, n_out)\n[p.shape for p in m.parameters()]\n\n[torch.Size([784, 50]), torch.Size([50]), torch.Size([50, 1]), torch.Size([1])]\n\n\n\n[p.shape for p in Linear(n_in, n_h).parameters()]\n\n[torch.Size([784, 50]), torch.Size([50])]\n\n\nLet‚Äôs fit with our new training loop:\n\nfit(5)\n\nepoch=0 | loss=2118316928.0\nepoch=1 | loss=195283376.0\nepoch=2 | loss=18002500.0\nepoch=3 | loss=1659511.5\nepoch=4 | loss=152958.9"
  },
  {
    "objectID": "posts/08_nntrain_setup/index.html#third-refactor-use-nn.module",
    "href": "posts/08_nntrain_setup/index.html#third-refactor-use-nn.module",
    "title": "nntrain (0/n): Preliminaries",
    "section": "Third refactor: use nn.Module",
    "text": "Third refactor: use nn.Module\nFinally we are in a position to use PyTorch‚Äôs nn.Module, since we understand all of it‚Äôs behavior! We can simplify:\n\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self, n_in, n_h, n_out):\n        super().__init__()\n        self.layers = [nn.Linear(n_in, n_h), nn.ReLU(), nn.Linear(n_h, n_out)]\n        for i,l in enumerate(self.layers):               # ^ we use the nn.Linear and nn.ReLU from PyTorch\n            self.add_module(f'layer_{i}', l)             # we need to register the modules explicitly\n        self.loss = nn.MSELoss()                         # we use the MSELoss from PyTorch\n        \n    def forward(self, x, y):\n        for l in self.layers: x = l(x)\n        return self.loss(x.squeeze(-1), y)\n\n\n# Autograd needs all tensors to be float\nx_train = x_train.to(torch.float32)\ny_train = y_train.to(torch.float32)\nm = Model(n_in, n_h, n_out)\n\n\ndef fit(epochs):\n    for epoch in range(epochs):\n        for i in range(0,len(x_train), bs):\n            xb = x_train[i:i+bs]\n            yb = y_train[i:i+bs]\n\n            loss = m(xb, yb)\n            loss.backward()\n\n            with torch.no_grad():\n                for p in m.parameters():\n                    p -= lr * p.grad\n                m.zero_grad()\n        print(f'{epoch=} | {loss=:.1f}')"
  },
  {
    "objectID": "posts/08_nntrain_setup/index.html#fourth-refactor-nn.modulelist-and-nn.sequential",
    "href": "posts/08_nntrain_setup/index.html#fourth-refactor-nn.modulelist-and-nn.sequential",
    "title": "nntrain (0/n): Preliminaries",
    "section": "Fourth refactor: nn.ModuleList and nn.Sequential",
    "text": "Fourth refactor: nn.ModuleList and nn.Sequential\nTo simplify the storing of the layers array and the registration of the modules, we can use nn.ModuleList. Up till now, we compute the loss as part of the forward pass of the model, let‚Äôs change that and let the model return the predictions. With these predictions we can now also compute a metric: accuracy, which will represent the percentage of images correctly classified by the model.\n\nclass Model(nn.Module):\n    def __init__(self, n_in, n_h, n_out):\n        super().__init__()\n        self.layers = nn.ModuleList([nn.Linear(n_in, n_h), nn.ReLU(), nn.Linear(n_h, n_out)])\n        \n    def forward(self, x, y):\n        for l in self.layers: x = l(x)\n        return x\n\nThis turns out to be such an elementary operation, that PyTorch has a module for it: nn.Sequential.\n\nimport torch.nn.functional as F\n\n\nlayers = [nn.Linear(n_in,n_h), nn.ReLU(), nn.Linear(n_h, n_out)]\nmodel = nn.Sequential(*layers)\n\nAnd let‚Äôs update our training loop as we mentioned:\n\nThe loss needs to be computed separately, since we took it out of the model\nLet‚Äôs now also use a loss function that actually makes sense: cross entropy loss instead of MSE\nWe then need to switch back to using 10 output activations conforming with the 10 categories\n\n\nn_out = 10\n\nlayers = [nn.Linear(n_in,n_h), nn.ReLU(), nn.Linear(n_h, n_out)]\nmodel = nn.Sequential(*layers)\n\nLet‚Äôs also add a metric: accuracy, to see how our model is doing. For this, we need to find the class that our model predicts. However, the model is outputting not a single class, it outputs logits: the unweighted predictions for any of the 10 classes. When applying a softmax to these logits, we turn them into 10 probabilities: the probability that our model assigns to each class.\nWhen computing the accuracy, we don‚Äôt actually just use the logits instead of the probabilities, since the softmax is a monotonically increasing we largest logit, will also have the largest probability.\n\nx0 = x_train[0]\nlogits = model(x0)\n\nprint(f'{logits=}')                           # Logit output of the model\n\nprobs = logits.softmax(dim=0)\n\nprint(f'{probs=}')                            # class probabilites\n\nassert torch.allclose(probs.sum(),            # probabilities sum to 1\n                      torch.tensor(1.0))      \n\nassert torch.all(probs &gt; 0)                   # no negative probabilities\n\nassert (logits.argmax() == probs.argmax())\n\nlogits=tensor([-0.1345,  0.1549, -0.0635,  0.0619,  0.0516, -0.0358,  0.1625, -0.0322,\n        -0.0614,  0.1931], grad_fn=&lt;AddBackward0&gt;)\nprobs=tensor([0.0844, 0.1127, 0.0906, 0.1027, 0.1016, 0.0931, 0.1136, 0.0935, 0.0908,\n        0.1171], grad_fn=&lt;SoftmaxBackward0&gt;)\n\n\n\ndef accuracy(preds, targs):\n    return (preds.argmax(dim=1) == targs).float().mean()\n\n\nloss_func = F.cross_entropy\ny_train = y_train.to(torch.long)\n\nfor epoch in range(epochs):\n    for i in range(0,len(x_train), bs):\n        xb = x_train[i:i+bs]\n        yb = y_train[i:i+bs]\n\n        preds = model(xb)\n        acc = accuracy(preds, yb)\n        loss = loss_func(preds, yb)\n        loss.backward()\n\n        with torch.no_grad():\n            for p in model.parameters():\n                p -= lr * p.grad\n            model.zero_grad()\n    print(f'{epoch=} | {loss=:.3f} | {acc=:.3f}')\n\nepoch=0 | loss=1.071 | acc=0.684\nepoch=1 | loss=0.992 | acc=0.681\nepoch=2 | loss=0.934 | acc=0.688\nepoch=3 | loss=0.889 | acc=0.697\nepoch=4 | loss=0.853 | acc=0.706"
  },
  {
    "objectID": "posts/08_nntrain_setup/index.html#fifth-refactor-add-an-optimizer",
    "href": "posts/08_nntrain_setup/index.html#fifth-refactor-add-an-optimizer",
    "title": "nntrain (0/n): Preliminaries",
    "section": "Fifth refactor: add an Optimizer",
    "text": "Fifth refactor: add an Optimizer\nWe can further refactor the model by adding an Optimizer, this is an object that will have access to the parameters and does the updating of the weights (step) and zeroing the gradient. Most notably, we want to go from:\n\n# ...\n# with torch.no_grad():\n#     for p in model.parameters():\n#         p -= lr * p.grad\n#     model.zero_grad()\n# ...\n\nto:\n\n# opt.step()\n# opt.zero_grad()\n\nSo that the training loop becomes:\n\ndef fit(epochs):\n    for epoch in range(epochs):\n        for i in range(0,len(x_train), bs):\n            xb = x_train[i:i+bs]\n            yb = y_train[i:i+bs]\n\n            preds = model(xb)\n            acc = accuracy(preds, yb)\n            loss = loss_func(preds, yb)\n            loss.backward()\n\n            opt.step()                       # optimizer takes care of the weight update\n            opt.zero_grad()                  # as well as zeroing the grad\n        print(f'{epoch=} | {loss=:.3f} | {acc=:.3f}')\n\nSo we introduce the Optimizer, which has exactly these two methods:\n\nclass Optimizer():\n    def __init__(self, params, lr=0.5):\n        self.params = list(params)\n        self.lr = lr\n        \n    def step(self):\n        with torch.no_grad():\n            for p in self.params: p -= self.lr * p.grad\n        \n    def zero_grad(self):\n        with torch.no_grad():\n            for p in self.params: p.grad.zero_()\n\n\nlayers = [nn.Linear(n_in,n_h), nn.ReLU(), nn.Linear(n_h, n_out)]\nmodel = model = nn.Sequential(*layers)\nopt = Optimizer(model.parameters(), lr)\n\n\nfit(5)\n\nepoch=0 | loss=2.074 | acc=0.447\nepoch=1 | loss=1.832 | acc=0.582\nepoch=2 | loss=1.571 | acc=0.653\nepoch=3 | loss=1.354 | acc=0.676\nepoch=4 | loss=1.195 | acc=0.673\n\n\nThe optimizer we just created is basically the SGD optimizer from PyTorch so let‚Äôs use that:\n\ndef get_model():\n    layers = [nn.Linear(n_in, n_h), nn.ReLU(), nn.Linear(n_h, n_out)]\n    model = nn.Sequential(*layers)\n    \n    opt = torch.optim.SGD(model.parameters(), lr)\n    \n    return model, opt\n\nmodel, opt = get_model()\nfit(5)\n\nepoch=0 | loss=2.026 | acc=0.456\nepoch=1 | loss=1.751 | acc=0.559\nepoch=2 | loss=1.502 | acc=0.605\nepoch=3 | loss=1.314 | acc=0.630\nepoch=4 | loss=1.179 | acc=0.635"
  },
  {
    "objectID": "posts/08_nntrain_setup/index.html#end",
    "href": "posts/08_nntrain_setup/index.html#end",
    "title": "nntrain (0/n): Preliminaries",
    "section": "End",
    "text": "End\nWe have come a long way, and covered a lot of ground. We have seen many of the fundamental components of training a neural network: the data, a simple model, training loops, loss functions, metrics and optimizers. We have seen why things like nn.Module exist, and understand it‚Äôs behavior. Furthermore, we have seen that the need for nn.Module and torch.optim comes out of the need for simplifying things in the training loop.\nIn the next post, we will get to datasets and dataloaders as a way to further improve the training loop, and we will start adding our first things into the nntrain library üï∫."
  },
  {
    "objectID": "posts/09_nntrain_ds/index.html",
    "href": "posts/09_nntrain_ds/index.html",
    "title": "nntrain (1/n): Datasets and Dataloaders",
    "section": "",
    "text": "In this series, I want to discuss the creation of a small PyTorch based library for training neural networks: nntrain. It‚Äôs based off the excellent part 2 of Practical Deep Learning for Coders by Jeremy Howard, in which from lessons 13 to 18 (roughly) the development of the miniai library is discussed.\nWe‚Äôll try to build everything as much as possible from scratch to understand how things work. Once the main functionality of components is implemented and verified, we can switch over to PyTorch‚Äôs version. This is similar to how things are done in the course. However, this is not just a ‚Äúcopy / paste‚Äù of the course: on many occasions I take a different route, and most of the code is my own. That is not to say that all of this is meant to be extremely innovative, instead I had the following goals:\nnb_dev is another great project from the fastai community, which allows python libraries to be written in jupyter notebooks. This may sound a bit weird and controversial, but it has the advantage that we can create the source code for our library in the very same environment in which we want to experiment and interact with our methods, objects and structure while we are building the library. For more details on why this is a good idea and other nice features of nb_dev, see here.\nSo without further ado, let‚Äôs start with where we left off in the previous post:"
  },
  {
    "objectID": "posts/09_nntrain_ds/index.html#end-of-last-post",
    "href": "posts/09_nntrain_ds/index.html#end-of-last-post",
    "title": "nntrain (1/n): Datasets and Dataloaders",
    "section": "End of last post",
    "text": "End of last post\nfrom datasets import load_dataset,load_dataset_builder\nimport torchvision.transforms.functional as TF\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nname = \"fashion_mnist\"\nds_builder = load_dataset_builder(name)\nds_hf = load_dataset(name, split='train')\n\nx_train = torch.stack([TF.to_tensor(i).view(-1) for i in ds_hf['image']])\ny_train = torch.stack([torch.tensor(i) for i in ds_hf['label']])\n\ndef fit(epochs):\n    for epoch in range(epochs):\n        for i in range(0,len(x_train), bs):\n            xb = x_train[i:i+bs]\n            yb = y_train[i:i+bs]\n\n            preds = model(xb)\n            acc = accuracy(preds, yb)\n            loss = loss_func(preds, yb)\n            loss.backward()\n\n            opt.step()\n            opt.zero_grad()\n        print(f'{epoch=} | {loss=:.3f} | {acc=:.3f}')\n\ndef accuracy(preds, targs):\n    return (preds.argmax(dim=1) == targs).float().mean()        \n\ndef get_model_opt():\n    layers = [nn.Linear(n_in, n_h), nn.ReLU(), nn.Linear(n_h, n_out)]\n    model = nn.Sequential(*layers)\n    \n    opt = torch.optim.SGD(model.parameters(), lr)\n    \n    return model, opt\n\nn_in  = 28*28\nn_h   = 50\nn_out = 10\nlr    = 0.01\nbs    = 1024\nloss_func = F.cross_entropy\n\nmodel, opt = get_model_opt()\nfit(5)"
  },
  {
    "objectID": "posts/09_nntrain_ds/index.html#datasets",
    "href": "posts/09_nntrain_ds/index.html#datasets",
    "title": "nntrain (1/n): Datasets and Dataloaders",
    "section": "Datasets",
    "text": "Datasets\nAll the stuff in this post will be based on tackling the minibatch construct we currently have in the training loop on lines 16-18:\n...\nfor i in range(0,len(x_train), bs):\n    xb = x_train[i:i+bs]\n    yb = y_train[i:i+bs]\n...\nAnd the first refactor will be to create a Dataset object, which allows us to simplify:\n...\nfor i in range(0,len(x_train), bs):\n    xb, yb = dataset[i:i+bs]\n...\nThis is pretty straight-forward, a Dataset is something that holds our data and upon ‚Äúindexing into‚Äù it returns a sample of the data:\n\nclass Dataset():\n    \n    def __init__(self, x_train, y_train):\n        self.x_train = x_train\n        self.y_train = y_train\n        \n    def __getitem__(self, i):\n        return self.x_train[i], self.y_train[i]\n    \n    def __len__(self):\n        return len(self.x_train)\n\n\nds = Dataset(x_train, y_train)\nprint([i.shape for i in ds[0]])\n\n[torch.Size([784]), torch.Size([])]\n\n\nNext, we want to further improve the training loop and get to this behavior:\n...\nfor xb, yb in dataloader:\n...\nSo our dataloader needs to wrap the dataset, and provide some kind of an iterator returning batches of data, based on the specified batch size. Let‚Äôs create one:\n\nclass DataLoader():\n    \n    def __init__(self, dataset, batch_size):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        \n    def __iter__(self):\n        for i in range(0,len(self.dataset),self.batch_size):\n            yield self.dataset[i:i+self.batch_size]\n\nNow the training loop is simplified to:\n\ndef fit(epochs):\n    for epoch in range(epochs):\n        for xb, yb in dl:\n            preds = model(xb)\n            acc = accuracy(preds, yb)\n            loss = loss_func(preds, yb)\n            loss.backward()\n\n            opt.step()\n            opt.zero_grad()\n        print(f'{epoch=} | {loss=:.3f} | {acc=:.3f}')\n\n\ndl = DataLoader(ds, bs)\n\nmodel, opt = get_model_opt()\nfit(5)\n\nepoch=0 | loss=2.062 | acc=0.441\nepoch=1 | loss=1.785 | acc=0.597\nepoch=2 | loss=1.531 | acc=0.637\nepoch=3 | loss=1.334 | acc=0.645\nepoch=4 | loss=1.190 | acc=0.660"
  },
  {
    "objectID": "posts/09_nntrain_ds/index.html#next-up-shuffling-the-data",
    "href": "posts/09_nntrain_ds/index.html#next-up-shuffling-the-data",
    "title": "nntrain (1/n): Datasets and Dataloaders",
    "section": "Next up: shuffling the data",
    "text": "Next up: shuffling the data\nThe above training loop already looks pretty good, it‚Äôs small and concise, and fairly generic. The next improvement we are going to make is something that doesn‚Äôt improve the code of the training loop, but improves training of the model. So far during training, we cycle each epoch through the data in the exact same order. This means that all training samples are always batched together with the exact same other samples. This is not good for training our model, instead we want to shuffle the data up. So that each epoch, we have batches of data that have not yet been batched up together. This additional variation helps the model to generalize as we will see.\nThe simplest implementation would be to create a list of indices, which we put in between the dataset and the sampling of the mini-batches. In case we don‚Äôt need to shuffle, this list will just be [0, 1, ... len(dataset)].\n\nimport random\n\nclass DataLoader():\n    \n    def __init__(self, dataset, batch_size, shuffle):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        \n    def __iter__(self):\n        self.indices = list(range(0, len(self.dataset)))\n        if self.shuffle: \n            random.shuffle(self.indices)\n            \n        for i in range(0,len(self.dataset),self.batch_size):\n            yield self.dataset[self.indices[i:i+self.batch_size]]\n\n\nmodel, opt = get_model_opt()\ndl = DataLoader(ds, bs, shuffle=True)\nfit(5)\n\nepoch=0 | loss=2.067 | acc=0.429\nepoch=1 | loss=1.800 | acc=0.515\nepoch=2 | loss=1.539 | acc=0.592\nepoch=3 | loss=1.358 | acc=0.618\nepoch=4 | loss=1.187 | acc=0.692\n\n\nThis works just fine, but let‚Äôs see if we can encapsulate this logic in a separate class. We start with a simple Sampler class that we can iterate through and either gives indices in order, or shuffled:\n\nclass Sampler():\n    def __init__(self, ds, shuffle=False):\n        self.range = list(range(0, len(ds)))\n        self.shuffle = shuffle\n        \n    def __iter__(self):\n        if self.shuffle: random.shuffle(self.range)\n        for i in self.range:\n            yield i\n\n\ns = Sampler(ds, False)           # shuffle = False\nfor i, sample in enumerate(s): \n    print(sample, end=', ')\n    if i == 5: break\n\n0, 1, 2, 3, 4, 5, \n\n\n\ns = Sampler(ds, True)            # shuffle = TRUE\nfor i, sample in enumerate(s): \n    print(sample, end=', ')\n    if i == 5: break\n\n58844, 19394, 36509, 38262, 51037, 46835, \n\n\nNext, let‚Äôs create a BatchSampler that does the same, but returns the indexes in batches. For that we can use the islice() function from the itertools module:\n\nfrom itertools import islice\n\ndef printlist(this): print(list(this))\n\nlst = list(range(0, 10))         # create a list of 10 numbers\n\nprintlist(islice(lst, 0, 3))     # with islice we can get a slice out of the list\nprintlist(islice(lst, 5, 10))\n\n[0, 1, 2]\n[5, 6, 7, 8, 9]\n\n\n\nprintlist(islice(lst, 4))        # we can also get the \"next\" 4 elements\nprintlist(islice(lst, 4))        # doing that twice gives the same first 4 elements\n\n[0, 1, 2, 3]\n[0, 1, 2, 3]\n\n\n\nlst = iter(lst)                  # however if we put an iterator on the list:\n\nprintlist(islice(lst, 4))        # first 4 elements\nprintlist(islice(lst, 4))        # second 4 elements\nprintlist(islice(lst, 4))        # remaining 2 elements\nprintlist(islice(lst, 4))        # iterator has finished..\n\n[0, 1, 2, 3]\n[4, 5, 6, 7]\n[8, 9]\n[]\n\n\nAnd thus we create our BatchSampler:\n\nclass BatchSampler():\n    def __init__(self, sampler, batch_size):\n        self.sampler = sampler\n        self.batch_size = batch_size\n        \n    def __iter__(self):\n        it = iter(self.sampler)\n        while True:\n            res = list(islice(it, self.batch_size))\n            if len(res) == 0:    # return when the iterator has finished          \n                return           \n            yield res\n\nLet‚Äôs see the BatchSamepler in action:\n\ns = Sampler(list(range(0,10)), shuffle=False)\nbatchs = BatchSampler(s, 4)\nfor i in batchs:\n    printlist(i)\n\n[0, 1, 2, 3]\n[4, 5, 6, 7]\n[8, 9]\n\n\nAnd let‚Äôs incorporate it into the DataLoader:\n\nclass DataLoader():\n    \n    def __init__(self, dataset, batch_sampler):\n        self.dataset = dataset\n        self.batch_sampler = batch_sampler\n        \n    def __iter__(self):\n        for batch in self.batch_sampler:\n            yield self.dataset[batch]\n\n\ns = Sampler(ds, shuffle=True)\ndl = DataLoader(ds, BatchSampler(s, bs))\n\nmodel, opt = get_model_opt()\nfit(5)\n\nepoch=0 | loss=1.981 | acc=0.462\nepoch=1 | loss=1.698 | acc=0.567\nepoch=2 | loss=1.468 | acc=0.620\nepoch=3 | loss=1.346 | acc=0.613\nepoch=4 | loss=1.202 | acc=0.656"
  },
  {
    "objectID": "posts/09_nntrain_ds/index.html#collation",
    "href": "posts/09_nntrain_ds/index.html#collation",
    "title": "nntrain (1/n): Datasets and Dataloaders",
    "section": "Collation",
    "text": "Collation\nAnd this works pretty good. However, there is one caveat. In the very beginning of this post we did:\nx_train = torch.stack([TF.to_tensor(i).view(-1) for i in ds_hf['image']])\ny_train = torch.stack([torch.tensor(i) for i in ds_hf['label']])\nAnd we ideally would like these transformations to be part of the Dataloaders / Dataset paradigm. So instead of first transforming the Huggingface Dataset into x_train and y_train, we want to directly use the dataset. We can do so by adding a collate function. This wraps around a list of individual samples into the datasets, and receives a list of individual x,y tuples ([(x1,y1), (x2,y2), ..]) as argument. In that function, we can determine how to treat these items and parse it in a way that is suitable to our needs. i.e.:\n\nbatch the x and y, so that we transform from [(x1,y1), (x2,y2), ..] to [(x_1,x_2, ..), (y_1,y_2, ..)]\nmove individual items x_i and y_i to tensors\nstack the x tensors and y tensors respectively into one big tensor\n\nSo let‚Äôs update our DataLoader with a collate_func that wraps around individual samples:\n\nclass DataLoader():\n    \n    def __init__(self, dataset, batch_sampler, collate_func):\n        self.dataset = dataset\n        self.batch_sampler = batch_sampler\n        self.collate_func = collate_func\n        \n    def __iter__(self):\n        for batch in self.batch_sampler:\n            yield self.collate_func(self.dataset[sample] for sample in batch)\n\nAnd now let‚Äôs create a custom collate function to deal with our data. Specifically, remember that a sample of our huggingface dataset is a dictionary (and not a tuple) with keys image and label holding a PIL.Image.image object and a number (representing any out of 10 classes) respectively.\nSo our collate_func should:\n\ntransform the dictionary into a tuple\nmove everything to a tensor\nzip the results so that x and y are batched\nand combine the list of tensors for x and y respectively into one big tensor\n\n\ndef collate_func(data):\n    data = [(TF.to_tensor(sample['image']).view(-1), torch.tensor(sample['label'])) for sample in data]\n    x, y = zip(*data)\n    return torch.stack(x), torch.stack(y)\n\nAnd let‚Äôs see it in action, now using the huggingface dataset ds_hf:\n\ns = Sampler(ds_hf, shuffle=True)\ndl = DataLoader(ds_hf, BatchSampler(s, bs), collate_func)\n\nmodel, opt = get_model_opt()\nfit(5)\n\nepoch=0 | loss=2.125 | acc=0.345\nepoch=1 | loss=1.899 | acc=0.497\nepoch=2 | loss=1.635 | acc=0.609\nepoch=3 | loss=1.389 | acc=0.640\nepoch=4 | loss=1.260 | acc=0.641\n\n\nNot bad, we have replicated the main logic of PyTorch‚Äôs DataLoader. The version from PyTorch has a slightly different API as we don‚Äôt have to specify the BatchSampler, instead we can just pass shuffle=True:\n\nfrom torch.utils.data import DataLoader\n\ns = Sampler(ds_hf, shuffle=True)\ndl = DataLoader(ds_hf, batch_size=bs, shuffle=True, collate_fn=collate_func)\n\nmodel, opt = get_model_opt()\nfit(5)\n\nepoch=0 | loss=2.107 | acc=0.434\nepoch=1 | loss=1.840 | acc=0.620\nepoch=2 | loss=1.605 | acc=0.641\nepoch=3 | loss=1.354 | acc=0.641\nepoch=4 | loss=1.258 | acc=0.618"
  },
  {
    "objectID": "posts/09_nntrain_ds/index.html#validation-set",
    "href": "posts/09_nntrain_ds/index.html#validation-set",
    "title": "nntrain (1/n): Datasets and Dataloaders",
    "section": "Validation set",
    "text": "Validation set\nLet‚Äôs add a validation set to make sure we validate on data we are not training on. For that we are going to pull the data from the datasets library without the splits argument, which will give us a dataset dictionary containing both a training and a test dataset:\n\nhf_dd = load_dataset(name)\nhf_dd\n\nReusing dataset fashion_mnist (/root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1)\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 60000\n    })\n    test: Dataset({\n        features: ['image', 'label'],\n        num_rows: 10000\n    })\n})\n\n\nAnd let‚Äôs create two dataloaders, one for the train and one for the validation set. For the validation loader we can double the batch size since we won‚Äôt be computing gradients for the forward pass:\n\ntrain_loader = DataLoader(hf_dd['train'], batch_size=bs, shuffle=True, collate_fn=collate_func)\nvalid_loader = DataLoader(hf_dd['test'], batch_size=2*bs, shuffle=False, collate_fn=collate_func)\n\nWe change the training loop in a couple of ways:\n\ncompute loss and metrics more correctly, by taking care of the batch-size and taking the average over all data\nadd a seperate forward pass for the validation set\n\n\ndef fit(epochs):\n    for epoch in range(epochs):\n        model.train()                                       # put the model in \"train\" mode\n        n_t = train_loss_s = 0                              # initialize variables for computing averages\n        for xb, yb in train_loader:\n            preds = model(xb)\n            train_loss = loss_func(preds, yb)\n            train_loss.backward()\n            \n            n_t += len(xb)\n            train_loss_s += train_loss.item() * len(xb)\n            \n            opt.step()\n            opt.zero_grad()\n        \n        model.eval()                                        # put the model in \"eval\" mode\n        n_v = valid_loss_s = acc_s = 0                      # initialize variables for computing averages\n        for xb, yb in valid_loader:\n            with torch.no_grad():                           # no need to compute gradients on validation set\n                preds = model(xb)\n                valid_loss = loss_func(preds, yb)\n                \n                n_v += len(xb)\n                valid_loss_s += valid_loss.item() * len(xb)\n                acc_s += accuracy(preds, yb) * len(xb)\n        \n        train_loss = train_loss_s / n_t                     # compute averages of loss and metrics\n        valid_loss = valid_loss_s / n_v\n        acc = acc_s / n_v\n        print(f'{epoch=} | {train_loss=:.3f} | {valid_loss=:.3f} | {acc=:.3f}')\n\n\nmodel, opt = get_model_opt()\n\nfit(5)\n\nepoch=0 | train_loss=2.198 | valid_loss=2.095 | acc=0.276\nepoch=1 | train_loss=1.980 | valid_loss=1.852 | acc=0.539\nepoch=2 | train_loss=1.718 | valid_loss=1.591 | acc=0.617\nepoch=3 | train_loss=1.481 | valid_loss=1.387 | acc=0.624\nepoch=4 | train_loss=1.305 | valid_loss=1.241 | acc=0.637\n\n\nAnd that‚Äôs it for this post (almost)! We have seen a lot of details on Datasets, Dataloaders and the transformation of data. We have used these concepts to improve our training loop: shuffling the training data on each epoch, and the computation of the metrics on the validation set. But before we close off, let‚Äôs make our very first exports into the library, so that next time we can continue where we finished off."
  },
  {
    "objectID": "posts/09_nntrain_ds/index.html#first-exports",
    "href": "posts/09_nntrain_ds/index.html#first-exports",
    "title": "nntrain (1/n): Datasets and Dataloaders",
    "section": "First exports",
    "text": "First exports\nWhen exporting code to a module with nbdev the first thing we need to do is declare the default_exp directive. This makes sure that when we run the export, the module will be exported to dataloaders.py\n\n #| default_exp dataloaders\n\nNext, we can export any code into the module by adding #|export on top of the cell we want to export. For example:\n\n #| export\n\ndef print_hello():\n    print('hello')\n\nTo export, we simply execute:\n\nimport nbdev; nbdev.nbdev_export()\n\nThis will create a file called dataloaders.py in the library folder (in my case nntrain) with the contents:\n# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_dataloaders.ipynb.\n\n# %% auto 0\n__all__ = ['func']\n\n# %% ../nbs/01_dataloaders.ipynb 59\ndef print_hello():\n    print('hello')\nSo what do we want to export here? Let‚Äôs see if we can create some generic code for loading data from the Huggingface datasets library into a PyTorch Dataloader:\n\n #|export\nimport torchvision.transforms.functional as TF\nfrom torch.utils.data import DataLoader\nimport torch\nimport PIL\n\n\n #|export\n\ndef hf_ds_collate_fn(data, flatten=True):\n    '''\n    Collation function for building a PyTorch DataLoader from a a huggingface dataset.\n    Tries to put all items from an entry into the dataset to tensor.\n    PIL images are converted to tensor, either flattened or not \n    '''\n\n    def to_tensor(i, flatten):\n        if isinstance(i, PIL.Image.Image):\n            if flatten:\n                return torch.flatten(TF.to_tensor(i))\n            return TF.to_tensor(i)\n        else:\n            return torch.tensor(i)\n    \n    to_tensor = partial(to_tensor, flatten=flatten)      # partially apply to_tensor() with flatten arg\n    data = [map(to_tensor, el.values()) for el in data]  # map each item from a dataset entry through to_tensor()\n    data = zip(*data)                                    # zip data of any length not just (x,y) but also (x,y,z)\n    return (torch.stack(i) for i in data)\n\n\n #|export\nclass DataLoaders:\n    def __init__(self, train, valid):\n        '''Class that exposes two PyTorch dataloaders as train and valid arguments'''\n        self.train = train\n        self.valid = valid\n    \n    @classmethod\n    def _get_dls(cls, train_ds, valid_ds, bs, collate_fn, **kwargs):\n        '''Helper function returning 2 PyTorch Dataloaders as a tuple for 2 Datasets. **kwargs are passed to the DataLoader'''\n        return (DataLoader(train_ds, batch_size=bs, shuffle=True, collate_fn=collate_fn, **kwargs),\n                DataLoader(valid_ds, batch_size=bs*2, collate_fn=collate_fn, **kwargs))\n        \n    @classmethod\n    def from_hf_dd(cls, dd, batch_size, collate_fn=hf_ds_collate_fn, **kwargs):\n        '''Factory method to create a Dataloaders object for a Huggingface Dataset dict,\n        uses the `hf_ds_collate_func` collation function by default, **kwargs are passes to the DataLoaders'''\n        return cls(*cls._get_dls(*dd.values(), batch_size, collate_fn, **kwargs))\n\nWith show_doc() we can include the documentations of class methods:\n\n #|hide\nfrom nbdev.showdoc import *\n\n\nshow_doc(DataLoaders.from_hf_dd)\n\n\n\nDataLoaders.from_hf_dd\n\n DataLoaders.from_hf_dd (dd, batch_size, collate_fn=&lt;function\n                         hf_ds_collate_fn&gt;, **kwargs)\n\nFactory method to create a Dataloaders object for a Huggingface Dataset dict, uses the hf_ds_collate_func collation function by default, **kwargs are passes to the DataLoaders\n\n\n\nExample usage:\n\ndef fit(epochs):\n    for epoch in range(epochs):\n        model.train()                                       \n        n_t = train_loss_s = 0                              \n        for xb, yb in dls.train:\n            preds = model(xb)\n            train_loss = loss_func(preds, yb)\n            train_loss.backward()\n            \n            n_t += len(xb)\n            train_loss_s += train_loss.item() * len(xb)\n            \n            opt.step()\n            opt.zero_grad()\n        \n        model.eval()                                        \n        n_v = valid_loss_s = acc_s = 0                      \n        for xb, yb in dls.valid: \n            with torch.no_grad():                           \n                preds = model(xb)\n                valid_loss = loss_func(preds, yb)\n                \n                n_v += len(xb)\n                valid_loss_s += valid_loss.item() * len(xb)\n                acc_s += accuracy(preds, yb) * len(xb)\n        \n        train_loss = train_loss_s / n_t                     \n        valid_loss = valid_loss_s / n_v\n        acc = acc_s / n_v\n        print(f'{epoch=} | {train_loss=:.3f} | {valid_loss=:.3f} | {acc=:.3f}')\n\n\nhf_dd = load_dataset('fashion_mnist')\nbs    = 1024\ndls = DataLoaders.from_hf_dd(hf_dd, bs)\n\nReusing dataset fashion_mnist (/root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1)\n\n\n\n\n\n\nmodel, opt = get_model_opt()\n\nfit(1)\n\nepoch=0 | loss=2.094 | acc=0.431\n\n\n\n #|hide\nimport nbdev; nbdev.nbdev_export()\n\nAnd that‚Äôs it. We have created our first module of the nntrain libraryüï∫. Links:\n\nDataloaders Notebook: the ‚Äúsource‚Äù of the source code\nDataloaders module: the .py source code exported from the notebook\nDocumentation: automatically created from the notebook and hosted on Github"
  },
  {
    "objectID": "posts/06_stable_diffusion_basics/index.html",
    "href": "posts/06_stable_diffusion_basics/index.html",
    "title": "Introduction to Stable Diffusion - Concepts",
    "section": "",
    "text": "Stable Diffusion, a generative deep learning algorithm developed in 2022, is capable of creating images from prompts. For example, when presented the prompt: A group of people having lunch on the moon, the algorithm creates the following image:\nAnd although this image isn‚Äôt perfect, it‚Äôs pretty amazing that it took less then 30 seconds to create this image. The algorithm ‚Äúimagined‚Äù that people on the moon should be wearing space suits, and that lunch is generally eaten in a sitting position and around a table. Also, the surroundings look indeed pretty moonish. Not bad at all!\nIn this post, we will have a look at the main components involved in creating this image, and follows largely the steps of Lesson 9 of Deep Learning for Coders."
  },
  {
    "objectID": "posts/06_stable_diffusion_basics/index.html#intuition",
    "href": "posts/06_stable_diffusion_basics/index.html#intuition",
    "title": "Introduction to Stable Diffusion - Concepts",
    "section": "Intuition",
    "text": "Intuition\nConsider some kind of black box system that takes some input data, and based on it creates some output data. Let‚Äôs say, it takes an image of a handwritten digit as input, and outputs the probability that the image is indeed a hand written digit. Visually something like this:\n\n\n\n\n\nIn statistics, we would call this a probability density function. It‚Äôs a function that takes data as input, and gives the probability that\n\nif the input data is coming indeed from the distribution,\nwhat‚Äôs the probability that we see this data?\n\nApplied to our use-case: if the presented image is indeed from the distribution (of images) that represent hand written digits, what‚Äôs the probability that we observe the presented image?\nWith such a system, we could start with an image made up of pure noise and iteratively do:\n\nget the probability \\(p_0\\) of the image being a handwritten digit from the black box system\nchange the value of one of the pixels at random\nget the new probability \\(p_1\\) whether the image is a handwritten digit from the black box system\nwhen \\(p_1 &gt; p_0\\) update the image with the changed pixel value\n\nWhen following this procedure long enough and thus updating pixel for pixel, we would gradually change all the values of our pixels of our image, until eventually it will start to resemble a handwritten digit.\nIn principle, this is the simple intuition behind stable diffusion."
  },
  {
    "objectID": "posts/06_stable_diffusion_basics/index.html#the-main-component-unet",
    "href": "posts/06_stable_diffusion_basics/index.html#the-main-component-unet",
    "title": "Introduction to Stable Diffusion - Concepts",
    "section": "The main component: Unet",
    "text": "The main component: Unet\nSo how are we going to create this system that will return the probability that an image is depicting a handwritten digit? Let‚Äôs try to create a model, that will do so. To get the training data, we need lots of images that depict handwritten digits. Something like this:\n\n\n\n\n\nSince these images represent actual hand-written digits, the system will need to output a probability close to 1 for these images. But how do we get images that ‚Äúsomewhat‚Äù or ‚Äúrarely‚Äù represent handwritten digits and are associated with lower probability values? We somehow have to ‚Äúcrappify‚Äù these existing images. We can do this by using these same images and sprinkle them with different amounts of noise. The more noise we add, the less the image will resemble a handwritten digit. Visually:\n\n\n\n\n\nNow we can train a network which we feed the noisified images as input and use the noise image as label. So instead of predicting the probability that an image depicts a handwritten digit, the model will predict the noise. By using a simple MSE loss on the actual noise (labels) and the predictions the model will learn how to predict the noise from looking at a noisified images.\nThe idea behind this model is that once this model is trained, we could run inference on some random noise. The model will give us a prediction of all the noise in the image, which when removed from the input, renders an image of a digit.\nIt turns out that this process works much better if, instead of removing all the noise that was predicted by the model at once, we just remove a little bit of the noise that was predicted. This way, we end up with an image which is just a bit less noisy then what we started with. We then feed this less noisy image again into our network, and thus iteratively remove more and more noise from the image, until after a certain amount of steps (50 for example) we end-up with an image that is free of noise.\nOne model architecture that is takes images as input and also outputs images is called a Unet and forms the first component of our Stable Diffusion system:\n\n\n\n\n\n\n\n\nComponent\nInputs\nOutputs\n\n\n\n\nUnet\nNoisy images\nNoise"
  },
  {
    "objectID": "posts/06_stable_diffusion_basics/index.html#compression-variational-autoencoder",
    "href": "posts/06_stable_diffusion_basics/index.html#compression-variational-autoencoder",
    "title": "Introduction to Stable Diffusion - Concepts",
    "section": "Compression: Variational Autoencoder",
    "text": "Compression: Variational Autoencoder\nWhen working with images in neural networks we often reduce the resolution of images or use smaller patches of the original image to make sure everything fits on the GPU. With stable diffusion, we naturally want to output images of high resolution, so we either need very large GPUs, or instead we use a compression trick by making use of a Variational Autoencoder (VAE).\nA VAE is a network architecture having an encoder and a decoder. In the encoder the image input is being transformed through a series of convolutional layers into a compressed representation, the latent. In the decoder this compressed latent is passed through a series of layers that are trying to reconstruct the original image. Visually:\n\n\n\n\n\nThis might look like a boring network architecture at first. But it‚Äôs actually a very neat way to compress things: We can feed this model all the different noisified images mentioned earlier, and use an MSE loss on the inputs and outputs. This will train the model to create compressed representations of our images (the latents) that can be used by the decoder to recreate the original image. This means that the latent representation carries close to the same amount of ‚Äúinformation‚Äù as our full-size images.\nWith this, we can now train the previously discussed Unet on all the latents instead of the full size images!\nDuring inference the combined architecture looks like this: we run any input first through the encoder returning a highly compressed version of our input (i.e.¬†the latents). We then run it through the Unet, which will output a latent representation of the noise. If we (partly) subtract the noise latent from the noisy image latent, we end up with a latent representation of our image which is a bit less noisy then what we started with. Finally, to move from latent representation to full-size images, we can use the decoder of the VAE. Visually:\n\n\n\n\n\nTo summarize:\n\n\n\n\n\n\n\n\nComponent\nInputs\nOutputs\n\n\n\n\nVAE encoder\nNoisy image\nNoisy image latents\n\n\nUnet\nNoisy image latents\nNoise latents\n\n\nVAE decoder\nNoise latents\nNoise"
  },
  {
    "objectID": "posts/06_stable_diffusion_basics/index.html#prompting-clip",
    "href": "posts/06_stable_diffusion_basics/index.html#prompting-clip",
    "title": "Introduction to Stable Diffusion - Concepts",
    "section": "Prompting: CLIP",
    "text": "Prompting: CLIP\nSo how can we create prompting? Let‚Äôs start simple and imagine we just want to specify which handwritten digit we would like to generate, so any number between 0 and 9. We could do this by training the Unet not only on the noisy image (input) and noise (output), but instead also give it a representation of the digit we sprinkled the noise on as input. The most generic way to do this, would be to create a one-hot encoded representation of the digit, visually:\n\n\n\n\n\nTo create an image depicting the digit ‚Äúthree‚Äù from pure noise, we would then start with a random noise latent and feed it together with the one-hot encoded representation of the digit into the Unet. This way, the Unet is ‚Äúguided‚Äù to create an image of digit ‚Äúthree‚Äù and not just any image, visually:\n\n\n\n\n\nTo continue, how are we going to scale this for any text prompt besides our 10 digits? We can‚Äôt possibly create a one-hot encoding of any possible prompt, that would make our vector infinitely large. Instead, we want to compress the encoding in some finite, high dimensional space, e.g.¬†we want to create an embedding encoding of our prompt.\nTo create these embeddings, we first of all need again lots of data. For example by capturing a lot of images from the internet, these image generally have a textual description in the HTML tag.\nWe can feed the text and images into two separate encoders. These encoders take the text and image respectively and output a vector. Next, we can align the vector representations in a matrix and take the dot-product between them. We want the text and image vectors of the same ‚Äúobject‚Äù to align, this means their dot-product should be large. Also, we want the vectors of different objects to not align, so their dot-product should be small. Visually:\n\n\n\n\n\nA loss function that does exactly this, is called the Contrastive Loss. And the model described here is called Contrastive Language Image Pre-training (CLIP).\nDuring inference, we can use the trained text-encoder and apply it to the prompt. The outputted embedding can then be used as the encoding we feed into our Unet in combination with the noisy image latent.\nTo summarize:\n\n\n\n\n\n\n\n\nComponent\nInputs\nOutputs\n\n\n\n\nCLIP text encoder\nPrompt\nEmbedding\n\n\nVAE encoder\nNoisy image\nNoisy image latents\n\n\nUnet\nNoisy image latents + Prompt embedding\nNoise latents\n\n\nVAE decoder\nNoise latents\nNoise"
  },
  {
    "objectID": "posts/06_stable_diffusion_basics/index.html#noise-scheduler",
    "href": "posts/06_stable_diffusion_basics/index.html#noise-scheduler",
    "title": "Introduction to Stable Diffusion - Concepts",
    "section": "Noise scheduler",
    "text": "Noise scheduler\nAbove it was stated, that ‚Äúdifferent‚Äù amounts of noise are sprinkled on our images during training, and during inference ‚Äúsome‚Äù amount of noise is being subtracted from the image. In the next post, which will be a ‚Äúcode‚Äù version of this post, we will see more how this exactly works, but let‚Äôs introduce one more concept here:\nTo formalize the amounts of noise we will use something called a noise schedule, which maps an integer value (called the timestep \\(t\\)) to an amount of noise we will add to our image. This noise schedule is a monotonically decreasing function of \\(t\\), so large values of \\(t\\) will add a small amount of noise and small values of \\(t\\) add a large amount of noise. A typical noise schedule looks something like this:\n\n\n\n\n\nWith this noise schedule, we can pick different amounts of noise during training and add it to the images in the batch. Additionally, we will feed the noise parameter to the Unet, so that it knows how much noise was added to the image. This sould make it easier for the model to reconstruct the noise.\nTo summarize:\n\n\n\n\n\n\n\n\nComponent\nInputs\nOutputs\n\n\n\n\nCLIP text encoder\nPrompt\nEmbedding\n\n\nVAE encoder\nNoisy image\nNoisy image latents\n\n\nUnet\nNoisy image latents + Prompt embedding + Noise level\nNoise latents\n\n\nVAE decoder\nNoise latents\nNoise\n\n\n\nThat‚Äôs it for now! If you came this far, I hope you enjoyed it. For me, it helped a lot in my understanding by writing all this down. In the next blog post, we will have a look at how these concepts translate into code by making use of HuggingFace libraries such as diffusers and transformers"
  },
  {
    "objectID": "posts/07_stable_diffusion_code/index.html",
    "href": "posts/07_stable_diffusion_code/index.html",
    "title": "Introduction to Stable Diffusion - Code",
    "section": "",
    "text": "In the previous blog post, the main components and some intuition behind Stable Diffusion were introduced. Now, let‚Äôs see how we can use the HuggingFace diffusers library to generate images. The content of this blog post is based on Lesson 9 and Lesson 10 of Deep Learning for Coders. The end-to-end pipeline is very practical and easy to use, it‚Äôs basically a one-liner. We create a diffusion pipeline by downloading pre-trained models from a repo in the HuggingFace hub. Then, we can call this pipe object with a certain prompt:\n# pip install diffusers==0.12.1\n# pip install accelerate\n# pip install transformers==4.25.1\n\nfrom torchvision import transforms as tfms\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom PIL import Image\n\nnum_inference_steps = 50\nbatch_size = 1\n\npipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\").to(\"cuda\")\n\nprompt = \"Homer from the Simpsons on his roadbike climbing a mountain in the Pyrenees\"\n\n\n\n\nThe config attributes {'scaling_factor': 0.18215} were passed to AutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\ntorch.manual_seed(114)\npipe(prompt, num_inference_steps=num_inference_steps, guidance_scale=7.5).images[0]\nNot bad, but not great either. Let‚Äôs dive one layer deeper, and create the components described in the previous post: the Unet, the autoencoder, text encoder and noise scheduler:\nfrom diffusers import AutoencoderKL, LMSDiscreteScheduler, UNet2DConditionModel\nfrom transformers import CLIPTextModel, CLIPTokenizer, logging\n\nlogging.set_verbosity_error()\n\n# Autoencoder, to go from image -&gt; latents (encoder) and back (decoder)\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(\"cuda\")\n\n# UNet, to predict the noise (latents) from noisy image (latents)\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\").to(\"cuda\")\n\n# Tokenizer and Text encoder to create prompt embeddings\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(\"cuda\")\n\n# The noise scheduler\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\nscheduler.set_timesteps(num_inference_steps)\n\nThe config attributes {'scaling_factor': 0.18215} were passed to AutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\nTo use these components, we have to first tokenize the prompt. Tokenization is nothing more then transforming each word of the prompt into it‚Äôs associated integer according to a ‚Äúvocabulary‚Äù. The ‚Äúvocabulary‚Äù is the mapping of words to integers and is thus generally quite large.\ntext_input = tokenizer(prompt,               # the prompt we want to tokenize\n                       padding=\"max_length\", # pad the tokenized input to the max length\n                       return_tensors=\"pt\")  # return PyTorch tensors\ntext_input.input_ids\n\ntensor([[49406, 16931,   633,   518, 21092,   525,   787,  4370,  3701,  9877,\n           320,  3965,   530,   518, 39744, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407]])\nAbove we see the integers that are associated with each word in our prompt. We can decode the integers back into words and see if it matches our prompt. Let‚Äôs have a look at the first 5 tokens:\n[tokenizer.decode(token) for token in text_input.input_ids[0]][:5]\n\n['&lt;|startoftext|&gt;', 'homer', 'from', 'the', 'simpsons']\nWe see that all capital letters have been removed by the tokenization, and a special token is inserted at the beginning of the prompt. Also, we see the integer 49407 is being used to pad our input to the maximum length:\ntokenizer.decode(49407)\n\n'&lt;|endoftext|&gt;'\nNext, we will pass these tokens through the text-encoder to turn each token into an embedding vector. Since we have 77 tokens and the embeddings are of size 768, this will be a tensor of shape [77, 768].\ntext_embeddings = text_encoder(text_input.input_ids.to(\"cuda\"))[0]\ntext_embeddings.shape\n\ntorch.Size([1, 77, 768])\nWhen generating a completely new image, we start with a fully random noisy latent, so let‚Äôs create one:\ntorch.manual_seed(1024)\nlatents = torch.randn((batch_size,              # batch size: 1\n                       unet.config.in_channels, # input channels of the unet: 4\n                       unet.config.sample_size, # height dimension of the unet: 64\n                       unet.config.sample_size) # width dimension of the unet: 64\n                     ).to(\"cuda\")               # put the tensor on the GPU\n\nlatents = latents * scheduler.init_noise_sigma  # scale the noise\n\nlatents.shape\n\ntorch.Size([1, 4, 64, 64])\nThe latents thus carry 4 channels and are of size 64 by 64. Let‚Äôs pass this latent iteratively through the Unet, each time subtracting partly the amount of predicted noise (the output of the Unet)\nfor i, t in enumerate(scheduler.timesteps):\n    inputs = latents\n    inputs = scheduler.scale_model_input(inputs, t)\n\n    # predict the noise \n    with torch.no_grad(): \n        pred = unet(inputs, t, encoder_hidden_states=text_embeddings).sample\n\n    # update the latents by removing the predicted noise according to the noise schedule\n    latents = scheduler.step(pred, t, latents).prev_sample\nLet‚Äôs visualize the four channels of this latent representation in grey-scale:\nfig, axs = plt.subplots(1, 4, figsize=(16, 4))\nfor c in range(4):\n    axs[c].imshow(latents[0][c].cpu(), cmap='Greys')\nTo transform the latent representation to full-size images, we can use the decoder of the VAE. Note that when we do that, we move from a tensor of shape [4, 64, 64] to [3, 512, 512]:\nprint(latents.shape, vae.decode(latents).sample.shape)\n\ntorch.Size([1, 4, 64, 64]) torch.Size([1, 3, 512, 512])\nAnd let‚Äôs visualize the result:\n#scale back according to the VAE paper\nwith torch.no_grad(): image = vae.decode(1 / 0.18215 * latents).sample\n\n# move tensor to numpy\nimage = image[0].detach().cpu().permute(1, 2, 0).numpy()\n# scale the values to 0-255\nimage = ((image / 2 + 0.5).clip(0, 1) * 255).round().astype(\"uint8\")\nImage.fromarray(image)\nUnfortunately, the result looks very bad and especially much worse then our one-liner. The main reason for this, is that the StableDiffusionPipeline is using something called Classifier Free Diffusion Guidance. So let‚Äôs have a look at that. But before we do, let‚Äôs add two code snippets to transfrom from the latent representation to the full size image representation and back. We will do this a couple of times, so it helps to keep the code a bit cleaner:\ndef latents_to_image(latent):\n    with torch.no_grad(): \n        image = vae.decode(1 / 0.18215 * latent).sample\n\n    image = image[0].detach().cpu().permute(1, 2, 0).numpy()\n    image = ((image / 2 + 0.5).clip(0, 1) * 255).round().astype(\"uint8\")\n    return Image.fromarray(image)\n    \ndef image_to_latent(input_im):\n    with torch.no_grad():\n        latent = vae.encode(torch.Tensor(np.transpose(np.array(input_im) / 255., (2, 0, 1))).unsqueeze(0).to('cuda')*2-1)\n    return 0.18215 * (latent).latent_dist.sample()"
  },
  {
    "objectID": "posts/07_stable_diffusion_code/index.html#classifier-free-diffusion-guidance",
    "href": "posts/07_stable_diffusion_code/index.html#classifier-free-diffusion-guidance",
    "title": "Introduction to Stable Diffusion - Code",
    "section": "Classifier Free Diffusion Guidance",
    "text": "Classifier Free Diffusion Guidance\nClassifier Free Guidance refers to a technique in which two images are being constructed at the same time from the same latent. One of the images is being reconstructed based on the specified prompt (conditional generation), the other image is being generated by an empty prompt (unconditional generation). By mixing the two images in the process according to a parameter (called the guidance-scale) the generated image for the prompt is going to look much better:\n\ntorch.manual_seed(3016)\n\ncond_input = tokenizer(\"Homer from the Simpsons on his roadbike climbing a mountain in the Pyrenees\", padding=\"max_length\", return_tensors=\"pt\") \ncond_embeddings = text_encoder(cond_input.input_ids.to(\"cuda\"))[0]\n\n# Create embeddings for the unconditioned process\nuncond_input = tokenizer(\"\", padding=\"max_length\", return_tensors=\"pt\") \nuncond_embeddings = text_encoder(uncond_input.input_ids.to(\"cuda\"))[0]\n\n# Concatenate the embeddings\nembeddings = torch.cat([cond_embeddings, uncond_embeddings])\n\nguidance_scale = 7.5\n\n# Create a \"fresh\" random latent to start with\nlatents = torch.randn((batch_size, unet.config.in_channels, unet.config.sample_size, unet.config.sample_size)).to(\"cuda\")               \nlatents = latents * scheduler.init_noise_sigma\n\nfor i, t in enumerate(scheduler.timesteps):\n    inputs = torch.cat([latents, latents]) # concatenate the latents\n    inputs = scheduler.scale_model_input(inputs, t)\n\n    # predict the noise \n    with torch.no_grad(): \n        pred = unet(inputs, t, encoder_hidden_states=embeddings).sample\n    \n    # pull both images apart again\n    pred_cond, pred_uncond = pred.chunk(2)\n    \n    # mix the results according to the guidance scale parameter\n    pred = pred_uncond + guidance_scale * (pred_cond - pred_uncond)\n\n    # update the latents by removing the predicted noise according to the noise schedule\n    latents = scheduler.step(pred, t, latents).prev_sample\n    \nlatents_to_image(latents)\n\n\n\n\nMuch better! As you can see, Classifier Free Guidance is a simple technique but it works very well. This morning (03-07-2023) I saw a tweet that introduced the same concept to the world of Large Language Models (LLMs):"
  },
  {
    "objectID": "posts/07_stable_diffusion_code/index.html#negative-prompt",
    "href": "posts/07_stable_diffusion_code/index.html#negative-prompt",
    "title": "Introduction to Stable Diffusion - Code",
    "section": "Negative Prompt",
    "text": "Negative Prompt\nAs mentioned, the unconditional image with Classifier Free Guidance is created from an empty prompt. It turns out that we can use the prompt of this second image as a so-called negative prompt. If there are certain elements we don‚Äôt want to see in our image, we can specify it in this prompt.\nWe can see this by rewriting the Classifier Free Guidance equation:\n\\[\\begin{align}\np &= p_{uc} + g (p_{c} - p_{uc}) \\\\\np &= g p_{c} + (1 - g) p_{uc} \\\\\n\\end{align}\\]\nSo with a guidance scale value larger than 1, the unconditional prediction \\(p_{uc}\\) is being subtracted from the conditional prediction \\(p_c\\), which has the effect of removing the concept from the conditional image.\nAn example of Homer Simpson eating lunch:\n\ncond_input = tokenizer(\"Homer Simpson eating lunch\", padding=\"max_length\", return_tensors=\"pt\") \ncond_embeddings = text_encoder(cond_input.input_ids.to(\"cuda\"))[0]\n\nuncond_input = tokenizer(\"\", padding=\"max_length\", return_tensors=\"pt\") \nuncond_embeddings = text_encoder(uncond_input.input_ids.to(\"cuda\"))[0]\n\n\n\nCode\ntorch.manual_seed(105)\n\nembeddings = torch.cat([cond_embeddings, uncond_embeddings])\n\nguidance_scale = 7.5\n\nlatents = torch.randn((batch_size, unet.config.in_channels, unet.config.sample_size, unet.config.sample_size)).to(\"cuda\")               \nlatents = latents * scheduler.init_noise_sigma\n\nfor i, t in enumerate(scheduler.timesteps):\n    inputs = torch.cat([latents, latents]) # concatenate the latents\n    inputs = scheduler.scale_model_input(inputs, t)\n\n    with torch.no_grad(): \n        pred = unet(inputs, t, encoder_hidden_states=embeddings).sample\n    \n    pred_cond, pred_uncond = pred.chunk(2)\n    \n    pred = pred_uncond + guidance_scale * (pred_cond - pred_uncond)\n\n    latents = scheduler.step(pred, t, latents).prev_sample\n\nlatents_to_image(latents)\n\n\n\n\n\nAnd removing the blue chair by using a negative prompt:\n\nuncond_input = tokenizer(\"blue chair\", padding=\"max_length\", return_tensors=\"pt\") \nuncond_embeddings = text_encoder(uncond_input.input_ids.to(\"cuda\"))[0]\n\n\n\nCode\ntorch.manual_seed(105)\n\nembeddings = torch.cat([cond_embeddings, uncond_embeddings])\n\nguidance_scale = 7.5\n\nlatents = torch.randn((batch_size, unet.config.in_channels, unet.config.sample_size, unet.config.sample_size)).to(\"cuda\")               \nlatents = latents * scheduler.init_noise_sigma\n\nfor i, t in enumerate(scheduler.timesteps):\n    inputs = torch.cat([latents, latents]) # concatenate the latents\n    inputs = scheduler.scale_model_input(inputs, t)\n\n    with torch.no_grad(): \n        pred = unet(inputs, t, encoder_hidden_states=embeddings).sample\n    \n    pred_cond, pred_uncond = pred.chunk(2)\n    \n    pred = pred_uncond + guidance_scale * (pred_cond - pred_uncond)\n\n    latents = scheduler.step(pred, t, latents).prev_sample\n\nimage = latents_to_image(latents)\nimage\n\n\n\n\n\nAnd gone is the blue chair! I must admit that this doesn‚Äôt always work as great as in this example, in fact I had to try out quite a lot of prompts in combination with negative prompts to find a good example for this post.."
  },
  {
    "objectID": "posts/07_stable_diffusion_code/index.html#image-to-image-generation",
    "href": "posts/07_stable_diffusion_code/index.html#image-to-image-generation",
    "title": "Introduction to Stable Diffusion - Code",
    "section": "Image-to-image generation",
    "text": "Image-to-image generation\nImage-to-image generation is another super interesting process, in which we use both a prompt and an image to guide the generation process. This comes in handy, if for example we want to create a variant of an image we already have. Let‚Äôs say we have an awesome image of Homer eating a burger, and we want to have a similar image but instead we want Marge to eat the burger, or we want Homer to eat a slice of pizza instead. We can then feed both the correct promt as well as the already existing image to guide the image generation process even more.\nThe way this works, is by not starting with a completely random latent, but instead build a noisy latent of our existing image.\nLet‚Äôs start with the image above and add some noise to it, for example by adding the noise for level 15 (we have 50 noise levels in total, so level 15 means that we still have 35 denoising steps to go):\n\nlatent = image_to_latent(image)\nnoise_latent = torch.randn_like(latent)\n\nsampling_step = 15\nnoised_latent = scheduler.add_noise(latent, noise_latent, timesteps=torch.tensor([scheduler.timesteps[sampling_step]]))\n\nlatents_to_pil(noised_latent)[0]\n\n\n\n\nIf you squint with your eyes you can already see some structure, in the middle there is some yellow blob sitting around (eating lunch..). Let‚Äôs take this noisy latent, and do the remaining 35 denoising steps with a different prompt:\n\ntorch.manual_seed(105)\n\ncond_input = tokenizer(\"Homer Simpson eating Hot Pot\", padding=\"max_length\", return_tensors=\"pt\") \ncond_embeddings = text_encoder(cond_input.input_ids.to(\"cuda\"))[0]\n\nuncond_input = tokenizer(\"\", padding=\"max_length\", return_tensors=\"pt\") \nuncond_embeddings = text_encoder(uncond_input.input_ids.to(\"cuda\"))[0]\n\nembeddings = torch.cat([cond_embeddings, uncond_embeddings])\n\nguidance_scale = 7.5\n\n# We start with the noised_latent coming from the image, defined above\nlatents = noised_latent\n\nfor i, t in enumerate(scheduler.timesteps):\n    # we only do the steps starting from the specified level\n    if i &gt;= sampling_step:\n        \n        inputs = torch.cat([latents, latents])\n        inputs = scheduler.scale_model_input(inputs, t)\n\n        with torch.no_grad(): \n            pred = unet(inputs, t, encoder_hidden_states=embeddings).sample\n\n        pred_cond, pred_uncond = pred.chunk(2)\n        pred = pred_uncond + guidance_scale * (pred_cond - pred_uncond)\n\n        latents = scheduler.step(pred, t, latents).prev_sample\n    \nlatents_to_image(latents)\n\n\n\n\nThis image is very similar to what we started with. Color scheme, composition and camera angle are all the same. At the same time, the prompt is also reflected by a change of dishes on the table.\nAnd that‚Äôs it, that‚Äôs image-to-image generation. As you can see, it‚Äôs nothing deeply complicated, it‚Äôs just a smart way to re-use the components we have already seen.\nI hope this blog post shows how the components that are introduced in the previous post, translate to code. The examples shown here, only touch upon what can be achieved. In fact, the lessons upon which this post is based show a lot more interesting concepts such as textual inversion. If you are interested, have a look here"
  },
  {
    "objectID": "posts/04_matmul/index.html",
    "href": "posts/04_matmul/index.html",
    "title": "Fast matrix multiplications",
    "section": "",
    "text": "Matrix multiplications are kind of boring, so why write a blog post about them? Well, matrix multiplications are the most basic computation that is being performed by neural networks. So it‚Äôs probably good to be familiar with them, although we never do them by hand. Also, we are going to focus on speeding them up by doing vectorization. Vectorization is something we often have to do, to make sure everything runs as quickly as possible, and it‚Äôs thus a good exercise to understand how to achieve this. Especially since it involves being very familiar with matrices, their shapes, broadcasting operations and the like.\nThis post follows the first lecture of Part 2 of the FastAI course (2019), I will provide some additional explanations, and present one other optimization that is not presented in the lecture."
  },
  {
    "objectID": "posts/04_matmul/index.html#definition",
    "href": "posts/04_matmul/index.html#definition",
    "title": "Fast matrix multiplications",
    "section": "Definition",
    "text": "Definition\nMatrix multiplication is not difficult, it goes like this:\n\nFor matrix A of size [ar x ac] ¬† ([4 x 3] in the image below)\nand matrix B of size [br x bc] ¬† ([3 x 2] in the image below)\nthe matrix product A * B is of size [ar x bc] ([4 x 2] in the image below).\nSo the matrix product is thus only defined when ac == br (3 == 3 in the image below)\n\n\n\n\n\n\nSo for any valid matrix multiplication, we have three dimensions that need to considered:\n\nar: the row dimension of matrix A. The size of this dimension will become the size of the row dimension of the output matrix (black arrow in the image above)\nbc: the column dimension of matrix B. The size of this dimension will become the size of the column dimension of the output matrix (purple arrow in the image above)\nac: the column dimension of Matrix A and br: the row dimension of matrix B: they need to be equal (red arrow in the image above)\n\nWhy do ac and bc need to be equal? Well, because we take the inner product over this dimension when computing the cell values of the new matrix, and inner-products are only defined for vectors of equal length. Below, I will also refer to this dimension as the dimension over which we collapse (or the ‚Äúcollapsible‚Äù dimension), since in the output matrix, this dimension is no longer present.\n\n\n\n\n\nIn other words, to compute cell \\(C_{i,j}\\) we take the inner product between row i of matrix A and column j of matrix B. Let‚Äôs have a look at one other cell, to make sure we understand fully what‚Äôs going on. In the next figure we compute the value for cell \\(C_{3,2}\\), we thus take the inner-product between row 3 of matrix A and column 2 of matrix B:\n\n\n\n\n\nLet‚Äôs do this in code, to confirm these statements:\n\nimport torch\n\na = torch.randn(4,3)\nb = torch.randn(3,2)\n\n\n# Confirm the shape of the output matrix\n(a@b).shape\n\ntorch.Size([4, 2])\n\n\n\n# Confirm the value of one output cell (C00)\nC00_manual = (a[0,0] * b[0,0]) + (a[0,1] * b[1,0]) + (a[0,2] * b[2,0])\nC00_auto = (a@b)[0,0]\n\nassert torch.allclose(C00_manual, C00_auto)\n\nNow, let‚Äôs create our own matrix multiplication function:\n\ndef matmul(a, b):\n    # fill in the sizes of the dimensions\n    ar, ac = a.shape\n    br, bc = b.shape\n\n    # assert that our matrices can be multiplied \n    assert ac == br\n\n    # create an output tensor of the expected size (ar x bc)\n    out = torch.zeros(ar, bc)\n\n    # iterate over the rows of the output matrix (--&gt; length ar)\n    for i in range(out.shape[0]):\n        # iterate over the columns of the output matrix (--&gt; length bc)\n        for j in range(out.shape[1]):\n            # iterate over the \"collapsed\" dimension (--&gt; length ac and length br), \n            for k in range(ac):\n                out[i, j] += a[i, k] * b[k, j]\n    \n    return out\n\n# Confirm that the result is correct\nassert torch.allclose(matmul(a,b), a@b)\n\nAlthough this is functionally correct, it‚Äôs not very efficient. In fact, to compute the value of one cell of the output matrix, we are doing three separate multiplications. In other words, for each cell out[i,j] we are calling three times (once for every value of k):\nout[i, j] += a[i, k] * b[k, j]\nLet‚Äôs try to reduce the computation of one cell to just one single call."
  },
  {
    "objectID": "posts/04_matmul/index.html#first-improvement",
    "href": "posts/04_matmul/index.html#first-improvement",
    "title": "Fast matrix multiplications",
    "section": "First improvement",
    "text": "First improvement\nTo do so, we need to get rid of the loop over the ‚Äúcollapsible‚Äù dimension k. We can simply do this by replacing the k with a :, so that we select the whole dimension instead of just one element in that dimension. The multiplication (*) is doing an element wise multiplication, so we have to wrap the result with a .sum().\n\ndef matmul2(a, b):\n    ar, ac = a.shape\n    br, bc = b.shape\n    assert ac == br\n\n    out = torch.zeros(ar,bc)\n    # iterate over the rows of the output matrix (i)\n    for i in range(out.shape[0]):\n        # iterate over the columns of the output matrix (j)\n        for j in range(out.shape[1]):\n            out[i, j] = (a[i, :] * b[:, j]).sum()\n    \n    return out\n\n# Confirm that the result is correct\nassert torch.allclose(matmul(a,b), a@b)"
  },
  {
    "objectID": "posts/04_matmul/index.html#second-improvement",
    "href": "posts/04_matmul/index.html#second-improvement",
    "title": "Fast matrix multiplications",
    "section": "Second improvement",
    "text": "Second improvement\nThe improvement above, gives us the value of a cell in one single call:\nout[i, j] = (a[i, :] * b[:, j]).sum()\nThis is great, let‚Äôs try to vectorize this even further, and get rid of the second loop (the loop over j), this means that we need to compute the values of a single row of the output matrix in one call, e.g.\nout[i,:] = ...\nWe know that the value of cell \\(C_{ij}\\) is the inner product between row i of A and column j of B. We also know that any row of matrix C will have two values. Let‚Äôs compute them manually:\n\nout_00 = (a[0,:] * b[:,0]).sum()\nout_01 = (a[0,:] * b[:,1]).sum()\n\nC0_manual = torch.stack([out_00, out_01])\nC0_auto = (a@b)[0]\n\nassert torch.allclose(C0_manual, C0_auto)\n\nprint(f'{out_00=}', f'{out_01=}', f'{C0_manual=}', sep='\\n')\n\nout_00=tensor(-0.0213)\nout_01=tensor(0.3668)\nC0_manual=tensor([-0.0213,  0.3668])\n\n\nObserve that for the computation of one row of output, we need:\n\none single row of A (a[0,:])\nthe full matrix of B, we need both the first (b[:,0]) column and the second column (b[:,1]).\n\nLet‚Äôs check the sizes of both and see whether we can use broadcasting:\n\nprint(f'{a[0,:].shape=}', f'{b.shape=}', sep='\\n')\n\na[0,:].shape=torch.Size([3])\nb.shape=torch.Size([3, 2])\n\n\nUnfortunately, size [3] and [3,2] don‚Äôt broadcast. To make them broadcast, we have to add an empty dimension at the end of the row of the A matrix. Then, the shapes [3, 1] and [3, 2] can be broadcasted to another by duplicating the former in the column direction:\n\nt = a[0,:].unsqueeze(-1) # [3, 1]\n\nt.broadcast_to(b.shape) # [3, 2]\n\ntensor([[ 0.9193,  0.9193],\n        [-0.0426, -0.0426],\n        [ 1.3566,  1.3566]])\n\n\nNow that both object are the same size we can do an element-wise multiplication and then sum over the rows to arrive at an output of size [1,2]:\n\nC0_manual = (t*b).sum(dim=0)\nC0_auto = (a@b)[0,:]\n\nassert torch.allclose(C0_manual, C0_auto)\n\nprint(f'{C0_manual=}', f'{C0_manual.shape=}', sep='\\n')\n\nC0_manual=tensor([-0.0213,  0.3668])\nC0_manual.shape=torch.Size([2])\n\n\nSo let‚Äôs implement this:\n\ndef matmul3(a, b):\n    ar, ac = a.shape\n    br, bc = b.shape\n    assert ac == br\n\n    out = torch.zeros(ar,bc)\n    # iterate over the rows of the output matrix (i)\n    for i in range(out.shape[0]):\n        out[i, :] = (a[i, :].unsqueeze(-1) * b).sum(dim=0)\n    \n    return out\n\n# Confirm that the result is correct\nassert torch.allclose(matmul(a,b), a@b)"
  },
  {
    "objectID": "posts/04_matmul/index.html#third-improvement",
    "href": "posts/04_matmul/index.html#third-improvement",
    "title": "Fast matrix multiplications",
    "section": "Third improvement",
    "text": "Third improvement\nFor the final improvement, we need to get rid of the only remaining loop over the rows of our output matrix (i). So let‚Äôs understand very well what we are having at the moment:\n\nWe are iterating over the (4) rows of our output matrix\nFor each row, we are computing the (2) values of our row at once by doing out[i, :] = (a[i, :].unsqueeze(-1) * b).sum(dim=0) and let‚Äôs break this down once again in steps:\n\na[i, :] has shape [3] and represents one row of A\nwith a[i, :].unsqueeze(-1) we add an extra dimension so that we can broadcast, the result has shape [3, 1]\nb has shape [3, 2] and is the full B matrix\nelement-wise multiplication of 2. and 3. gives a matrix of shape [3, 2]\nby summing over the rows (.sum(dim=0)) we arrive at the result of shape [2]\n\n\nWe want to improve this by instead of iterating over the 4 rows, do these computations all at once for all rows. So let‚Äôs start by not selecting one row of A (a[i,:]) but instead just the whole a matrix:\n\na has shape [4, 3]\nsimilarly to what we did before, we can a.unsqueeze(-1) to add an extra dimension, the result has shape [4, 3, 1]\nsame as before, b has shape [3, 2] and is the full B matrix\nbroadcasting of 2. and 3. will do the following:\n\na.unsqueeze(-1) has shape [4, 3, 1] and get‚Äôs expanded to [4, 3, 2] to match the shape of b ([3, 2])\nbut b also needs to match a, first an additional empty dimension is added in the front: [1, 3, 2] and then it get‚Äôs expanded to [4, 3, 2]\nThus, the element-wise multiplication of 2. and 3. gives a matrix (tensor) of shape [4, 3, 2], let‚Äôs call it t. It‚Äôs import to realize what this t represents. For that, notice that the first dimension (length 4) and last dimension (length 2) are the dimensions of our output matrix ([4, 2]). The middle dimension (length 3) represents the element wise multiplications of row i in matrix A and column j of matrix B. So t[0, :, 0] gives us 3 numbers which represent the element-wise multiplications of the first row of A and the first column of B.\n\nSo to finalize and get to the final matrix product A*B, we collapse (sum) over this middle dimension:\n\n\ndef matmul4(a, b):\n    return (a.unsqueeze(-1) * b).sum(dim=1)\n\n# Confirm that the result is correct\nassert torch.allclose(matmul(a,b), a@b)"
  },
  {
    "objectID": "posts/04_matmul/index.html#timings",
    "href": "posts/04_matmul/index.html#timings",
    "title": "Fast matrix multiplications",
    "section": "Timings",
    "text": "Timings\nTo see what kind of a speed-up we have achieved, let‚Äôs look at the timings of our first version with three loops and the timings of our optimized version:\n\n%timeit -n 1000 matmul(a,b)\n\n318 ¬µs ¬± 13.7 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n%timeit -n 1000 matmul4(a,b)\n\n10.7 ¬µs ¬± 1.46 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n\n\nNice, our optimized version is about 30 times faster then our un-optimized version with 3 loops! Additionally, let‚Äôs check the timings of doing the matrix multiplication with einsum:\n\n%timeit -n 1000 torch.einsum('ij,jk-&gt;ik', a, b)\n\n25.9 ¬µs ¬± 3.78 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n\n\nSurprisingly, our optimized version is twice as fast as einsum. This is certainly something I didn‚Äôt expect.\nFinally, let‚Äôs also check the timings of using the @ operator:\n\n%timeit -n 1000 a@b\n\n3.29 ¬µs ¬± 522 ns per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n\n\nAs expected, this is even faster then our optimized version, probably because it runs in optimized C / CUDA code"
  },
  {
    "objectID": "posts/01_blog_setup/index.html",
    "href": "posts/01_blog_setup/index.html",
    "title": "Blog setup",
    "section": "",
    "text": "In this blog post I‚Äôll explain how I created this blog, using Quarto and GitHub. In step 4 I‚Äôll show how to setup GitHub Actions, this has advantages over the other ways to publish our blog:\nI‚Äôm working on a Macbook, and using VS Code for code editing. If you are on a Linux or Windows machine, be aware that things might be a bit different from what I describe here.\nI am assuming you already have a GitHub account, that VS Code is installed and configured to run Python and Jupyter Notebooks."
  },
  {
    "objectID": "posts/01_blog_setup/index.html#step-1-install-quarto",
    "href": "posts/01_blog_setup/index.html#step-1-install-quarto",
    "title": "Blog setup",
    "section": "Step 1: install Quarto",
    "text": "Step 1: install Quarto\nFirst of all you need to install Quarto, go here, download and install the software. You should do this on the machine that you want to use for writing your blog, in my case my Macbook laptop.\nOnce installed you will have access to the quarto Command Line Interface (CLI). To make sure everything works as expected, open a terminal and execute:\n\n\nTerminal\n\nquarto --help\n\nThis should render some outputs describing the different commands and options that are part of the Quarto CLI and shows that Quarto is installed successfully."
  },
  {
    "objectID": "posts/01_blog_setup/index.html#step-2-create-a-github-repo",
    "href": "posts/01_blog_setup/index.html#step-2-create-a-github-repo",
    "title": "Blog setup",
    "section": "Step 2: create a GitHub repo",
    "text": "Step 2: create a GitHub repo\nTo host our blog we will use GitHub Pages, which is a service to host a website from a GitHub repository. Based on the name you pick for your repository you will create a so-called project-website or your unique user-website. For any general repo named my-awesome-repo, the website will be hosted on https://&lt;github-username&gt;.github.io/my-awesome-repo. This is a project-websites and you can create as many as you like.\nTo create your user-website, you have to name the repo exactly like this: &lt;github-username&gt;.github.io, the user-website will be hosted at https://&lt;github-username&gt;.github.io.\nThis is exactly what I want, so I create a new repo with the name: lucasvw.github.io.\nI find it helpful to add a .gitignore file with a Python template, to which we can later add some more entries to facilitate storing the right files on GitHub. Also make sure that the repo is Public (and not set to Private). Additionally, I added a README file and choose the Apache2 License.\nNext, I clone this repo to my machine by running:\n\n\nTerminal\n\ngit clone git@github.com:lucasvw/lucasvw.github.io.git"
  },
  {
    "objectID": "posts/01_blog_setup/index.html#step-3-add-a-quarto-project-to-the-repo",
    "href": "posts/01_blog_setup/index.html#step-3-add-a-quarto-project-to-the-repo",
    "title": "Blog setup",
    "section": "Step 3: add a Quarto project to the repo",
    "text": "Step 3: add a Quarto project to the repo\nNext, open VS Code and open the cloned repo. Then access the VS Code terminal and run:\n\n\nTerminal\n\nquarto create-project --type website:blog\n\nThis will add a number of files to our repo, which represent the basic structure of our blog. Most importantly:\n\nposts: here we will create our blog entries (one subfolder per blog entry)\n_quarto.yml: configuration file for our blog such as the theme, name, GitHub and Twitter links\nabout.qmd: source code for the ‚Äúabout‚Äù page.\nindex.qmd: source code for the landing page.\n\n\n\n\n\n\n\nNote\n\n\n\n.qmd files are like markdown files, but with lots of additional functionality from Quarto. Go here for more information on Markdown syntax and here for Quarto Markdown\n\n\nTo see what we currently have, let‚Äôs render our blog locally:\n\n\nTerminal\n\nquarto preview\n\nAlternatively, we can install the Quarto extension in VS Code, which will show a render button in the top right corner on any opened qmd file.\nTo publish the current contents to GitHub pages, we can run:\n\n\nTerminal\n\nquarto publish gh-pages\n\nWhen doing so, we get a message that we have to change the branch from which GitHub Pages builds the site. To do this, I go to https://github.com/lucasvw/lucasvw.github.io/settings/pages and select gh-pages instead of the main branch.\nAnd voila, in a few moments our blog will be running live at https://lucasvw.github.io/"
  },
  {
    "objectID": "posts/01_blog_setup/index.html#step-4-finalize-set-up-github-actions",
    "href": "posts/01_blog_setup/index.html#step-4-finalize-set-up-github-actions",
    "title": "Blog setup",
    "section": "Step 4: Finalize set-up: GitHub Actions",
    "text": "Step 4: Finalize set-up: GitHub Actions\nWhen we run the quarto publish gh-pages command, Quarto processes our files and turns them into web readable files (HTML, JS, CSS etc). It stores these files in our gh-pages branch and pushes them to our remote GitHub repo. This is great, but it means that this doesn‚Äôt store our source files.\nTo do so, let‚Äôs first open our .gitignore file and make sure that it contains the following entries so that we don‚Äôt check in any files we don‚Äôt need.\n\n\n.gitignore\n\n# Quarto\n/.quarto/\n_site/\n\n# Mac files\n.DS_Store\n\nNext, we can commit all the remaining files to Git and push them to our remote repo. If we ever lose access to our local machine, we can restore everything we need from GitHub.\nHowever, now we have 2 things we need to do whenever we finish our work:\n\nstore our source files on the main branch and push to GitHub\nrun the publish command to update the blog\n\nThis is a bit annoying and it would be much better if we can just push to the main branch and GitHub would take care of building our website and updating it. This also allows us to create blog entries on any machine that has access to git, we don‚Äôt need to have quarto installed. This is particularly practical if we want to write blog entries from our deep learning server. So let‚Äôs use GitHub actions for this.\n\n\n\n\n\n\nNote\n\n\n\nBefore you continue make sure you have at least once run a quarto publish gh-pages command, this is necessary for the things below to work\n\n\nFirst we need to add the following snippet to _quarto.yml\n\n\n_quarto.yml\n\nexecute:\n  freeze: auto\n\nThis will make sure that GitHub actions doesn‚Äôt execute any executable code, but will show the pre-rendered outputs it finds in the _freeze folder.\nFinally, create the file .github/workflows/publish.yml and populate it with the following code:\n\n\n.github/workflows/publish.yml\n\non:\n  workflow_dispatch:\n  push:\n    branches: main\n\nname: Quarto Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v3\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n\n      - name: Render and Publish\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\nOnce we push these things to GitHub, we are good to go. Whenever we push anything to the main branch, this workflow will execute and take care of updating the gh-pages branch and updating the blog."
  },
  {
    "objectID": "posts/13_ngram/index.html",
    "href": "posts/13_ngram/index.html",
    "title": "N-gram language models",
    "section": "",
    "text": "In this post, I‚Äôll discuss a very simple language model: n-grams! To keep things simple we will be concerned with single words, and not for example complete sentences. The goal will be to have a language model that learns from a corpus of (first-)names, and is able to create new name-sounding names. The idea as well as the dataset comes from Andrej Karpahy‚Äôs lecture series on Neural Networks.\nLike most (any?) language model, n-gram models can be used to predict the next token in a sequence. The n in n-gram relates to the length of the used context. Bi-gram models only use the previous token as context, tri-gram models use the last two tokens to predict the following token.\nLet‚Äôs clarify what we mean with prediction of the next token: an n-gram model learns a probability distribution over any of the tokens that can follow from any of the possible contexts. A bi-gram character-level model has thus a learned probability distribution over all the characters (a through z, and a token to denote the end of a word, so 26+1 tokens) that can follow from any character.\nThe simplest possible way to learn a probability distribution from training data, is by simply keeping track of the statistics in the training corpus:\nIf we then normalize these counts (by dividing through the sum) we have a probability distribution!"
  },
  {
    "objectID": "posts/13_ngram/index.html#data",
    "href": "posts/13_ngram/index.html#data",
    "title": "N-gram language models",
    "section": "Data",
    "text": "Data\nEverything starts with training data:\n\nimport random\nfrom functools import reduce\nfrom pathlib import Path\nfrom urllib.request import urlretrieve\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport torcheval.metrics as tem\nimport fastcore.all as fc\n\nfrom nntrain.dataloaders import DataLoaders\nfrom nntrain.learner import *\nfrom nntrain.activations import *\n\n\npath = Path('./data')\npath.mkdir(parents=True, exist_ok=True)\npath = path / 'names.txt'\nurl = 'https://raw.githubusercontent.com/karpathy/makemore/master/names.txt'\n\n_ = urlretrieve(url, path)\n\nwith open(path, 'r') as f:\n    lines = f.read().splitlines()\n    \nrandom.seed(42)\nrandom.shuffle(lines)\n\n\nprint('first 5 names in array: ', end=' ')\nprint(lines[0:5])\nprint(f'{len(lines)=}')\n\nfirst 5 names in array:  ['yuheng', 'diondre', 'xavien', 'jori', 'juanluis']\nlen(lines)=32033"
  },
  {
    "objectID": "posts/13_ngram/index.html#bi-gram-count-model",
    "href": "posts/13_ngram/index.html#bi-gram-count-model",
    "title": "N-gram language models",
    "section": "Bi-gram count model",
    "text": "Bi-gram count model\nFrom the data, we first create the vocabulary:\n\nunique_chars = list(set(\"\".join(lines)))\nunique_chars.sort()\nvocabulary = unique_chars + ['.']\nprint(f'{vocabulary=}')\nprint(f'{len(vocabulary)=}')\n\nvocabulary=['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '.']\nlen(vocabulary)=27\n\n\nFrom the vocabulary we create a mapping from token to integer. We will use this mapping to encode our tokens (characters) into integers. This is called numericalisation:\n\nc2i = {c:i for i, c in enumerate(vocabulary)}\ni2c = {i:c for i, c in enumerate(vocabulary)}\n\nAnd just for clarity, let‚Äôs have a look at the bi-grams for the first name in the dataset:\n\n\nCode\ncutoff = round(len(lines) * 0.8)\ntrain_lines = lines[:cutoff]\nval_lines = lines[cutoff:]\n\nname = \".\" + train_lines[0] + \".\"\nprint(f'name : \"{train_lines[0]}\"')\nprint(f'name with start and end token : {name}')\nprint('bigrams:')\nbigrams = zip(name[:-1], name[1:])\nfor bigram in bigrams:\n    context = bigram[0]\n    next_char = bigram[1]\n    print(f'{context} --&gt; {next_char} \\t numericalized: {c2i[context]:&gt;2} --&gt; {c2i[next_char]:&gt;2}')\n\n\nname : \"yuheng\"\nname with start and end token : .yuheng.\nbigrams:\n. --&gt; y      numericalized: 26 --&gt; 24\ny --&gt; u      numericalized: 24 --&gt; 20\nu --&gt; h      numericalized: 20 --&gt;  7\nh --&gt; e      numericalized:  7 --&gt;  4\ne --&gt; n      numericalized:  4 --&gt; 13\nn --&gt; g      numericalized: 13 --&gt;  6\ng --&gt; .      numericalized:  6 --&gt; 26\n\n\nTo keep track of the occurrences we will use a matrix of size \\([27, 27]\\) (the size of the vocabulary). The rows reflect the possible values of the context and the columns reflect any possible character following this context. We will initialize this matrix with zeros, and increase the value of the associated cell by one, every time we encounter an n-gram:\n\ncounts = torch.zeros([27,27], dtype=int)              # initialize counts matrix\nfor name in train_lines:                              # iterate through all the names\n    name = \".\" + name + \".\"                           # add start and end token to the name\n    bigrams = zip(name[:-1], name[1:])                # create all bi-grams for the name (list of tuples)\n    for bigram in bigrams:                            # iterate through bi-grams\n        counts[c2i[bigram[0]], c2i[bigram[1]]] +=1    # increase the counts in the matrix for the encountered bi-gram\n\nThe count matrix is visualized below. On the vertical axis (the rows of the matrix) we have the context characters and on the horizontal axis (the columns of the matrix) the characters that follow from this context are shown. Bright (white) colors represent low counts, and dark blue colors represent high counts. The first observation is that the matrix is actually quite sparse. Many bi-grams appear very few times, and a couple of b-grams occur very frequently. We further observe:\n\nLooking at the first row: an ‚Äúa‚Äù is followed very often either by an ‚Äún‚Äù or even more often by a ‚Äú.‚Äù. Names often end with an ‚Äúa‚Äù!\nLooking at the last row: names also start pretty often with an ‚Äúa‚Äù\n\n\n\nCode\nfig, ax = plt.subplots(figsize=(10,10))\n\nax.imshow(counts, cmap='Blues');\nax.set_xticks(list(range(0,27)), list(c2i.keys()));\nax.set_yticks(list(range(0,27)), list(c2i.keys()));\nax.tick_params(axis='both', which='major', labelsize=20);\n\n\n\n\n\nLet‚Äôs use this language model to generate some new names:\n\ng = torch.Generator().manual_seed(42)\n\n# Normalize over the rows\nprobs = counts / counts.sum(dim=1, keepdim=True)\n\nnames = []\nfor i in range(10):                           # generate 20 names\n    name = '.'                                # initialize each name with a starting token\n    while True:                               # loop until break\n        ctx = c2i[name[-1]]                   # index of last character\n        pdf = probs[ctx]                      # pdf over characters that follow from last character\n        s = torch.multinomial(pdf, 1, generator=g) # draw a sample given the pdf\n        c = i2c[s.item()]                     # transform to character\n        name += c                             # append new character to name\n        if c == '.':                          # stop sampling upon reaching an end of word token\n            names.append(name)                # store away sampled name\n            break\n\nnames\n\n['.akyleloniatanacaro.',\n '.eynn.',\n '.con.',\n '.thmarie.',\n '.di.',\n '.n.',\n '.kahannoa.',\n '.jadde.',\n '.ethann.',\n '.dalaydah.']\n\n\nHmm, not super great names.. I guess it‚Äôs not surprising given the simplicity of this model.\nHow could we improve on this result? The most obvious way, would be to increase the context length (3-grams, 4-grams etc). This way, the model has more knowledge of what previously occurred, and can possibly create better predictions for the next character. This will probably work fine for n=3 and 4, but with even larger values of n, we will run into problems. The reason for this in short is that the counts matrix is going to be extremely sparse as the amount of rows (= the amount of possible contexts) increases exponentially: For a bi-gram, we have a context of just one token, so 27 possible values. For a tri-gram we have 2 tokens: that‚Äôs already \\(27^2\\) possible values.\nWe thus have to come up with another strategy to increase the performance of our model, for which we turn to neural networksüòè"
  },
  {
    "objectID": "posts/13_ngram/index.html#neural-network",
    "href": "posts/13_ngram/index.html#neural-network",
    "title": "N-gram language models",
    "section": "Neural network",
    "text": "Neural network\nIt turns out that the count model described above, has an equivalence to a very simple neural network that‚Äôs composed of an embedding layer and uses cross-entropy loss.\n\n\n\n\n\n\nNote\n\n\n\nAn embedding layer is a layer that is used to ‚Äúencode‚Äù our data, which roughly translates to the way we input our data into a neural network. We already saw above that we numericalized our characters (a-z) as integers. We will have to do something similar for our neural network, since we can‚Äôt input characters into a neural network. However, instead of feeding integers into our network, we can also employ an embedding layer, which will basically create a vector out of our integers. Each integer value will be mapped to it‚Äôs own vector, and the values contained in this vector will be learned during training. The advantage of this, is that the model can easily learn different ‚Äúattributes‚Äù that make up the individual tokens. For example, it could use the first dimension in the vector to denote whether the token is a vowel (a, e, i, o, u) and the second dimension to represent the likelihood of starting a sentence. The emphasis in the last sentence is on could, since these things have to be learned by the network itself during the training-process.\n\n\nIf we create an embedding layer with embedding dimensions equal to that of the vocabulary, the outputs of this layer will also conform to the size of our vocabulary, e.g.¬†[batch_size * 26]. These raw outputs of our model are referred to as logits, and they can be anything: positive, negative, small or large. We can exponentiate these logits to get numbers that are always positive, and these values are equivalent to values in the ‚Äúcounts‚Äù matrix. We can then normalize these exponentiated logits row-wise, to get to probabilities. And finally adding a negative log-likelihood loss on these probabilities is guiding the network to establish an embedding matrix with weights that are practically identical to the (log transformed) counts matrix from above. The combined operation of exponentiating, normalizing and negative log-likelihood is what we call cross-entropy loss. See also an earlier blog post.\nWe are going to train this model using nntrain, the small neural network training library we have been created before. But before we do, we have to create the datasets and dataloaders. Let‚Äôs make the dataset generic so that it can create datasets for any n-gram model we wish. As a reminder, let‚Äôs have a quick look how 2-grams, 3-grams and 4-grams look for the first name in the data Emma:\n\n\nCode\nname = lines[0]\nname = '.' + name + '.'\nprint('2-grams:')\nfor i in zip(name[:-1], name[1:]):\n    print(i[0], '---&gt;', i[1])\n\n\n2-grams:\n. ---&gt; y\ny ---&gt; u\nu ---&gt; h\nh ---&gt; e\ne ---&gt; n\nn ---&gt; g\ng ---&gt; .\n\n\n\n\nCode\nname = lines[0]\nname = '..' + name + '.'\nprint('3-grams:')\nfor i in zip(name[:-2], name[1:-1], name[2:]):\n    print(i[0], i[1], '---&gt;', i[2])\n\n\n3-grams:\n. . ---&gt; y\n. y ---&gt; u\ny u ---&gt; h\nu h ---&gt; e\nh e ---&gt; n\ne n ---&gt; g\nn g ---&gt; .\n\n\n\n\nCode\nname = lines[0]\nname = '...' + name + '.'\nprint('4-grams:')\nfor i in zip(name[:-3], name[1:-2], name[2:-1], name[3:]):\n    print(i[0], i[1], i[2], '---&gt;', i[3])\n\n\n4-grams:\n. . . ---&gt; y\n. . y ---&gt; u\n. y u ---&gt; h\ny u h ---&gt; e\nu h e ---&gt; n\nh e n ---&gt; g\ne n g ---&gt; .\n\n\nWe observe:\n\nirrespective of n, we always end up with the same amount of samples\nwhen we increase the context to n=3 and n=4, we need to add additional start-word tokens to make sure we don‚Äôt skip the first sample(s)\n\nLet‚Äôs create a dataset in which we can set the value of n:\n\nclass NgramDataset():\n    def __init__(self, lines, n=2):\n        self.x = []    # store the xs, the context, the left hand side of the n-gram\n        self.y = []    # store the ys, the labels, the right hand side of the n-gram\n        for line in lines:           \n            x = '.'*(n-1)         # the first x is always full of \"start word tokens\"\n            for y in line + '.':  # the first y is always the first letter of the name\n                self.x.append([c2i[xi] for xi in x])  # convert to int and store\n                self.y.append(c2i[y])                 # convert to int and store\n                x = (x + y)[1:]                       # update x\n        self.x = torch.tensor(self.x)                 # from list to tensor \n        self.y = torch.tensor(self.y).squeeze()       # from list to tensor\n    \n    def __getitem__(self, i):\n        return self.x[i], self.y[i]\n\n    def __len__(self):\n        return len(self.x)\n\nNext, let‚Äôs create the model, which should also be able to deal with n-grams. Instead of using PyTorch‚Äôs nn.Embedding layer, we will use a custom weight tensor which will act as the weights of our embedding layer. The reason for this, is that nn.Embedding is only two dimensional. For an n-gram of n &gt; 2, we would thus have to stack-up all the possible character combinations of the context in the row dimension. This is a bit tedious to implement, so instead we will use an explicit n-dimensional weight tensor. For the trigram this means: the first (second) dimension is for the first (second) character in the context, and the last dimension is for the label.\n\nclass NgramNet(nn.Module):\n    def __init__(self, n=2):\n        super().__init__()\n        self.n = n\n        self.embedding = nn.Parameter(torch.randn((27,)*n).requires_grad_())\n        \n    def forward(self, x):\n        # logits are obtained by indexing into the embedding matrix\n        # for n=2 this is simply self.embedding[x], for n&gt;2 it's a bit involved:\n        logits = self.embedding[[x[:,i] for i in range(0,self.n-1)]]\n        return logits\n\n\ntrain_ds = NgramDataset(train_lines)\nval_ds = NgramDataset(val_lines)\n\nbs = 10000\ntrain_loader = torch.utils.data.DataLoader(train_ds, shuffle=True, batch_size=bs)\nval_loader = torch.utils.data.DataLoader(val_ds, batch_size=bs*2)\ndls = DataLoaders(train_loader, val_loader)\n\n\nm = NgramNet()\nn_2gram = m.embedding.numel()\n\nsubs = [ProgressS(True),\n        MetricsS(),\n        DeviceS(device)]\n\nl = Learner(m, dls, F.cross_entropy, torch.optim.Adam, None, subs=subs)\nl.fit(5, lr=1e-1)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\n\n\n\n\n0\ntrain\n2.991\n\n\n0\neval\n2.575\n\n\n1\ntrain\n2.510\n\n\n1\neval\n2.477\n\n\n2\ntrain\n2.468\n\n\n2\neval\n2.463\n\n\n3\ntrain\n2.462\n\n\n3\neval\n2.460\n\n\n4\ntrain\n2.460\n\n\n4\neval\n2.459\n\n\n\n\n\n\n\n\nNow, let‚Äôs patch our model with a generate() method, returning names sampled from the learned representation. We can pass in a generator to make sure we get the same random behavior as we have seen above when generating names with the counts model:\n\n@fc.patch\ndef generate(self:NgramNet, n=10, generator=None):\n    names = []\n    for i in range(n):\n        name = '.' * (self.n-1)\n        while True:\n            # the following 2 lines are a involved, but I couldn't find a cleaner way\n            # to make this work for both n=2 and n&gt;2, \n            # since indexing works differently for both cases\n            idx = [c2i[i] for i in name[-(self.n-1):]]\n            logits = reduce(lambda emb, i: emb[i], idx, self.embedding).detach().cpu()\n\n            s = torch.multinomial(F.softmax(logits, dim=0), 1, generator=generator)\n            c = i2c[s.item()]\n            name += c\n            if c == '.':\n                names.append(name)\n                break\n    return names\n\n\nm.generate(generator=torch.Generator().manual_seed(42))\n\n['.akyleloniatanacaro.',\n '.eynn.',\n '.con.',\n '.thmarie.',\n '.di.',\n '.n.',\n '.kahannoa.',\n '.jadde.',\n '.ethann.',\n '.dalaydah.']\n\n\nAs you can see, these names are extremely similar to the ones we created above with the counts based model, and this is actually pretty surprising. One model was based on common sense, logic and simple counting. The other model on neural networks, embedding layers, loss functions, training loops, backward passes etc etc. Nonetheless, the results are the same!\nThe reason for this, is that the network architecture (and the loss) implies mathematical equivalence between both approaches. This can probably be shown explicitly with lots of complicated math, which I am not even going to try (I guess the answer will involve the principle of ‚Äúmaximum likelihood‚Äù..)\nWe can also have a look at both the weight matrices to see that the embedding matrix is extremely similar to the counts matrix. The way this network is set-up, is thus resulting in a weight matrix which is practically identical to the counts matrix!\n\nfig, axs = plt.subplots(1,2, figsize=(10,5))\n\naxs[0].imshow(F.softmax(m.embedding.detach().cpu(), dim=1).numpy());\naxs[0].set_title('\"softmaxed\" embedding matrix: \\n turning logits into probabilities');\naxs[1].imshow(probs);\naxs[1].set_title('Probs matrix from the count model');"
  },
  {
    "objectID": "posts/13_ngram/index.html#next-steps",
    "href": "posts/13_ngram/index.html#next-steps",
    "title": "N-gram language models",
    "section": "Next steps",
    "text": "Next steps\nThis extremely cool equivalence means we can switch to a neural network paradigm and tweak the network to improve performance. In other words, we can use any trick in the book for training neural networks! This is neat, because with the explicit counting based approach we were stuck with the ‚Äúcurse of dimensionality‚Äù for growing values of n.\nIn the next post, I‚Äôll discuss a first improvement: a simple but interesting model from Y. Bengio et al described in the paper A Neural Probabilistic Language Model. But let‚Äôs quickly train a couple of networks with higher n-grams to see how that goes.\n\ndef fit_ngram(n):\n    train_ds = NgramDataset(train_lines, n=ngram)\n    val_ds = NgramDataset(val_lines, n=ngram)\n\n    train_loader = torch.utils.data.DataLoader(train_ds, shuffle=True, batch_size=bs)\n    val_loader = torch.utils.data.DataLoader(val_ds, batch_size=bs*2)\n\n    dls = DataLoaders(train_loader, val_loader)\n\n    m = NgramNet(n=ngram)\n    n_elem = m.embedding.numel()\n\n    l = Learner(m, dls, F.cross_entropy, torch.optim.Adam, None, subs=subs)\n    l.fit(5, lr=1e-1)\n    \n    return n_elem, l.loss.detach().cpu()\n\n\nnum_elems, eval_losses = [], []\n\n\nngram = 3\nnum_elem, eval_loss = fit_ngram(ngram)\nnum_elems.append(num_elem)\neval_losses.append(eval_loss)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\n\n\n\n\n0\ntrain\n3.072\n\n\n0\neval\n2.480\n\n\n1\ntrain\n2.323\n\n\n1\neval\n2.257\n\n\n2\ntrain\n2.224\n\n\n2\neval\n2.234\n\n\n3\ntrain\n2.208\n\n\n3\neval\n2.230\n\n\n4\ntrain\n2.203\n\n\n4\neval\n2.228\n\n\n\n\n\n\n\n\nThe evaluation loss went down from 2.595 for the bi-gram model to 2.437 for the tri-gram model, an improvement! Let‚Äôs try a 4-gram model as well:\n\nngram = 4\nnum_elem, eval_loss = fit_ngram(ngram)\nnum_elems.append(num_elem)\neval_losses.append(eval_loss)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\n\n\n\n\n0\ntrain\n3.067\n\n\n0\neval\n2.492\n\n\n1\ntrain\n2.217\n\n\n1\neval\n2.195\n\n\n2\ntrain\n2.014\n\n\n2\neval\n2.145\n\n\n3\ntrain\n1.956\n\n\n3\neval\n2.138\n\n\n4\ntrain\n1.932\n\n\n4\neval\n2.139\n\n\n\n\n\n\n\n\n\nngram = 5\nnum_elem, eval_loss = fit_ngram(ngram)\nnum_elems.append(num_elem)\neval_losses.append(eval_loss)\n\n\n\n\n\n\n\n\nepoch\nmode\nloss\n\n\n\n\n0\ntrain\n3.216\n\n\n0\neval\n2.690\n\n\n1\ntrain\n2.287\n\n\n1\neval\n2.344\n\n\n2\ntrain\n1.955\n\n\n2\neval\n2.255\n\n\n3\ntrain\n1.810\n\n\n3\neval\n2.230\n\n\n4\ntrain\n1.735\n\n\n4\neval\n2.230\n\n\n\n\n\n\n\n\nAnd here, we start to see problems. The train loss is still getting better (1.735 for 5-gram vs 1.932 for 4-gram) but the evaluation loss is worse (2.198 for 5-gram vs 2.129 for 4-gram).\n\nfig, ax = plt.subplots(1)\nax.bar(range(0,3), eval_losses)\nax.set_xticks(range(0,3), [f'3gram \\n loss: {eval_losses[0]:.3f}', f'4gram \\n loss: {eval_losses[1]:.3f}', f'5gram \\n loss: {eval_losses[2]:.3f}']);\nax.set_title('Exponential increase in parameters');\n\n\n\n\nLet‚Äôs also quickly compare the number of elements in the embedding layer for the 2, 3 and 4-gram models:\n\nfig, ax = plt.subplots(1)\nax.bar(range(0,3), num_elems)\nax.set_xticks(range(0,3), [f'3gram \\n #: {num_elems[0]}', f'4gram \\n #: {num_elems[1]}', f'5gram \\n #: {num_elems[2]}']);\nax.set_title('Exponential increase in parameters');"
  },
  {
    "objectID": "posts/13_ngram/index.html#final-remarks-on-language-models",
    "href": "posts/13_ngram/index.html#final-remarks-on-language-models",
    "title": "N-gram language models",
    "section": "Final remarks on language models",
    "text": "Final remarks on language models\nAfter reading this blog post, I was thinking about what exactly makes this neural network a language model? The neural network obviously doesn‚Äôt know it‚Äôs a language model. It doesn‚Äôt know that the inputs are letters and that it‚Äôs predicting the next token in the sequence. Here are some closing thoughts:\n\nMany samples in the training data have the same context (features), but a different next character (label). Consider a bi-gram model and two names: ‚ÄúEmma‚Äù, ‚ÄúEsmeralda‚Äù. The first bi-gram for each of these names (skipping the start name token) are (e, m) for Emma and (e, s) for Esmeralda. They have the same context (e) but a different label (m and s respectively). This happens of course quite often in the data, and shows there is not a 1:1 relation from inputs to outputs. The model thus needs to learn a probability distribution over the possible labels. This learned probability distribution is what makes the model generative, since we can sample iteratively from it to create new names.\nThe point above is formalized in the models we created, by setting up a loss function that is used for multi-class classification (cross entropy loss). In non-generative models, this loss is used to predict the correct (single!) class for any input. During inference, the class that has the largest logit will be our prediction for the input. However, for generative models we don‚Äôt look at the class with the largest logit, instead we look at all the logits and turn it into a probability distribution (by taking the softmax) to sample over it. In this regard, there is no difference in network architecture between generative and non-generative models per se, but a difference in the way we use the network.\nLanguage models have a finite set of values the inputs and outputs can take on. Since we are working with a vocabulary, any input or output token is necessarily an element in this set. This is different for example from a regression problem in which we try to estimate the housing price from the square footage. In that case both the feature (square footage) and the label (price) can take on any value. The fact that the output has a finite set of outcomes is formalized by the loss function described above. The fact that the input is finite, is formalized by making use of an embedding matrix to encode our inputs.\nIn the past I have worked a couple of times on time series forecasting. And one simple way to create a probabilistic forecast, would be to employ the n-gram models defined above. The only requirement would be that the values are discretized."
  }
]