[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Helloüëã and welcome to my blog!"
  },
  {
    "objectID": "posts/02_paperspace_setup/index.html",
    "href": "posts/02_paperspace_setup/index.html",
    "title": "Paperspace setup",
    "section": "",
    "text": "Most people don‚Äôt have a GPU installed in their working machine that is suited for Deep Learning, and in fact you don‚Äôt need to. It‚Äôs quite easy to setup a remote GPU server nowadays, and in this blog I will explain how to do so with Paperspace Gradient.\nI started using Paperspace because of a recommendation from Jeremy Howard in his Live Coding Videos. If you haven‚Äôt seen these lectures, I can highly recommend them. They are a great resource on many things related to getting started with Deep Learning. Jeremy shows a lot of productivity hacks and practical tips on getting a good setup.\nHowever, the Paperspace setup explanations are a bit out-dated which can lead to confusion when following along with the video‚Äôs. Also, after the recording of the videos Jeremy created some nice scripts which simplify the setup. This blog will hopefully help others to navigate this and quickly set-up a remote GPU server. I would advice anybody who wants to try Paperspace, to first watch the videos from Jeremy to have a general idea of how it works, and then follow these steps to quickly get set-up.\nOnce you have signed up to Paperspace, go to their Gradient service and create a new project. Paperspace has a free tier, as well as a pro- ($8/month) and growth-plan ($39/month). I personally signed up for the pro-plan, which has a very good value for money. You get 15Gb persistent storage and free Mid instance types. If available, I use the A4000, which is the fastest and comes with 16GB of GPU memory.\nWith the pro-plan you can create up to 3 servers, or ‚ÄúNotebooks‚Äù as they are called by Paperspace (throughout this blog I‚Äôll refer to them as Notebook Servers). So let‚Äôs create one:"
  },
  {
    "objectID": "posts/02_paperspace_setup/index.html#first-look-at-our-notebook-server",
    "href": "posts/02_paperspace_setup/index.html#first-look-at-our-notebook-server",
    "title": "Paperspace setup",
    "section": "First look at our Notebook Server",
    "text": "First look at our Notebook Server\nNext, let‚Äôs open a terminal and get familiar with our Server\n\n\nTerminal\n\n&gt; which python\n/usr/local/bin/python\n\n&gt; python --version\nPython 3.9.13\n\nAnd let‚Äôs also check the PATH variable:\n\n\nTerminal\n\n&gt; echo $PATH\n/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin: /usr/sbin:/usr/bin:/sbin:/bin:/root/mambaforge/bin\n\nThe python command is thus pointing to the system Python installation. However, on the PATH variable we are also seeing an entry at the end mentioning mambaforge.\nAnd indeed we can execute:\n\n\nTerminal\n\n&gt; mamba list | grep python\n\nipython                   8.5.0              pyh41d4057_1    conda-forge\nipython_genutils          0.2.0                      py_1    conda-forge\npython                    3.10.6          h582c2e5_0_cpython    conda-forge\npython-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge\npython-fastjsonschema     2.16.2             pyhd8ed1ab_0    conda-forge\npython_abi                3.10                    2_cp310    conda-forge\n\nSo we are having both a mamba based Python 3.10.6 and a system installation of Python 3.9.13.\nLet‚Äôs open a Jupyter Notebook and see which Python version is running:\n\n\nUntitled.ipynb\n\nimport sys\nsys.version\n\nWhich returns: '3.9.13 (main, May 23 2022, 22:01:06) \\n[GCC 9.4.0]'. Jupyter is thus running the system Python installation.\n\n\n\n\n\n\nNote\n\n\n\nIn the videos Jeremy mentions that we should never use the system Python but instead always create a Mamba installation. However, since we are working here on a virtual machine that is only used for running Python, this shouldn‚Äôt be a problem. Just be aware that we are using the system Python which is totally separate from the Mamba setup.\n\n\nSince we are running the system Python version, we can inspect all the packages that are installed:\n\n\nTerminal\n\n&gt; pip list\n\n...\nfastai                            2.7.10\nfastapi                           0.92.0\nfastbook                          0.0.28\nfastcore                          1.5.27\nfastdownload                      0.0.7\nfastjsonschema                    2.15.3\nfastprogress                      1.0.3\n...\ntorch                             1.12.0+cu116\ntorchaudio                        0.12.0+cu116\ntorchvision                       0.13.0+cu116\n..."
  },
  {
    "objectID": "posts/02_paperspace_setup/index.html#persisted-storage-at-paperspace",
    "href": "posts/02_paperspace_setup/index.html#persisted-storage-at-paperspace",
    "title": "Paperspace setup",
    "section": "Persisted Storage at Paperspace",
    "text": "Persisted Storage at Paperspace\nIn general, things are not persisted on Paperspace. So anything we store during a session, will be gone when we restart our Notebook Server. However, Paperspace comes with two special folders that are persisted. It‚Äôs important to understand how these folder works since we obviously need to persist our work. Not only that, but we also need to persist our configuration files from services lik GitHub, Kaggle and HuggingFace and potentially any other config files for tools or services we are using.\nThe persisted folders are called /storage and /notebooks. Anything in our /storage is shared among all the Notebook Servers we are running, whereas anything that is stored in the /notebooks folder is only persisted on that specific Notebook Server."
  },
  {
    "objectID": "posts/02_paperspace_setup/index.html#set-up",
    "href": "posts/02_paperspace_setup/index.html#set-up",
    "title": "Paperspace setup",
    "section": "Set up",
    "text": "Set up\nIn the first few videos, Jeremy shows a lot of tricks on how to install new packages and set up things like Git and GitHub. After the recording of these videos, he made a GitHub repo which facilitates this setup greatly and makes most of the steps from the videos unnecessary. So let‚Äôs use that:\n\n\nTerminal\n\n&gt; git clone https://github.com/fastai/paperspace-setup.git\n&gt; cd paperspace-setup\n&gt; ./setup.sh\n\nTo understand what this does, let‚Äôs have a look at setup.sh:\n\n\nsetup.py\n\n#!/usr/bin/env bash\n\nmkdir /storage/cfg\ncp pre-run.sh /storage/\ncp .bash.local /storage/\necho install complete. please start a new instance\n\nFirst it‚Äôs creating a new directory inside of our /storage folder called cfg. As we will see, this is where we will store all our configuration files and folders.\nNext, the script copies 2 files to our storage folder. Let‚Äôs have a closer look at those\n\npre-run.sh\nDuring startup of a Notebook Server (upon creation or restart), Paperspace automatically executes the script it finds at /storage/pre-run.sh. This is really neat, since we can create a script at this location to automate our setup!\nFor the full script, click here, and let‚Äôs have a closer look at this first snippet:\n\n\npre-run.sh (snippet)\n\nfor p in .local .ssh .config .ipython .fastai .jupyter .conda .kaggle\ndo\n        if [ ! -e /storage/cfg/$p ]; then\n                mkdir /storage/cfg/$p\n        fi\n        rm -rf ~/$p\n        ln -s /storage/cfg/$p ~/\ndone\n\nSo we are iterating through a list of folder names (.local .ssh ...) on line 1, and for each one we create a directory inside of /storage/cfg on line 4. We only do this if the directory doesn‚Äôt already exist on line 3. Next, each of these folders is symlinked to the home directory (~/) on line 7.\nThis means that:\n\nWhen we store something in any of these symlinked folders (e.g.¬†~/.local), it‚Äôs actually being written to the associated storage folder (e.g.¬†/storage/cfg/.local) because of the symlink.\nWhenever we restart our Notebook Server, all the stuff that has previously been persisted (e.g.¬†in /storage/cfg/.local) are made available again in the home directory (e.g.¬†~/.local).\n\nThis is very nice, because as it turns out: many tools keep their configuration files in this home folder. So by persisting this data, they will keep working across restarts of our Notebook servers.\nLet‚Äôs a closer look at the folders we are persisting:\n\n.local\nWe saw before that the FastAI runtime comes with a number of installed Python packages. If we want to install additional packages, we could do: pip install &lt;package&gt;. However, pip installs the packages in /usr/local/lib, and are thus not persisted. To make sure our packages are persisted, we can instead install with pip install --user &lt;package&gt;. This --user flag, tells pip to install the package only for the current user, and so it installs into the ~/.local directory. So by persisting this folder, we make sure that we our custom installed python packages are persisted, awesome!\n\n\n.ssh\nTo authenticate with GitHub without using passwords, we use ssh keys. To create a pair of keys, we run: ssh-keygen. This creates the private key (id_rsa) and the public key (id_rsa.pub) to the ~/.ssh folder. Once we upload the public key to GitHub we can authenticate with GitHub, and by persisting this folder we can authenticate upon restart!\nBy now you probably get the idea, any of these folders represent a certain configuration we want to persist:\n\n.conda: contains conda/mamba installed packages\n.kaggle: contains a kaggle.json authentication file\n.fastai: contains downloaded datasets and some other configuration\n.config, .ipython and .jupyter: contain config files for various pieces of software such as matplotlib, ipython and jupyter.\n\nI personally also added .huggingface to this list, to make sure my HuggingFace credentials are also persisted. See here for the PR back into the main repo.\nIn the second part of the script we do exactly the same thing, but for a number of files instead of directories.\n\n\npre-run.sh (snippet)\n\nfor p in .git-credentials .gitconfig .bash_history\ndo\n        if [ ! -e /storage/cfg/$p ]; then\n                touch /storage/cfg/$p\n        fi\n        rm -rf ~/$p\n        ln -s /storage/cfg/$p ~/\ndone\n\nNow that we understand pre-run.sh, let‚Äôs have a look at the second file we store in our /storage folder:\n\n\n\n.bash.local\n\n\n.bash.local\n\n#!/usr/bin/env bash\n\nalias mambai='mamba install -p ~/.conda '\nalias pipi='pip install --user '\n\nexport PATH=~/.local/bin:~/.conda/bin/:$PATH\n\nPaperspace runs this script whenever we open a terminal. As you can see it defines two aliases to easily install things persistently with either mamba (mambai) or pip (pipi).\nAny binaries that are installed this way, are installed in ~/.local/bin (through pip) and to ~/.conda/bin/ (through mamba). We need to add these paths to the PATH variable, to make sure we can call them from the command line.\n\n\nNote on Mamba\nAt this point you might wonder why we have the Mamba installation at all, since we have seen that the system Python is used. In fact, our Mamba environment is totally decoupled from what we are using in our Jupyter notebook, and installing packages through mamba will not make them available in Jupyter. Instead, we should install Python packages through pip.\nSo what do we need Mamba for? I guess Jeremy has done this to be able to install binaries that he wants to use from the Terminal. For example, in the videos he talks about ctags which he installs through mamba. Since installing none-Python specific binaries through pip can be complicated, we can use Mamba instead. In other words, we can use it as a general package manager, somewhat similar to apt-get.\n\n\nFinal words\nIn my opinion Paperspace offers a great product for very fair money, especially if combined with the setup described in this blog!"
  },
  {
    "objectID": "posts/03_aiornot/index.html",
    "href": "posts/03_aiornot/index.html",
    "title": "First competitionüèÖ",
    "section": "",
    "text": "In the past couple of weeks I have participated in the first ever Hugging Face competition: aiornot. And as a matter of fact, it was also my first competition to participate in! The competition consisted of 62060 images (18618 train and 43442 test images) which were either created by an AI or not (binary image classification).\nToday, the competition has finished and the private leaderboard has been made public. I‚Äôm super happy (and proud üòá) that I finished in 15th place (98 participants):"
  },
  {
    "objectID": "posts/03_aiornot/index.html#credit-where-credit-is-due",
    "href": "posts/03_aiornot/index.html#credit-where-credit-is-due",
    "title": "First competitionüèÖ",
    "section": "Credit where credit is due:",
    "text": "Credit where credit is due:\n\nü§ó Hugging Face\nI would like to thank Hugging Face and in particular Abhishek Thakur for organizing this competition. I started looking for a first competition at Kaggle a few weeks back, and was very interested in the RSNA competition but quickly found that it was probably a bit too complicated for my first competition. I then saw a tweet from Abhishek announcing this competition and found it a perfect competition to get started.\n\n\nfastai\nIn the past month I have been following the fastai course and I am extremely grateful to Jeremy Howard and Sylvain Gugger for creating fastai. The book, the course, the videos and the great community they have built is really something special and is perfectly tailored for anybody who wants to get started with Deep Learning. Without fastai I could never have pulled this off üôè."
  },
  {
    "objectID": "posts/03_aiornot/index.html#learnings-and-notes",
    "href": "posts/03_aiornot/index.html#learnings-and-notes",
    "title": "First competitionüèÖ",
    "section": "Learnings and notes",
    "text": "Learnings and notes\n\nI quickly learned that data augmentation didn‚Äôt work well on this data. Initially I was a bit surprised by this, but upon inspection of the images I arrived at the following intuition. Normally we want to classify images by what‚Äôs being displayed in the image. So 2 images of a bike should both be classified as such. However, in this dataset we can have images of the same object but if one is created by an AI, and the other is not then they should be classified differently. So instead of looking at what‚Äôs being displayed, it probably has to learn more about the style or the way the image is built up. I can imagine that data augmentation makes this more difficult, especially warping, affine transformations and brightness, contrast augmentations. I was happily surprised to find that the 2nd and 4th place solutions also didn‚Äôt use these data augmentation!\nTraining on larger images works very well. I got a large performance boost for switching to sizes of 416. Jeremy Howard mentioned that this generally works well, and I think because of the nature of these images it worked especially well. To train large models on large images, I heavily relied on Gradient Accumulation to not have to reduce the batchsize.\nTransformer based models such as SWIN and VIT performed not as good as models based on convolutions, I used the convnext models.\nProgressive resizing didn‚Äôt work for me.\nI tried training on 5 epochs and 10 epochs. 10 epochs never gave me better results.\n\nLast but not least:\nParticipating in competitions is very motivating and rewarding. Working individually through courses, exercises and lecture notes is very interesting, but you don‚Äôt get a lot of feedback to how you are doing. Am I doing well? Should I spend more time on investigations into certain areas? When participating in a real-world competition you have a very clear goal, and you get immediate feedback on how you are doing. This type of project based learning has the advantage that it‚Äôs very clear what you need to focus on: anything that you encounter during the project.\nIt‚Äôs also great that it has a finite timeline, so that afterwards you can have a sense of achievement which motivates a lot. The Germans have a very nice word for this: Erfolgserlebnis.\n\n\n\nImage for Stable Diffusion prompt: ‚ÄúSense of achievement when finishing my first ever machine learning competition‚Äù"
  },
  {
    "objectID": "posts/05_crossentropy/index.html",
    "href": "posts/05_crossentropy/index.html",
    "title": "Cross entropy any which way",
    "section": "",
    "text": "Cross entropy is one of the most commonly used loss functions. In this post, we will have a look at how it works, and compute it in a couple of different ways.\nConsider a network that is build for image classification. During the forward pass, images are passed into the network and the network processes the data layer by layer, until evenually some final activations are being returned by the model. These final activations are called ‚Äúlogits‚Äù and represent the unnormalized predictions of our model.\nSince we generally use mini-batches during training, these logits are of shape [bs, num_classes]\nimport torch\nimport torch.nn.functional as F\n\ng = torch.manual_seed(42) # use a generator for reproducability\n\nbs = 32 # batch size of 32\nnum_classes = 3 # image classification with 3 different classes\n\nlogits = torch.randn(size=(bs, num_classes), generator=g) # size: [32,3]\n\nlogits[0:4] # show the logits for the first couple of samples\n\ntensor([[ 1.9269,  1.4873,  0.9007],\n        [-2.1055,  0.6784, -1.2345],\n        [-0.0431, -1.6047, -0.7521],\n        [ 1.6487, -0.3925, -1.4036]])\nEach row of this tensor represents the unnormalized predictions for each of our samples in the batch. We can normalize these predictions by applying a softmax. The softmax function does two things:\nThis makes sure that we can treat the output of this as probabilities, because:\nSpecifically:\n# Unnormalized predictions for our first sample (3 classes)\nlogits[0]\n\ntensor([1.9269, 1.4873, 0.9007])\n# Exponentiated predictions, making them all positive\nexp_logits = logits[0].exp()\nexp_logits\n\ntensor([6.8683, 4.4251, 2.4614])\n# Turn these values into probabilities by dividing by the sum\nprobs = exp_logits / exp_logits.sum()\n\n# verify that the sum of the probabilities sum to 1\nassert torch.allclose(probs.sum(), torch.tensor(1.))\n\nprobs\n\ntensor([0.4993, 0.3217, 0.1789])\nSo, let‚Äôs create a softmax function that does this for a whole batch:\ndef softmax(logits):\n    exp_logits = logits.exp() # shape: [32, 3]\n    exp_logits_sum = exp_logits.sum(dim=1, keepdim=True) # shape: [32, 1]\n    \n    # Note: this get's correctly broadcasted, since the exp_logits_sum will \n    # expand to [32, 3], so each value in exp_logits gets divided by the sum over its row\n    probs = exp_logits / exp_logits_sum # shape: [32, 3]\n    \n    return probs \n\nprobs = softmax(logits)\nprobs[0:4]\n\ntensor([[0.4993, 0.3217, 0.1789],\n        [0.0511, 0.8268, 0.1221],\n        [0.5876, 0.1233, 0.2891],\n        [0.8495, 0.1103, 0.0401]])\nNext, we want to compute the loss for which also need our labels. These labels represent the ground truth class for each of our samples in the batch. Since we have 3 classes they will be between 0 and 3 (e.g.¬†either 0, 1 or 2)\ng = torch.manual_seed(42) # use a generator for reproducability\n\nlabels = torch.randint(low=0, high=3, size=(32,), generator=g)\nlabels\n\ntensor([0, 2, 1, 1, 0, 2, 1, 2, 1, 2, 1, 1, 2, 0, 0, 1, 2, 1, 0, 1, 1, 2, 1, 2,\n        2, 1, 2, 0, 1, 1, 0, 0])\nFor classification we use the Negative Log Likelihood loss function, which is defined as such:\n\\[\n\\textrm{NLL} = - \\sum_{i}{q_i * \\log(p_i)}\n\\]\nwith \\(i\\) being the index that moves along the classes (3 in our example) and \\(q_i\\) being the probability that the ground truth label is class \\(i\\) (this is a somewhat strange formulation, since this probability is either 1 (for the correct class) or 0 (for all the non-correct classes)). Finally, \\(p_i\\) is the probability that the model associated to class \\(i\\).\nFor the very first row of our probs ([0.4993, 0.3217, 0.1789]) and our first label (0) we thus get:\n\\[\\begin{align}\n\\textrm{NLL} &= - ( (1 \\cdot \\log(0.4993)) + (0 \\cdot \\log(0.3217)) + (1 \\cdot \\log(0.1789)) ) \\\\\n\\textrm{NLL} &= - ( (1 \\cdot \\log(0.4993)) ) \\\\\n\\textrm{NLL} &= - \\log(0.4993)\n\\end{align}\\]\nFrom which we see that it‚Äôs just the negative log of the probability associated with the ground truth class.\nSince this computes only the NLL per sample, we also need a way to combine the NLL across the samples in our batch. We can do this either by summing or averaging, averaging has the advantage that the size of the loss remains the same when we change the batch-size, so let‚Äôs use that:\ndef nll(probs, labels):\n    # probs: shape [32, 3]\n    # labels: shape [32]\n    \n    # this plucks out the probability of the ground truth label per sample, \n    # it uses \"numpy's integer array indexing\":\n    # https://numpy.org/doc/stable/user/basics.indexing.html#integer-array-indexing\n    probs_ground_truth_class = probs[range(len(labels)), labels] # shape: [32]\n    \n    nll = -torch.log(probs_ground_truth_class).mean() # shape: []\n    return nll\nnll(probs, labels)\n\ntensor(1.3465)"
  },
  {
    "objectID": "posts/05_crossentropy/index.html#using-pytorch",
    "href": "posts/05_crossentropy/index.html#using-pytorch",
    "title": "Cross entropy any which way",
    "section": "Using PyTorch",
    "text": "Using PyTorch\nInstead of using our custom softmax, we can also use the build-in softmax function from PyTorch:\n\np = F.softmax(logits, dim=1) # dim=1 --&gt; compute the sum across the columns\nnll(p, labels)\n\ntensor(1.3465)\n\n\nInstead of using our custom nll we can also use the build-in version from PyTorch. However, nll_loss expects the log of the softmax (for numerical stability) so instead of softmax we have to use log_softmax:\n\np = F.log_softmax(logits, dim=1)\n\n# Assert that indeed the log_softmax is just the softmax followed by a log\nassert torch.allclose(p, F.softmax(logits, dim=1).log())\n\ntorch.nn.functional.nll_loss(p, labels)\n\ntensor(1.3465)\n\n\nThe combination of softmax and nll is called cross entropy, so we can also use PyTorch‚Äôs build-in version of that:\n\nF.cross_entropy(logits, labels)\n\ntensor(1.3465)\n\n\nInstead of the methods in nn.functional, we can also use classes. For that, we first create an instance of the object, and then ‚Äúcall‚Äù the instance:\n\nce = torch.nn.CrossEntropyLoss() # create a CrossEntropyLoss instance\nce(logits, labels) # calling the instance with the arguments returns the cross entropy\n\ntensor(1.3465)\n\n\nSimilarly, we can use classes for the log_softmax and nll_loss functions\n\nls = torch.nn.LogSoftmax(dim=1)\nnll = torch.nn.NLLLoss()\n\np = ls(logits)\nnll(p, labels)\n\ntensor(1.3465)\n\n\nThis is practical, if we want specify custom behavior of the loss function ahead of time of calling the actual loss function. For example, let‚Äôs say we want to compute the cross entropy loss based on ‚Äòsums‚Äô instead of ‚Äòaverages‚Äô. Then when using the method in F we would do:\n\nF.cross_entropy(logits, labels, reduction='sum')\n\ntensor(43.0866)\n\n\nSo whenever we call the loss, we have to specify the additional reduction argument.\nWhereas when using the loss classes, we can instantiate the class with that reduction argument, and then call the instance as per usual without passing anything but the logits and the labels:\n\n# instantiate \nce = torch.nn.CrossEntropyLoss(reduction='sum')\n\n# at some other point in your code, compute the loss as per default\nce(logits, labels)\n\ntensor(43.0866)\n\n\nThis is practical when the loss function is getting called by another object to which we don‚Äôt have easy access. So that we can‚Äôt easily change the arguments for that call. This is for example the case when using the FastAI Learner class, to which we pass the loss function which then get‚Äôs called by the Learner object with the default arguments (logits and labels). By using the classes, we can specify the reduction argument ahead of time and pass that instance to the Learner class."
  },
  {
    "objectID": "posts/01_blog_setup/index.html",
    "href": "posts/01_blog_setup/index.html",
    "title": "Blog setup",
    "section": "",
    "text": "In this blog post I‚Äôll explain how I created this blog, using Quarto and GitHub. In step 4 I‚Äôll show how to setup GitHub Actions, this has advantages over the other ways to publish our blog:\nI‚Äôm working on a Macbook, and using VS Code for code editing. If you are on a Linux or Windows machine, be aware that things might be a bit different from what I describe here.\nI am assuming you already have a GitHub account, that VS Code is installed and configured to run Python and Jupyter Notebooks."
  },
  {
    "objectID": "posts/01_blog_setup/index.html#step-1-install-quarto",
    "href": "posts/01_blog_setup/index.html#step-1-install-quarto",
    "title": "Blog setup",
    "section": "Step 1: install Quarto",
    "text": "Step 1: install Quarto\nFirst of all you need to install Quarto, go here, download and install the software. You should do this on the machine that you want to use for writing your blog, in my case my Macbook laptop.\nOnce installed you will have access to the quarto Command Line Interface (CLI). To make sure everything works as expected, open a terminal and execute:\n\n\nTerminal\n\nquarto --help\n\nThis should render some outputs describing the different commands and options that are part of the Quarto CLI and shows that Quarto is installed successfully."
  },
  {
    "objectID": "posts/01_blog_setup/index.html#step-2-create-a-github-repo",
    "href": "posts/01_blog_setup/index.html#step-2-create-a-github-repo",
    "title": "Blog setup",
    "section": "Step 2: create a GitHub repo",
    "text": "Step 2: create a GitHub repo\nTo host our blog we will use GitHub Pages, which is a service to host a website from a GitHub repository. Based on the name you pick for your repository you will create a so-called project-website or your unique user-website. For any general repo named my-awesome-repo, the website will be hosted on https://&lt;github-username&gt;.github.io/my-awesome-repo. This is a project-websites and you can create as many as you like.\nTo create your user-website, you have to name the repo exactly like this: &lt;github-username&gt;.github.io, the user-website will be hosted at https://&lt;github-username&gt;.github.io.\nThis is exactly what I want, so I create a new repo with the name: lucasvw.github.io.\nI find it helpful to add a .gitignore file with a Python template, to which we can later add some more entries to facilitate storing the right files on GitHub. Also make sure that the repo is Public (and not set to Private). Additionally, I added a README file and choose the Apache2 License.\nNext, I clone this repo to my machine by running:\n\n\nTerminal\n\ngit clone git@github.com:lucasvw/lucasvw.github.io.git"
  },
  {
    "objectID": "posts/01_blog_setup/index.html#step-3-add-a-quarto-project-to-the-repo",
    "href": "posts/01_blog_setup/index.html#step-3-add-a-quarto-project-to-the-repo",
    "title": "Blog setup",
    "section": "Step 3: add a Quarto project to the repo",
    "text": "Step 3: add a Quarto project to the repo\nNext, open VS Code and open the cloned repo. Then access the VS Code terminal and run:\n\n\nTerminal\n\nquarto create-project --type website:blog\n\nThis will add a number of files to our repo, which represent the basic structure of our blog. Most importantly:\n\nposts: here we will create our blog entries (one subfolder per blog entry)\n_quarto.yml: configuration file for our blog such as the theme, name, GitHub and Twitter links\nabout.qmd: source code for the ‚Äúabout‚Äù page.\nindex.qmd: source code for the landing page.\n\n\n\n\n\n\n\nNote\n\n\n\n.qmd files are like markdown files, but with lots of additional functionality from Quarto. Go here for more information on Markdown syntax and here for Quarto Markdown\n\n\nTo see what we currently have, let‚Äôs render our blog locally:\n\n\nTerminal\n\nquarto preview\n\nAlternatively, we can install the Quarto extension in VS Code, which will show a render button in the top right corner on any opened qmd file.\nTo publish the current contents to GitHub pages, we can run:\n\n\nTerminal\n\nquarto publish gh-pages\n\nWhen doing so, we get a message that we have to change the branch from which GitHub Pages builds the site. To do this, I go to https://github.com/lucasvw/lucasvw.github.io/settings/pages and select gh-pages instead of the main branch.\nAnd voila, in a few moments our blog will be running live at https://lucasvw.github.io/"
  },
  {
    "objectID": "posts/01_blog_setup/index.html#step-4-finalize-set-up-github-actions",
    "href": "posts/01_blog_setup/index.html#step-4-finalize-set-up-github-actions",
    "title": "Blog setup",
    "section": "Step 4: Finalize set-up: GitHub Actions",
    "text": "Step 4: Finalize set-up: GitHub Actions\nWhen we run the quarto publish gh-pages command, Quarto processes our files and turns them into web readable files (HTML, JS, CSS etc). It stores these files in our gh-pages branch and pushes them to our remote GitHub repo. This is great, but it means that this doesn‚Äôt store our source files.\nTo do so, let‚Äôs first open our .gitignore file and make sure that it contains the following entries so that we don‚Äôt check in any files we don‚Äôt need.\n\n\n.gitignore\n\n# Quarto\n/.quarto/\n_site/\n\n# Mac files\n.DS_Store\n\nNext, we can commit all the remaining files to Git and push them to our remote repo. If we ever lose access to our local machine, we can restore everything we need from GitHub.\nHowever, now we have 2 things we need to do whenever we finish our work:\n\nstore our source files on the main branch and push to GitHub\nrun the publish command to update the blog\n\nThis is a bit annoying and it would be much better if we can just push to the main branch and GitHub would take care of building our website and updating it. This also allows us to create blog entries on any machine that has access to git, we don‚Äôt need to have quarto installed. This is particularly practical if we want to write blog entries from our deep learning server. So let‚Äôs use GitHub actions for this.\n\n\n\n\n\n\nNote\n\n\n\nBefore you continue make sure you have at least once run a quarto publish gh-pages command, this is necessary for the things below to work\n\n\nFirst we need to add the following snippet to _quarto.yml\n\n\n_quarto.yml\n\nexecute:\n  freeze: auto\n\nThis will make sure that GitHub actions doesn‚Äôt execute any executable code, but will show the pre-rendered outputs it finds in the _freeze folder.\nFinally, create the file .github/workflows/publish.yml and populate it with the following code:\n\n\n.github/workflows/publish.yml\n\non:\n  workflow_dispatch:\n  push:\n    branches: main\n\nname: Quarto Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v3\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n\n      - name: Render and Publish\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\nOnce we push these things to GitHub, we are good to go. Whenever we push anything to the main branch, this workflow will execute and take care of updating the gh-pages branch and updating the blog."
  },
  {
    "objectID": "posts/04_matmul/index.html",
    "href": "posts/04_matmul/index.html",
    "title": "Fast matrix multiplications",
    "section": "",
    "text": "Matrix multiplications are kind of boring, so why write a blog post about them? Well, matrix multiplications are the most basic computation that is being performed by neural networks. So it‚Äôs probably good to be familiar with them (although we never do them by hand). Also, we are going to focus on speeding them up by doing vectorization. Vectorization is something we often have to do, to make sure everything runs as quickly as possible, and it‚Äôs thus a good exercise to understand how to achieve this. Especially since it involves being very familiar with matrices, their shapes, broadcasting operations and the like.\nThis post follows the first lecture of Part 2 of the FastAI course (2019), I will provide some additional explanations, and present one other optimization that is not presented in the lecture."
  },
  {
    "objectID": "posts/04_matmul/index.html#definition",
    "href": "posts/04_matmul/index.html#definition",
    "title": "Fast matrix multiplications",
    "section": "Definition",
    "text": "Definition\nMatrix multiplication is not difficult, it basically goes like this:\n\nFor matrix A of size [ar x ac] ¬† ([4 x 3] in the image below)\nand matrix B of size [br x bc] ¬† ([3 x 2] in the image below)\nthe matrix product A * B is of size [ar x bc] ([4 x 2] in the image below).\nSo the matrix product is thus only defined when ac == br (3 == 3 in the image below)\n\n\n\n\n\n\nSo for any valid matrix multiplication, we have three dimensions that need to considered:\n\nar: the row dimension of matrix A. The size of this dimension will become the size of the row dimension of the output matrix (black arrow in the image above)\nbc: the column dimension of matrix B. The size of this dimension will become the size of the column dimension of the output matrix (purple arrow in the image above)\nac: the column dimension of Matrix A and br: the row dimension of matrix B: they need to be equal (red arrow in the image above)\n\nWhy do ac and bc need to be equal? Well, because we take the inner product over this dimension when computing the cell values of the new matrix, and inner-products are only defined for vectors of equal length. Below, I will also refer to this dimension as the dimension over which we collapse (or the ‚Äúcollapsible‚Äù dimension), since in the output matrix, this dimension is no longer present.\n\n\n\n\n\nIn other words, to compute cell \\(C_{i,j}\\) we take the inner product between row i of matrix A and column j of matrix B. Let‚Äôs have a look at one other cell, to make sure we understand fully what‚Äôs going on. In the next figure we compute the value for cell \\(C_{3,2}\\), we thus take the inner-product between row 3 of matrix A and column 2 of matrix B:\n\n\n\n\n\nLet‚Äôs do this in code and confirm what we have established above about the shapes of the matrices:\n\nimport torch\n\na = torch.randn(4,3)\nb = torch.randn(3,2)\n\n\n# Confirm the shape of the output matrix\n(a@b).shape\n\ntorch.Size([4, 2])\n\n\n\n# Confirm the value of one output cell (C00)\nC00_manual = (a[0,0] * b[0,0]) + (a[0,1] * b[1,0]) + (a[0,2] * b[2,0])\nC00_auto = (a@b)[0,0]\n\nassert torch.allclose(C00_manual, C00_auto)\n\nWith what we know, let‚Äôs create our own matmul function:\n\ndef matmul(a, b):\n    # fill in the sizes of the dimensions\n    ar, ac = a.shape\n    br, bc = b.shape\n\n    # assert that our matrices can be multiplied \n    assert ac == br\n\n    # create an output tensor of the expected size (ar x bc)\n    out = torch.zeros(ar, bc)\n\n    # iterate over the rows of the output matrix (--&gt; length ar)\n    for i in range(out.shape[0]):\n        # iterate over the columns of the output matrix (--&gt; length bc)\n        for j in range(out.shape[1]):\n            # iterate over the \"collapsed\" dimension (--&gt; length ac and length br), \n            for k in range(ac):\n                out[i, j] += a[i, k] * b[k, j]\n    \n    return out\n\n# Confirm that the result is correct\nassert torch.allclose(matmul(a,b), a@b)\n\nAlthough this is functionally correct, it‚Äôs not very efficient. In fact, to compute the value of one cell of the output matrix, we are doing three separate multiplications. In other words, for each cell out[i,j] we are calling three times (once for every value of k):\nout[i, j] += a[i, k] * b[k, j]\nLet‚Äôs try to reduce the computation of one cell to just one single call."
  },
  {
    "objectID": "posts/04_matmul/index.html#first-improvement",
    "href": "posts/04_matmul/index.html#first-improvement",
    "title": "Fast matrix multiplications",
    "section": "First improvement",
    "text": "First improvement\nTo do so, we need to get rid of the loop over the ‚Äúcollapsible‚Äù dimension k. We can simply do this by replacing the k with a :, so that we select the whole dimension instead of just one element in that dimension. The multiplication (*) is doing an element wise multiplication, so we have to wrap the result with a .sum().\n\ndef matmul2(a, b):\n    ar, ac = a.shape\n    br, bc = b.shape\n    assert ac == br\n\n    out = torch.zeros(ar,bc)\n    # iterate over the rows of the output matrix (i)\n    for i in range(out.shape[0]):\n        # iterate over the columns of the output matrix (j)\n        for j in range(out.shape[1]):\n            out[i, j] = (a[i, :] * b[:, j]).sum()\n    \n    return out\n\n# Confirm that the result is correct\nassert torch.allclose(matmul(a,b), a@b)"
  },
  {
    "objectID": "posts/04_matmul/index.html#second-improvement",
    "href": "posts/04_matmul/index.html#second-improvement",
    "title": "Fast matrix multiplications",
    "section": "Second improvement",
    "text": "Second improvement\nThe improvement above, gives us the value of a cell in one single call:\nout[i, j] = (a[i, :] * b[:, j]).sum()\nThis is great, let‚Äôs try to vectorize this even further, and get rid of the second loop (the loop over j), this means that we need to compute the values of a single row of the output matrix in one call, e.g.\nout[i,:] = ...\nWe know that the value of cell \\(C_{ij}\\) is the inner product between row i of A and column j of B. We also know that any row of matrix C will have two values. Let‚Äôs compute them manually:\n\nout_00 = (a[0,:] * b[:,0]).sum()\nout_01 = (a[0,:] * b[:,1]).sum()\n\nC0_manual = torch.stack([out_00, out_01])\nC0_auto = (a@b)[0]\n\nassert torch.allclose(C0_manual, C0_auto)\n\nprint(f'{out_00=}', f'{out_01=}', f'{C0_manual=}', sep='\\n')\n\nout_00=tensor(-0.0213)\nout_01=tensor(0.3668)\nC0_manual=tensor([-0.0213,  0.3668])\n\n\nObserve that for the computation of one row of output, we need:\n\none single row of A (a[0,:])\nthe full matrix of B, we need both the first (b[:,0]) column and the second column (b[:,1]).\n\nLet‚Äôs check the sizes of both and see whether we can use broadcasting:\n\nprint(f'{a[0,:].shape=}', f'{b.shape=}', sep='\\n')\n\na[0,:].shape=torch.Size([3])\nb.shape=torch.Size([3, 2])\n\n\nUnfortunately, size [3] and [3,2] don‚Äôt broadcast. To make them broadcast, we have to add an empty dimension at the end of the row of the A matrix. Then, the shapes [3, 1] and [3, 2] can be broadcasted to another by duplicating the former in the column direction:\n\nt = a[0,:].unsqueeze(-1) # [3, 1]\n\nt.broadcast_to(b.shape) # [3, 2]\n\ntensor([[ 0.9193,  0.9193],\n        [-0.0426, -0.0426],\n        [ 1.3566,  1.3566]])\n\n\nNow that both object are the same size we can do an element-wise multiplication and then sum over the rows to arrive at an output of size [1,2]:\n\nC0_manual = (t*b).sum(dim=0)\nC0_auto = (a@b)[0,:]\n\nassert torch.allclose(C0_manual, C0_auto)\n\nprint(f'{C0_manual=}', f'{C0_manual.shape=}', sep='\\n')\n\nC0_manual=tensor([-0.0213,  0.3668])\nC0_manual.shape=torch.Size([2])\n\n\nSo let‚Äôs implement this:\n\ndef matmul3(a, b):\n    ar, ac = a.shape\n    br, bc = b.shape\n    assert ac == br\n\n    out = torch.zeros(ar,bc)\n    # iterate over the rows of the output matrix (i)\n    for i in range(out.shape[0]):\n        out[i, :] = (a[i, :].unsqueeze(-1) * b).sum(dim=0)\n    \n    return out\n\n# Confirm that the result is correct\nassert torch.allclose(matmul(a,b), a@b)"
  },
  {
    "objectID": "posts/04_matmul/index.html#third-improvement",
    "href": "posts/04_matmul/index.html#third-improvement",
    "title": "Fast matrix multiplications",
    "section": "Third improvement",
    "text": "Third improvement\nFor the final improvement, we need to get rid of the only remaining loop over the rows of our output matrix (i). So let‚Äôs understand very well what we are having at the moment:\n\nWe are iterating over the (4) rows of our output matrix\nFor each row, we are computing the (2) values of our row at once by doing out[i, :] = (a[i, :].unsqueeze(-1) * b).sum(dim=0) and let‚Äôs break this down once again in steps:\n\na[i, :] has shape [3] and represents one row of A\nwith a[i, :].unsqueeze(-1) we add an extra dimension so that we can broadcast, the result has shape [3, 1]\nb has shape [3, 2] and is the full B matrix\nelement-wise multiplication of 2. and 3. gives a matrix of shape [3, 2]\nby summing over the rows (.sum(dim=0)) we arrive at the result of shape [2]\n\n\nWe want to improve this by instead of iterating over the 4 rows, do these computations all at once for all rows. So let‚Äôs start by not selecting one row of A (a[i,:]) but instead just the whole a matrix:\n\na has shape [4, 3]\nsimilarly to what we did before, we can a.unsqueeze(-1) to add an extra dimension, the result has shape [4, 3, 1]\nsame as before, b has shape [3, 2] and is the full B matrix\nbroadcasting of 2. and 3. will do the following:\n\na.unsqueeze(-1) has shape [4, 3, 1] and get‚Äôs expanded to [4, 3, 2] to match the shape of b ([3, 2])\nbut b also needs to match a, first an additional empty dimension is added in the front: [1, 3, 2] and then it get‚Äôs expanded to [4, 3, 2]\nnext, the element-wise multiplication of 2. and 3. gives a matrix (tensor) of shape [4, 3, 2], let‚Äôs call it t. It‚Äôs import to realize what this t represents. For that, notice that the first dimension (length 4) and last dimension (length 2) are the dimensions of our output matrix ([4, 2]). The middle dimension (length 3) represents the element wise multiplications of any row in matrix A and any column of matrix B. So by doing for example t[i, :, j].sum() we get the value for cell \\(C_{i,j}\\) of our output matrix!\n\nThis means, that to arrive at the final result, we will have to collapse (sum) over the middle dimension!\n\n\ndef matmul4(a, b):\n    return (a.unsqueeze(-1) * b).sum(dim=1)\n\n# Confirm that the result is correct\nassert torch.allclose(matmul(a,b), a@b)"
  },
  {
    "objectID": "posts/04_matmul/index.html#timings",
    "href": "posts/04_matmul/index.html#timings",
    "title": "Fast matrix multiplications",
    "section": "Timings",
    "text": "Timings\nTo see what kind of a speed-up we have achieved, let‚Äôs look at the timings of our first version with three loops and the timings of our optimized version:\n\n%timeit -n 1000 matmul(a,b)\n\n318 ¬µs ¬± 13.7 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n%timeit -n 1000 matmul4(a,b)\n\n10.7 ¬µs ¬± 1.46 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n\n\nNice, our optimized version is about 30 times faster then our un-optimized version with 3 loops! Additionally, let‚Äôs check the timings of doing the matrix multiplication with einsum:\n\n%timeit -n 1000 torch.einsum('ij,jk-&gt;ik', a, b)\n\n25.9 ¬µs ¬± 3.78 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n\n\nSurprisingly, our optimized version is twice as fast as einsum. This is certainly something I didn‚Äôt expect.\nFinally, let‚Äôs also check the timings of using the @ operator:\n\n%timeit -n 1000 a@b\n\n3.29 ¬µs ¬± 522 ns per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n\n\nAs expected, this is even faster then our optimized version, probably because it runs in optimized C / CUDA code"
  },
  {
    "objectID": "posts/06_stable_diffusion_basics/index.html",
    "href": "posts/06_stable_diffusion_basics/index.html",
    "title": "Introduction to Stable Diffusion - Concepts",
    "section": "",
    "text": "Stable Diffusion, a generative deep learning algorithm developed in 2022, is capable of creating images from prompts. For example, when presented the prompt: A group of people having lunch on the moon, the algorithm creates the following image:\nAnd although this image isn‚Äôt perfect, it‚Äôs pretty amazing that it took less then 30 seconds to create this image. The algorithm ‚Äúimagined‚Äù that people on the moon should be wearing space suits, and that lunch is generally eaten in a sitting position and around a table. Also, the surroundings look indeed pretty moonish. Not bad at all!\nIn this post, we will have a look at the main components involved in creating this image, and follows largely the steps of Lesson 9 of Deep Learning for Coders."
  },
  {
    "objectID": "posts/06_stable_diffusion_basics/index.html#intuition",
    "href": "posts/06_stable_diffusion_basics/index.html#intuition",
    "title": "Introduction to Stable Diffusion - Concepts",
    "section": "Intuition",
    "text": "Intuition\nConsider some kind of black box system that takes some input data, and based on it creates some output data. Let‚Äôs say, it takes an image of a handwritten digit as input, and outputs the probability that the image is indeed a hand written digit. Visually something like this:\n\n\n\n\n\nIn statistics, we would call this a probability density function. It‚Äôs a function that takes data as input, and gives the probability that\n\nif the input data is coming indeed from the distribution,\nwhat‚Äôs the probability that we see this data?\n\nApplied to our use-case: if the presented image is indeed from the distribution (of images) that represent hand written digits, what‚Äôs the probability that we observe the presented image?\nWith such a system, we could start with an image made up of pure noise and iteratively do:\n\nget the probability \\(p_0\\) of the image being a handwritten digit from the black box system\nchange the value of one of the pixels at random\nget the new probability \\(p_1\\) whether the image is a handwritten digit from the black box system\nwhen \\(p_1 &gt; p_0\\) update the image with the changed pixel value\n\nWhen following this procedure long enough and thus updating pixel for pixel, we would gradually change all the values of our pixels of our image, until eventually it will start to resemble a handwritten digit.\nIn principle, this is the simple intuition behind stable diffusion."
  },
  {
    "objectID": "posts/06_stable_diffusion_basics/index.html#the-main-component-unet",
    "href": "posts/06_stable_diffusion_basics/index.html#the-main-component-unet",
    "title": "Introduction to Stable Diffusion - Concepts",
    "section": "The main component: Unet",
    "text": "The main component: Unet\nSo how are we going to create this system that will return the probability that an image is depicting a handwritten digit? Let‚Äôs try to create a model, that will do so. To get the training data, we need lots of images that depict handwritten digits. Something like this:\n\n\n\n\n\nSince these images represent actual hand-written digits, the system will need to output a probability close to 1 for these images. But how do we get images that ‚Äúsomewhat‚Äù or ‚Äúrarely‚Äù represent handwritten digits and are associated with lower probability values? We somehow have to ‚Äúcrappify‚Äù these existing images. We can do this by using these same images and sprinkle them with different amounts of noise. The more noise we add, the less the image will resemble a handwritten digit. Visually:\n\n\n\n\n\nNow we can train a network which we feed the noisified images as input and use the noise image as label. So instead of predicting the probability that an image depicts a handwritten digit, the model will predict the noise. By using a simple MSE loss on the actual noise (labels) and the predictions the model will learn how to predict the noise from looking at a noisified images.\nThe idea behind this model is that once this model is trained, we could run inference on some random noise. The model will give us a prediction of all the noise in the image, which when removed from the input, renders an image of a digit.\nIt turns out that this process works much better if, instead of removing all the noise that was predicted by the model at once, we just remove a little bit of the noise that was predicted. This way, we end up with an image which is just a bit less noisy then what we started with. We then feed this less noisy image again into our network, and thus iteratively remove more and more noise from the image, until after a certain amount of steps (50 for example) we end-up with an image that is free of noise.\nOne model architecture that is takes images as input and also outputs images is called a Unet and forms the first component of our Stable Diffusion system:\n\n\n\n\n\n\n\n\nComponent\nInputs\nOutputs\n\n\n\n\nUnet\nNoisy images\nNoise"
  },
  {
    "objectID": "posts/06_stable_diffusion_basics/index.html#compression-variational-autoencoder",
    "href": "posts/06_stable_diffusion_basics/index.html#compression-variational-autoencoder",
    "title": "Introduction to Stable Diffusion - Concepts",
    "section": "Compression: Variational Autoencoder",
    "text": "Compression: Variational Autoencoder\nWhen working with images in neural networks we often reduce the resolution of images or use smaller patches of the original image to make sure everything fits on the GPU. With stable diffusion, we naturally want to output images of high resolution, so we either need very large GPUs, or instead we use a compression trick by making use of a Variational Autoencoder (VAE).\nA VAE is a network architecture having an encoder and a decoder. In the encoder the image input is being transformed through a series of convolutional layers into a compressed representation, the latent. In the decoder this compressed latent is passed through a series of layers that are trying to reconstruct the original image. Visually:\n\n\n\n\n\nThis might look like a boring network architecture at first. But it‚Äôs actually a very neat way to compress things: We can feed this model all the different noisified images mentioned earlier, and use an MSE loss on the inputs and outputs. This will train the model to create compressed representations of our images (the latents) that can be used by the decoder to recreate the original image. This means that the latent representation carries close to the same amount of ‚Äúinformation‚Äù as our full-size images.\nWith this, we can now train the previously discussed Unet on all the latents instead of the full size images!\nDuring inference the combined architecture looks like this: we run any input first through the encoder returning a highly compressed version of our input (i.e.¬†the latents). We then run it through the Unet, which will output a latent representation of the noise. If we (partly) subtract the noise latent from the noisy image latent, we end up with a latent representation of our image which is a bit less noisy then what we started with. Finally, to move from latent representation to full-size images, we can use the decoder of the VAE. Visually:\n\n\n\n\n\nTo summarize:\n\n\n\n\n\n\n\n\nComponent\nInputs\nOutputs\n\n\n\n\nVAE encoder\nNoisy image\nNoisy image latents\n\n\nUnet\nNoisy image latents\nNoise latents\n\n\nVAE decoder\nNoise latents\nNoise"
  },
  {
    "objectID": "posts/06_stable_diffusion_basics/index.html#prompting-clip",
    "href": "posts/06_stable_diffusion_basics/index.html#prompting-clip",
    "title": "Introduction to Stable Diffusion - Concepts",
    "section": "Prompting: CLIP",
    "text": "Prompting: CLIP\nSo how can we create prompting? Let‚Äôs start simple and imagine we just want to specify which handwritten digit we would like to generate, so any number between 0 and 9. We could do this by training the Unet not only on the noisy image (input) and noise (output), but instead also give it a representation of the digit we sprinkled the noise on as input. The most generic way to do this, would be to create a one-hot encoded representation of the digit, visually:\n\n\n\n\n\nTo create an image depicting the digit ‚Äúthree‚Äù from pure noise, we would then start with a random noise latent and feed it together with the one-hot encoded representation of the digit into the Unet. This way, the Unet is ‚Äúguided‚Äù to create an image of digit ‚Äúthree‚Äù and not just any image, visually:\n\n\n\n\n\nTo continue, how are we going to scale this for any text prompt besides our 10 digits? We can‚Äôt possibly create a one-hot encoding of any possible prompt, that would make our vector infinitely large. Instead, we want to compress the encoding in some finite, high dimensional space, e.g.¬†we want to create an embedding encoding of our prompt.\nTo create these embeddings, we first of all need again lots of data. For example by capturing a lot of images from the internet, these image generally have a textual description in the HTML tag.\nWe can feed the text and images into two separate encoders. These encoders take the text and image respectively and output a vector. Next, we can align the vector representations in a matrix and take the dot-product between them. We want the text and image vectors of the same ‚Äúobject‚Äù to align, this means their dot-product should be large. Also, we want the vectors of different objects to not align, so their dot-product should be small. Visually:\n\n\n\n\n\nA loss function that does exactly this, is called the Contrastive Loss. And the model described here is called Contrastive Language Image Pre-training (CLIP).\nDuring inference, we can use the trained text-encoder and apply it to the prompt. The outputted embedding can then be used as the encoding we feed into our Unet in combination with the noisy image latent.\nTo summarize:\n\n\n\n\n\n\n\n\nComponent\nInputs\nOutputs\n\n\n\n\nCLIP text encoder\nPrompt\nEmbedding\n\n\nVAE encoder\nNoisy image\nNoisy image latents\n\n\nUnet\nNoisy image latents + Prompt embedding\nNoise latents\n\n\nVAE decoder\nNoise latents\nNoise"
  },
  {
    "objectID": "posts/06_stable_diffusion_basics/index.html#noise-scheduler",
    "href": "posts/06_stable_diffusion_basics/index.html#noise-scheduler",
    "title": "Introduction to Stable Diffusion - Concepts",
    "section": "Noise scheduler",
    "text": "Noise scheduler\nAbove it was stated, that ‚Äúdifferent‚Äù amounts of noise are sprinkled on our images during training, and during inference ‚Äúsome‚Äù amount of noise is being subtracted from the image. In the next post, which will be a ‚Äúcode‚Äù version of this post, we will see more how this exactly works, but let‚Äôs introduce one more concept here:\nTo formalize the amounts of noise we will use something called a noise schedule, which maps an integer value (called the timestep \\(t\\)) to an amount of noise we will add to our image. This noise schedule is a monotonically decreasing function of \\(t\\), so large values of \\(t\\) will add a small amount of noise and small values of \\(t\\) add a large amount of noise. A typical noise schedule looks something like this:\n\n\n\n\n\nWith this noise schedule, we can pick different amounts of noise during training and add it to the images in the batch. Additionally, we will feed the noise parameter to the Unet, so that it knows how much noise was added to the image. This sould make it easier for the model to reconstruct the noise.\nTo summarize:\n\n\n\n\n\n\n\n\nComponent\nInputs\nOutputs\n\n\n\n\nCLIP text encoder\nPrompt\nEmbedding\n\n\nVAE encoder\nNoisy image\nNoisy image latents\n\n\nUnet\nNoisy image latents + Prompt embedding + Noise level\nNoise latents\n\n\nVAE decoder\nNoise latents\nNoise\n\n\n\nThat‚Äôs it for now! If you came this far, I hope you enjoyed it. For me, it helped a lot in my understanding by writing all this down. In the next blog post, we will have a look at how these concepts translate into code by making use of HuggingFace libraries such as diffusers and transformers"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Another Machine Learning Blog",
    "section": "",
    "text": "Introduction to Stable Diffusion - Concepts\n\n\n\n\n\n\n\nGenerative\n\n\nStable Diffusion\n\n\nDiffusers\n\n\n\n\n\n\n\n\n\n\n\n\nMar 18, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nCross entropy any which way\n\n\n\n\n\n\n\nLoss functions\n\n\nSoftmax\n\n\nNLL\n\n\nCross entropy\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nFast matrix multiplications\n\n\n\n\n\n\n\nFoundations\n\n\nMaths\n\n\nVectorization\n\n\nLinear Algebra\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nFirst competitionüèÖ\n\n\n\n\n\n\n\nDeep Learning\n\n\nImage Classification\n\n\nCompetition\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nPaperspace setup\n\n\n\n\n\n\n\nsetup\n\n\npaperspace\n\n\nGPU\n\n\nhow-to\n\n\nMLOps\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\n  \n\n\n\n\nBlog setup\n\n\n\n\n\n\n\nblogging\n\n\nsetup\n\n\nhow-to\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2023\n\n\nLucas van Walstijn\n\n\n\n\n\n\nNo matching items"
  }
]