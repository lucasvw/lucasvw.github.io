{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5a706372-b4f8-4d32-9470-24b837947fa5",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Transformers\"\n",
    "author: \"Lucas van Walstijn\"\n",
    "date: \"2023-11-03\"\n",
    "categories: [nlp, transformers, attention]\n",
    "image: \"attention.png\"\n",
    "comments:\n",
    "  utterances:\n",
    "    repo: lucasvw/BlogComments\n",
    "format:\n",
    "  html:\n",
    "    code-overflow: scroll\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c683ee1-4cf7-4d16-8e6e-2de5ebf4dc5b",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "import random\n",
    "from functools import reduce, partial\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torcheval.metrics as tem\n",
    "import fastcore.all as fc\n",
    "\n",
    "\n",
    "from nntrain.dataloaders import DataLoaders\n",
    "from nntrain.learner import *\n",
    "from nntrain.activations import *\n",
    "from nntrain.acceleration import *\n",
    "from nntrain.rnn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc73d2c-4bf0-4f1f-a5e3-8311ec12d146",
   "metadata": {},
   "source": [
    "Transformers were introduced in the paper *Attention is all you need* in 2017 by Vaswani et al, and as the title suggests focusses heavily on a something called *attention*. This architecture is still very much in use today and is used in text applications applications like ChatGPT, but also in other fields such as vision, audio and other sequence based fields such as protein folding.\n",
    "\n",
    "It's an extremely important architecture, and in this post I would like to investigate how it works. Let's discuss the transformer architecture in terms of a word level model trying to predict the next word in a sequence. With a vocabulary of size 100, some tokenized sentence of the training set might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2018d0e-757d-440f-b500-59b474dd2417",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[81, 14, 3, 94]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_vocab = 100\n",
    "\n",
    "random.seed(42)\n",
    "x_enc = [random.randint(0, len_vocab-1) for i in range(4)]\n",
    "x_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6577e58f-d923-45ef-b81b-cd02bec788e7",
   "metadata": {},
   "source": [
    "From this tokenized sequence we could naively create the following training samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb0774c-1465-4737-ba02-815dfdfbd36e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input -> output\n",
      "---------------\n",
      "81    -> 14\n",
      "14    -> 3\n",
      "3     -> 94\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "print(f'{\"input\".ljust(5)} -> output')\n",
    "print(f'{\"-\"*15}')\n",
    "for i in range(len(x_enc) - 1):\n",
    "    print(f'{x_enc[i].__repr__().ljust(5)} -> {x_enc[i+1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cef4e01-ce71-4bb0-b8ca-d2464fff78f5",
   "metadata": {},
   "source": [
    "This is essentially the bigram language model, in which only the previous token is used to predict the next. \n",
    "\n",
    "Now let's consider the general case in which for each prediction we can use the full history of tokens, we then get the following samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f48696-6513-4199-8d94-d3ef46457b49",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input        -> output\n",
      "----------------------\n",
      "[81]         -> 14\n",
      "[81, 14]     -> 3\n",
      "[81, 14, 3]  -> 94\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "print(f'{\"input\".ljust(12)} -> output')\n",
    "print(f'{\"-\"*22}')\n",
    "for i in range(len(x_enc) - 1):\n",
    "    print(f'{x_enc[:i+1].__repr__().ljust(12)} -> {x_enc[i+1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac64c11-d199-4071-9e58-384423c62897",
   "metadata": {},
   "source": [
    "As we progress further into the sequence we can use more *context* from the sequence to predict the next token. For the first sample we just have one single token for the prediction, for the last sample we can use 3 tokens for our prediction. The question is then: *how do we use this varying length context in an optimal way to make the best prediction for the next word?*\n",
    "\n",
    "The initial idea we will pursue is to \n",
    "\n",
    "1. create an embedding vector for each token\n",
    "2. average the embeddings of the tokens in the context\n",
    "\n",
    "Arguably, this is a pretty rough way of dealing with the information contained in the individual embeddings. And indeed, we will see that self-attention is a bit more involved than this. However, it's a good starting point to understand self-attention.\n",
    "\n",
    "If we represent an embedding vector of token $x$ as `E_[x]` and the average of two embedding vectors $x$ and $y$ as `E_[x,y]` we can visualize this initial idea outlined above as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c67de6a-be06-4390-a1cb-60eb09768c6d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw input  -> embedded input   -> avg embedded input  -> output\n",
      "--------------------------------------------------------------\n",
      "81         -> E_81             -> E_[81]              -> 14\n",
      "14         -> E_14             -> E_[81, 14]          -> 3\n",
      "3          -> E_3              -> E_[81, 14, 3]       -> 94\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "print(f'{\"raw input\".ljust(10)} -> {\"embedded input\".ljust(16)} -> {\"avg embedded input\".ljust(19)} -> output')\n",
    "print(f'{\"-\"*62}')\n",
    "for i in range(len(x_enc) - 1):\n",
    "    print(f'{x_enc[i].__repr__().ljust(10)} -> E_{x_enc[i].__repr__().ljust(14)} -> E_{x_enc[:i+1].__repr__().ljust(17)} -> {x_enc[i+1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a99930-257b-40ac-a72c-0774c7b48726",
   "metadata": {},
   "source": [
    "Let's discuss these transformation steps in detail, starting with the first: the creation of embedding vectors for the raw (integer) inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a47ca-768a-4583-a6a1-1350c26be976",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "emb_depth = 3\n",
    "\n",
    "Emb = nn.Embedding(num_embeddings=len_vocab, embedding_dim=emb_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c639d8b0-6ae0-40ea-9fd8-728139832ef4",
   "metadata": {},
   "source": [
    "With the help of this embedding layer, the inputs now look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a10607-5212-43d7-bde4-b76a95fe8d78",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2531, -2.1761, -0.8211],\n",
       "        [-1.1887, -0.3921,  0.7249],\n",
       "        [-0.3795,  0.0908,  0.8146]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs = Emb(torch.tensor(x_enc[:-1]))\n",
    "embs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d018c97c-4d37-49a2-bcb0-aa117d0f78cf",
   "metadata": {},
   "source": [
    "In other words, we have the mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e53a1a1-04b5-46db-ba84-0430a9daf7c3",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw input     -> embedding vector\n",
      "-------------------------------------------\n",
      "81            -> [-0.2531, -2.1761, -0.8211]\n",
      "14            -> [-1.1887, -0.3921, 0.7249]\n",
      "3             -> [-0.3795, 0.0908, 0.8146]\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "print(f'{\"raw input\".ljust(13)} -> embedding vector')\n",
    "print(f'{\"-\"*43}')\n",
    "for i, v in zip(x_enc, embs):\n",
    "    print(f'{i.__repr__().ljust(13)} -> {[round(i,4) for i in v.tolist()]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d8e743-3a3d-4cbb-9e81-9f378fe746e2",
   "metadata": {},
   "source": [
    "Now that we have a 2d tensor in which the individual embeddings are stacked vertically, let's move to the second transformation: we want to create a new tensor that represents the *running average of all the embeddings in the rows that came before it*. In other words, we now want a vertically stacked tensor consisting of: `[E_[81], E_[81, 14], E_[81, 14, 3]]`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3a15e0-057b-48a1-8c0e-5d3b30e50524",
   "metadata": {},
   "source": [
    "`E_[81]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b97615-0994-42fb-9340-d7bcbfbae549",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2531, -2.1761, -0.8211])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "embs[0].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc488134-65fb-4f39-9e09-cf5d84aa157b",
   "metadata": {},
   "source": [
    "`E_[81, 14]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc70bb1-7d96-456f-8380-9635d3cfc6fd",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7209, -1.2841, -0.0481])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "embs[0:2].mean(dim=0).data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d0259c-8037-4e49-9712-04fed284c1e4",
   "metadata": {},
   "source": [
    "`E_[81, 14, 3]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec6e6bd-8559-48c5-91f8-6fae5481d48f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6071, -0.8258,  0.2395])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "embs[0:3].mean(dim=0).data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2762a6e7-7d02-4c37-a714-24f756e169c5",
   "metadata": {},
   "source": [
    "And the full tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be747a32-e2f9-45fe-913d-ee7c9cec76c4",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2531, -2.1761, -0.8211],\n",
       "        [-0.7209, -1.2841, -0.0481],\n",
       "        [-0.6071, -0.8258,  0.2395]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "torch.stack([embs[0], embs[:-1].mean(dim=0), embs.mean(dim=0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f2e9c1-a1be-4d6f-a162-988ad9fd0fa5",
   "metadata": {},
   "source": [
    "If you open the collapsed code-blocks above, you see that the logic is implemented by calling `.mean()` on a subset of the rows. Let's see how we can get the same result, by using matrix multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3958b77-1290-4190-b165-e192409330a8",
   "metadata": {},
   "source": [
    "## Summation and averaging of vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdee0078-170a-4e2d-a567-a37f67368fb9",
   "metadata": {},
   "source": [
    "Since *an average* involves taking the sum followed by a normalization, let's start with the summation.\n",
    "\n",
    "Instead of calling `.sum()` on a vector, we can do a matrix multiplication with a vector consisting of ones (1's). This is essentially a *dot-product* of the two vectors, and since one of the vectors is filled with 1's we are just simply summing over the elements in the other vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425eabbd-5f8a-4ac1-9500-8de3e2cad1b1",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = torch.tensor(range(5), dtype=torch.float32)\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef10f16-b2b0-40a2-a46b-d4a58ab291ac",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_op = torch.ones(5)\n",
    "sum_op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df6c4df-2cdc-45ca-b0ba-f021d268e25e",
   "metadata": {},
   "source": [
    "The matrix multiplication `arr @ sum_op` (dot-product) between these vectors evaluates as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63492aee-0ba6-4e0a-8235-3c0627b57992",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot product between vectors:\n",
      "[0, 1, 2, 3, 4] and [1, 1, 1, 1, 1]:\n",
      "\n",
      "(0 x 1) + (1 x 1) + (2 x 1) + (3 x 1) + (4 x 1) = 10\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "print(\"dot product between vectors:\")\n",
    "print(f'{[round(i) for i in arr.tolist()]} and {[round(i) for i in sum_op.tolist()]}:', end='\\n\\n')\n",
    "out = []\n",
    "for weight, value in zip(sum_op, arr):\n",
    "    out.append(f'({value:.0f} x {weight:.0f})')\n",
    "print(\" + \".join(out) + f' = {arr@sum_op:.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e43af-95cd-4c3c-a48c-2e02e3d21af0",
   "metadata": {},
   "source": [
    "With this sum, we compute the average by dividing through the number of elements $n$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05caa8a5-8f44-49f2-8c77-3901b1f0fd65",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of array: 2\n"
     ]
    }
   ],
   "source": [
    "average = (arr @ sum_op) / sum_op.sum()\n",
    "print(f'Average of array: {average:.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b206c037-a73a-4c59-98c4-b137803f45a3",
   "metadata": {},
   "source": [
    "Equivalently, we could also pull the division into the `sum_op` itself, by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc93202f-80e3-45e8-afa8-66941d1869ac",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_op = sum_op / sum_op.sum()\n",
    "avg_op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0775a2-6839-4f5a-ba79-6c8b4eb7ea66",
   "metadata": {},
   "source": [
    "The matrix multiplication `arr @ avg_op` (dot-product) between these vectors evaluates as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32348828-eb9b-4cb6-b98c-8a2e337fc6e5",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot product between vectors:\n",
      "[0, 1, 2, 3, 4] and [0.2, 0.2, 0.2, 0.2, 0.2]:\n",
      "\n",
      "(0 x 0.2) + (1 x 0.2) + (2 x 0.2) + (3 x 0.2) + (4 x 0.2) = 2\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "print(\"dot product between vectors:\")\n",
    "print(f'{[round(i) for i in arr.tolist()]} and {[round(i,1) for i in avg_op.tolist()]}:', end='\\n\\n')\n",
    "out = []\n",
    "for weight, value in zip(avg_op, arr):\n",
    "    out.append(f'({value:.0f} x {weight:.1f})')\n",
    "print(\" + \".join(out) + f' = {arr@avg_op:.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b45bf7-e5a5-4609-ae54-6c79006c4793",
   "metadata": {},
   "source": [
    "This second formulation (`avg_op`) gives rise to the concept of *weighted averages*, which would occur when we manipulate the values in the `avg_op` and thereby giving a different *weight* to each item in the array. When doing so, we of course need to make sure the values sum to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84bc7eb-ef63-495e-b923-3447be70a78e",
   "metadata": {},
   "source": [
    "## Summation of matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef9020a-a357-4828-a526-aa888d202866",
   "metadata": {},
   "source": [
    "To take the sum over matrices we can do something similar, but depending on the multiplication order and the shape of the matrix we can either sum over the *columns* or over the *rows*. Let's start with a matrix over which we want to compute the sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4270f2-7d5a-4d74-b96c-a46afc2c5676",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [2., 3.],\n",
       "        [4., 5.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = torch.tensor(range(0,6), dtype=torch.float32).view(3,2)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408b7c16-e1fe-4c1b-935a-690ae4b054ed",
   "metadata": {},
   "source": [
    "And let's define the following `column_sum` matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efafadd-be3e-4fae-8acd-8776385345a4",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "column_sum = torch.ones(3,3)\n",
    "column_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc339639-e357-4cba-978e-ae64ea7182b2",
   "metadata": {},
   "source": [
    "The matrix multiplication `column_sum @ matrix` results in a matrix containing the column sum in *each row*, also note that the matrix has the same shape as the input matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63851b2-0f83-4662-9a3c-3145b12e832a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6., 9.],\n",
       "        [6., 9.],\n",
       "        [6., 9.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "column_sum @ matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0669999-52e9-4e66-9fb2-54775b8aa1e0",
   "metadata": {},
   "source": [
    "If we multiply instead with the following matrix, we can take the sum over the rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87dae5c-39f1-4ab5-8827-6e4e1a3dd8fb",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "row_sum = torch.ones(2,2)\n",
    "row_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ecfaba-62ab-40f8-a810-4fd95eba71db",
   "metadata": {},
   "source": [
    "This time, the output matrix of `matrix @ row_sum` will hold the row sum in *each column*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec19e7b-aef3-45f6-8dc0-6f8a490a11cd",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [5., 5.],\n",
       "        [9., 9.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "matrix @ row_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835d1ac1-6939-408f-bc9e-1fbba4a94b92",
   "metadata": {},
   "source": [
    "Since we eventually want to average the embeddings (which are stacked vertically) column by column, we will need the `column_sum`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920d80eb-1299-4050-9c26-2cf01ac0b6a5",
   "metadata": {},
   "source": [
    "Next, we need to make sure that the first sample (row) only averages over the first row, the second sample (row) averages over the first and second row and the third sample (row) averages over the first three rows.\n",
    "\n",
    "To achieve that, we can make use of the following matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85d923a-1844-4851-b330-2f35c646cfb7",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_up_to = torch.tril(column_sum)\n",
    "sum_up_to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c641d3d8-074e-48fe-9b4e-2dea212b52bc",
   "metadata": {},
   "source": [
    "This matrix is called a *lower triangular matrix*, as you can see all the values to the right of the diagonal are set to 0. When we multiply this with our matrix we get exactly what we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdde927f-2136-4ad3-8065-8c4e0ce17b13",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [2., 4.],\n",
       "        [6., 9.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_sum_up_to = sum_up_to @ matrix\n",
    "matrix_sum_up_to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb81f3-7b4f-4430-b7e4-c3966d53bc92",
   "metadata": {},
   "source": [
    "![matrix multiplication in detail](sum_up_to.png){width=600}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cc50de-0486-4929-b8e1-715c27e4692b",
   "metadata": {},
   "source": [
    "In the diagram above, we look at the definition of the yellow highlighted cell. We see that multiplying the matrix this lower triangular matrix is indeed doing exactly what we need: it takes the sum up to the second row, because of the zero value in cell `a23`, we dont add the value in the third row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf1130f-bea5-46f8-904b-ed04bcb0dc40",
   "metadata": {},
   "source": [
    "## Averaging of matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aee73e-c9e2-42de-987d-97dddb4bdb78",
   "metadata": {},
   "source": [
    "To go from sums to averages, we can do the same trick as what we did with the vectors: division by the row sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8381bf-8270-4272-baa4-557dcbe5f949",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_up_to = sum_up_to / sum_up_to.sum(dim=1, keepdims=True)\n",
    "avg_up_to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331b8a83-5945-4ac2-b775-398cdbb306d6",
   "metadata": {},
   "source": [
    "As you can see, the rows still sum up to 1, and the non-zero weights are all equal: this means that we are now taking an average instead of a sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd1d85a-9fc4-481a-b171-f4e6839be4e4",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [1., 2.],\n",
       "        [2., 3.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_avg_up_to = avg_up_to @ matrix\n",
    "matrix_avg_up_to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62903871-aea9-4655-aa7e-915dc9c0f5ab",
   "metadata": {},
   "source": [
    "## Another way of creating an averaging operator matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5027f8aa-8da7-4893-9678-d70307266b29",
   "metadata": {},
   "source": [
    "Above we did some matrix gymnastics to understand how we can use matrix multiplication for the purpose of summation and averaging. Most notably, we saw how we can use a lower triangular matrix, to create *running* sums and averages over the rows of a matrix. For auto-regressive problems this is important, since we want to only use information of *things that happened until now*. In other words, we don't include information from the \"future\", since that is exactly what we are trying to predict!\n",
    "\n",
    "As we will shortly see, *self-attention* doesn't just take simple averages over past embeddings, instead we want the model to be able to select *which parts of the past it finds interesting*. Mathematically this simply translates to the use of weighted averages, where the weights are going to be *data dependant*, e.g. learned by the model itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec67f73-7256-434f-a52c-ff93de020608",
   "metadata": {},
   "source": [
    "By doing so, the `avg_op` will still be lower triangular (zeros in the top right corner) to ensure the auto-regressive property but it will have non-unique weights per row and thus represent a weighted average instead of a simple average.\n",
    "\n",
    "To incorporate this, we will use another trick to create this `avg_up_to` matrix. We start with the `column_sum` matrix and fill the upper right elements with the largest negative value available: `-inf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa1c2d5-8788-4173-92a0-91f6f6165fb0",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_up_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a0be35-b275-4071-b040-9232a1522d38",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., -inf, -inf],\n",
       "        [1., 1., -inf],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.ones(3,3) # --> column_sum\n",
    "t.masked_fill_(sum_up_to == 0, -torch.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780da8a1-f69d-4b12-8847-7c08af5b7289",
   "metadata": {},
   "source": [
    "Next, we take the softmax along the row dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d718882-016c-4ed3-a2f4-eaefea990ee5",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.softmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb17e519-4588-4dd5-90fd-b16935424b83",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "This works because taking the softmax means that we exponentiate everything and then normalize. Exponentiating `-torch.inf` returns 0, so the weight is distributed among the other values, since these other values are all the same ($e^1$) we get an equal weight distribution.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7270d2-8994-432a-a2fa-ee55704e5480",
   "metadata": {},
   "source": [
    "So this results in the exact same matrix `avg_up_to` as we had before. This approach has the advantage however, that we can use a *general initial matrix*.\n",
    "\n",
    "For example, for a *general initial matrix consisting of some random values*, we get to the following weighted averages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f866e7-2c08-45c6-8560-1fa4d3c5afcc",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000],\n",
       "        [0.0954, 0.9046, 0.0000],\n",
       "        [0.4257, 0.4122, 0.1622]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(2)\n",
    "t = torch.randn(3,3)\n",
    "t.masked_fill_(sum_up_to == 0, -torch.inf).softmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b2d246-4df1-4bb0-b9b3-0e0e7c80997f",
   "metadata": {},
   "source": [
    "If we consider the values in the above weighted average matrix: for the third sample (row 3) we see that a large weight is assigned to the first and second token, and less to the last token. The intuition being that for predicting the fourth token from the first three: the information of the first two tokens are more relevant then the information of the third token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaa6f09-3f5d-45ab-a76e-c91f62516d77",
   "metadata": {},
   "source": [
    "## Affinities and the dot-product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b85e759-25bd-4762-a5d2-bf885b073851",
   "metadata": {},
   "source": [
    "The next thing we need to discuss is ofcourse this *general initial matrix*. Where is this coming from? How do we construct it? For that, we need to have another look at the dot-product. As we saw above, the dot-product between two vectors is simply an element-wise multiplication, followed by a sum. \n",
    "\n",
    "We also saw that by carefully specifying one of the two vectors involved in the dot-product, we could make sure the output is either the sum or the average of the other vector. \n",
    "\n",
    "However, in general the dot-product between 2 vectors represents something like the *affinity* between the two vectors. The reason for this, is that there is a geometric interpretation which states that the dot-product between two vectors is equal to the multiplication of the length of both vectors and the cosine of the angle $\\theta$ between the two vectors: $u \\cdot v = |u| |v| \\cos{\\theta}$\n",
    "\n",
    "The best intuition for this, comes from looking at 2D space and the following extreme cases:\n",
    "\n",
    "1. vectors: `(1,0)` and `(1,0)`. These two vectors couldn't be more equal, the dot-product equals 1. \n",
    "2. vectors: `(1,0)` and `(0,1)`. These two vectors are geometrically orthogonal to one another, their dot-product is zero because the cosine of 90 degrees is zero.\n",
    "3. vectors: `(1,0)` and `(-1,0)`. These two vectors point in opposite directions, the dot product is -1 since the cosine of 180 degrees is -1.\n",
    "\n",
    "These intuitions translate to vectors of higher dimensionality, and we thus could get affinities betweeen tokens by taking the dot-product between embedding vectors. To do so, we matrix multiply the embeddings with it's transpose, so that each embedding vector (a single row in the `embs` matrix) gets a dot-product with each other row:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdd13b5-2b72-46d2-aaf1-3f055ba5eb36",
   "metadata": {},
   "source": [
    "![Affinities from matrix multiplication](embs.png){width=700}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866d59cf-0eed-4ecc-8ab6-fff1418e89b2",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.4735,  0.5589, -0.7704],\n",
       "        [ 0.5589,  2.0921,  1.0061],\n",
       "        [-0.7704,  1.0061,  0.8159]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affinity_matrix = embs @ embs.T\n",
    "affinity_matrix.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0539a7-9479-4432-8cdf-4b3d1eed91f2",
   "metadata": {},
   "source": [
    "You might guess where this is going: *we could use the affinity matrix as the general initial matrix* which we can use as a basis for creating the weighted averages matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cd4615-2c4c-4432-9ef5-03f3afa0db02",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000],\n",
       "        [0.1775, 0.8225, 0.0000],\n",
       "        [0.0848, 0.5010, 0.4142]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_up_to = torch.tril(torch.ones(3,3))\n",
    "\n",
    "affinity_matrix.masked_fill_(sum_up_to == 0, -torch.inf).softmax(dim=1).data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff1585-c03a-4633-a860-089ec5361f16",
   "metadata": {},
   "source": [
    "And indeed, self-attention is doing almost exactly this. However, it's not computing affinities on the embeddings themselves, but on something called the *query* and the *keys*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed10a297-63b9-4cb5-ae83-81fa26e93f8d",
   "metadata": {},
   "source": [
    "## Queries, Keys and Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a785b6-e3a1-4eea-b98e-f25d63c0ae3a",
   "metadata": {},
   "source": [
    "Now that we know how affinities work, let's continue with how these concepts are implemented in self-attention. Instead of using the embedding vectors as we have done above, self-attention works with three additional vectors: the query, key and value vector. All three vectors are simple (independant) linear transformation of the embedding vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ac1c77-35af-4895-9f60-e67f30cf06a0",
   "metadata": {},
   "source": [
    "More specifically, the affinity matrix (previously computed as `embs @ embs.T`) is going to be replaced by the multiplication of the query with the key matrix. The intuition here is that the query represents *the information of a token with respect to a certain query or question* and the key vector represents *the information the tokens contains*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2c4e63-0cfc-44ae-8b98-d42e9ff71f00",
   "metadata": {},
   "source": [
    "As an example, let's say that we need to predict what follows after encountering the single word \"in\". When we just have this single word predicting the next word is extremely difficult, it could be something like a country, for example *in Spain*, or *in Italy*. Or perhaps something related to time: *in the morning* or *in the evening*. Or it could be something like *in the box* or *in the closet*. Knowing what *kind* of word comes next, all depends on the context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f223ad-0de3-43fd-9f54-c72ca31f8350",
   "metadata": {},
   "source": [
    "If we add some context it becomes much easier: \"Last year I was on vacation in  ...\". Now we immediately know that the first option (a country) is much more likely then the other two options (time and location/enclosure). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5afb78f-7337-4662-8b17-d7ce5ba65f24",
   "metadata": {},
   "source": [
    "The intuition is now, that the *query vector* of the word *in* is a representation of the question related to whether we are talking about a country, a time or a location/enclosure. And the key vectors of all the tokens represent whether they have any information on this. For example, the word \"vacation\" could emit a key that is similar to the query vector of \"in\", generating a high affinity between the tokens and thus resulting in a large weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa478921-b0fa-464f-a21e-a855496b9b32",
   "metadata": {},
   "source": [
    "The multiplication of the queries and the keys is thus what forms the affinity-matrix in self-attention. From this affinity-matrix the weighted averages matrix is computed in the way we have seen above (masking and softmax). You might expect that we then use this weighted averages matrix on our embeddings to get to weighted embeddings, but this is not the case. We don't directly average over the embeddings, instead we average the *values*. Similarly as for the query and key, this is just a linear transformation of the embedding itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ff2d85-585a-488b-a983-358e92197899",
   "metadata": {},
   "source": [
    "The last thing which concludes attention is a normalization. As we have seen in an earlier [post](https://lucasvw.github.io/posts/11_nntrain_activations/#iterative-matrix-multiplications), the distribution of activations can shrink or increase by doing matrix multiplications. In the attention module, there is a scaling of the activations $1/\\sqrt{d_k}$. This $d_k$ refers to the vector length of keys, values and queries, which is unique and is also referred to as the size of the (attention) head. after the multiplications of the keys and queries to make sure the distribution stays roughly standard normal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e6ef92-0e23-4615-9dae-2c330375a38e",
   "metadata": {},
   "source": [
    "We can now understand the full formula for masked self-attention:\n",
    "\n",
    "$$\n",
    "\\textrm{Masked Self Attention}(Q, K, V) = \\textrm{softmax} \\left( \\frac{\\textrm{mask}(QK^T)}{\\sqrt{d_k}} \\right) V\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d12029-2eef-42bd-8b3e-a1ae84330c03",
   "metadata": {},
   "source": [
    "## Masked Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bd576b-6a27-4185-b251-1bb14076ff15",
   "metadata": {},
   "source": [
    "Let's make a start with formalizing these things in code. The data we are going to work with is the same data as we discussed in the previous posts on language: a dataset consisting of a list of (person) names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c2d0e6-c7e8-4d6f-8136-ecfb549d2489",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "########### Load the data ###########\n",
    "path = Path('./data')\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "path = path / 'names.txt'\n",
    "url = 'https://raw.githubusercontent.com/karpathy/makemore/master/names.txt'\n",
    "\n",
    "# _ = urlretrieve(url, path)\n",
    "\n",
    "with open(path, 'r') as f:\n",
    "    lines = f.read().splitlines()\n",
    "    \n",
    "random.seed(42)\n",
    "random.shuffle(lines)\n",
    "\n",
    "train_size=0.8\n",
    "val_size=0.1\n",
    "\n",
    "train_lines = lines[0:int(train_size * len(lines))]\n",
    "val_lines = lines[int(train_size * len(lines)): int((train_size + val_size) * len(lines))]\n",
    "\n",
    "### Create vocabulary and mappings ###\n",
    "unique_chars = list(set(\"\".join(lines)))\n",
    "unique_chars.sort()\n",
    "vocabulary = ['.'] + unique_chars\n",
    "\n",
    "c2i = {c:i for i, c in enumerate(vocabulary)}\n",
    "i2c = {i:c for i, c in enumerate(vocabulary)}\n",
    "\n",
    "class SequentialDataset():\n",
    "    def __init__(self, lines, c2i, sequence_length):\n",
    "        self.lines = lines\n",
    "        self.c2i = c2i\n",
    "        self.sequence_length = sequence_length\n",
    "        self.shuffle_and_set_data()\n",
    "    \n",
    "    def shuffle_and_set_data(self):\n",
    "        random.shuffle(self.lines)\n",
    "        text = \".\" + \".\".join(self.lines) + \".\"\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        for i in range(0, len(text) - self.sequence_length - 1, self.sequence_length):\n",
    "            self.x.append([self.c2i[xi] for xi in text[i: i+self.sequence_length]])\n",
    "            self.y.append([self.c2i[yi] for yi in text[i+1: i+self.sequence_length+1]])\n",
    "        self.x = torch.tensor(self.x)\n",
    "        self.y = torch.tensor(self.y)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "class Shuffle(Subscriber):\n",
    "    def after_epoch(self, learn):\n",
    "        learn.dls.train.dataset.shuffle_and_set_data()\n",
    "\n",
    "def get_dls(context_length, batch_size):\n",
    "    train_ds = SequentialDataset(train_lines, c2i, context_length)\n",
    "    valid_ds = SequentialDataset(val_lines, c2i, context_length)\n",
    "        \n",
    "    train_dl = torch.utils.data.DataLoader(train_ds, shuffle=False, sampler=VerticalSampler(train_ds, batch_size), batch_size=batch_size, num_workers=4)\n",
    "    valid_dl = torch.utils.data.DataLoader(valid_ds, shuffle=False, sampler=VerticalSampler(valid_ds, batch_size), batch_size=batch_size, num_workers=4)\n",
    "\n",
    "    return DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59915f0c-7423-4204-874f-034a19c25291",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "sequence_length = 5\n",
    "batch_size      = 8\n",
    "dls = get_dls(sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f856f280-704b-4dee-ad17-38e577606b3c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 5])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = next(iter(dls.train))\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9ffbb8-4fbe-4b7d-8ff7-a6adaea92cf4",
   "metadata": {},
   "source": [
    "The samples are thus in the same structure as we used for the RNN (and LSTM) models:\n",
    "\n",
    "1. We have two dimensions in our training data: the first one being the batch dimension `B` and the second refers to a sequence of characters `T`. The `T` stands here for timestep, which is a strange name but is used for historical reasons\n",
    "2. The previous discussion on computing the rolling average over tensors only mentioned one of these dimensions: the sequence length dimension\n",
    "3. Don't be confused by the batch dimension, it simply means we are processing each items in the batch in parallel. All the items in the batch are processed independantly from one another\n",
    "3. We will see in the code, we need to be careful with these dimensions, to make sure the broadcasting rules of matrix multiplications are correctly applied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cadd5d-e5e9-4734-96ab-e3cf0757ec0f",
   "metadata": {},
   "source": [
    "Let's create a `MaskedSelfAttentionHead` module. We initialize this module by creating the linear layers by which we can create the query, key and value vectors from our embeddings. Remind yourself that a linear layer applied to an embedding vector is simply a linear transformation of that vector into a new space (of dimension `head_size`).\n",
    "\n",
    "We will also initialize the `mask` tensor consisting of a lower triangular matrix as a *buffer*. This makes sure that PyTorch will move this tensor to the appropriate device when moving the module, eventhough it's not treated as a module parameter (it's values are not optimized during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28774665-c361-4df9-b4f6-0c9b1bedfc8f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class MaskedSelfAttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_depth, head_size, sequence_length):\n",
    "        super().__init__()\n",
    "        self.queries = nn.Linear(emb_depth, head_size, bias=False)\n",
    "        self.keys    = nn.Linear(emb_depth, head_size, bias=False)\n",
    "        self.values  = nn.Linear(emb_depth, head_size, bias=False)\n",
    "        \n",
    "        self.register_buffer('mask', torch.tril(torch.ones(sequence_length, sequence_length)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b6209f-cf65-4426-b804-797c6fc2d3b9",
   "metadata": {},
   "source": [
    "Next, let's have a closer look at the `forward()` method of this module, starting with the argument: the data passed to this method consists of a *batch of embedding data*. We already saw that the raw input data is of shape `[B, T]`, once this data goes through an embedding layer it will be of shape: `[B, T, Ce]`, where `Ce` reflects the embedding depth.\n",
    "\n",
    "From this data, we first compute the queries, keys and value vectors. Because of the shape of the linear layers, these three tensors are now of shape `[B, T, Ch]`.\n",
    "\n",
    "As discussed, we want to compute the affinities between the queries and the keys which in the *single batch* case was computed as $QK^T$. Since we now also have a batch dimension, we only transpose the last two dimensions and the matrix multiplication becomes a *batched matrix multiplication*.\n",
    "\n",
    "These affinities are scaled, masked and softmaxed so that they represent weights. And finally the weights are applied to the batched values vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498d8aba-2316-451b-8c64-f8bba494ff48",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "@fc.patch()\n",
    "def forward(self:MaskedSelfAttentionHead, x_emb):  # x_emb: [B, T, Ce]\n",
    "    q = self.queries(x_emb)  # [B,T,Ch]\n",
    "    k = self.keys(x_emb)     # [B,T,Ch]\n",
    "    v = self.values(x_emb)   # [B,T,Ch]\n",
    "\n",
    "    affinity = q @ k.transpose(-2, -1)  # [B,T,Ch] @ [B, Ch, T] -> [B, T, T]\n",
    "    head_size = q.shape[-1]\n",
    "    scaled_affinity = affinity / head_size**0.5\n",
    "    masked_scaled_affinity = scaled_affinity.masked_fill_(self.mask == 0, -torch.inf)\n",
    "    masked_weights = masked_scaled_affinity.softmax(dim=-1)\n",
    "\n",
    "    masked_self_attention = masked_weights @ v # [B, T, T] @ [B,T,Ch] -> [B,T,Ch]\n",
    "\n",
    "    return masked_self_attention # [B,T,Ch]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1065d6d-6916-4456-b178-d602bacb405c",
   "metadata": {},
   "source": [
    "Our first transformer module is then very straight forward. Firstly, it will create embeddings for our input data, next it will compute the self-attention as we just discussed, and finally it will pass the self attention to a so-called *head*. This head is a simple linear layer which maps the activations to the right output size (the size of the vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae32025-f92a-4381-8c9d-3674620418c5",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_depth, head_size, sequence_length):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(len(c2i), emb_depth)\n",
    "        self.attention = MaskedSelfAttentionHead(emb_depth, head_size, sequence_length)\n",
    "        self.head = nn.Linear(head_size, len(c2i))\n",
    "        \n",
    "    def forward(self, x):           # x: [B, T]\n",
    "        x = self.token_embedding(x) # [B,T,Ce]\n",
    "        x = self.attention(x)       # [B,T,Ch]\n",
    "        x = self.head(x)            # [B,T,vocab]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b72389e",
   "metadata": {},
   "source": [
    "And let's see how this model trains on our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad863b5-8ec7-48a3-893f-47f8f5f16074",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>mode</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>2.813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>2.604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>2.515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>2.492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "      <td>2.479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train parameters\n",
    "epochs = 5\n",
    "lr     = 1e-1\n",
    "bs     = 1000\n",
    "\n",
    "# Hyperparameters\n",
    "sequence_length = 10\n",
    "emb_depth       = 64\n",
    "head_size       = 64\n",
    "\n",
    "dls = get_dls(sequence_length, bs)\n",
    "model = Transformer(emb_depth, head_size, sequence_length)\n",
    "\n",
    "subs = [ProgressS(True),\n",
    "        MetricsS(),\n",
    "        DeviceS(device),\n",
    "        Shuffle()]\n",
    "\n",
    "scheduler = partial(torch.optim.lr_scheduler.OneCycleLR, max_lr=lr, epochs=epochs, steps_per_epoch=len(dls.train))\n",
    "\n",
    "l = Learner(model, dls, multi_output_cross_entropy, torch.optim.Adam, None, subs=subs +[SchedulerS(scheduler)])\n",
    "l.fit(epochs, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69dd036-0e24-4e0a-8740-127059724179",
   "metadata": {},
   "source": [
    "The model seems to be training (the loss is going down), but it's not performing very well. Let's thus continue adding other components that are described in the paper:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cce541a-ecc5-4886-abd7-df35ef33bf85",
   "metadata": {},
   "source": [
    "## Positional embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1416bbac-28aa-4f43-a243-85300ac76997",
   "metadata": {},
   "source": [
    "So far we created embeddings for our tokens and used them as the basis for the query, key and value vectors. However, in doing so we never explicitly encoded the *positional* information in the embeddings: we don't have any mechanism of working with the position of the tokens. Consider the sentence: *I was working at home when my cat ...*. Positional information is important to encode the fact that *cat* was mentioned after *I*. This way, the model might know that the words that will follow, will have to relate to my cat and not to me. There are multiple ways of encoding this positional information, but for now let's just stick with another embedding layer which will simply embed the position of each token in the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acba826a-8ef1-446c-b271-1fa890920b5a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_depth, head_size, sequence_length):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(len(c2i), emb_depth)\n",
    "        self.position_embedding = nn.Embedding(sequence_length, emb_depth)\n",
    "        self.register_buffer('range', torch.arange(sequence_length))\n",
    "        self.attention = MaskedSelfAttentionHead(emb_depth, head_size, sequence_length)\n",
    "        self.head = nn.Linear(head_size, len(c2i))\n",
    "        \n",
    "    def forward(self, x):                     # x: [B,T]\n",
    "        x_tok = self.token_embedding(x)       # [B,T,Ce]\n",
    "        x_pos = self.position_embedding(self.range) # [B,T,Ce]\n",
    "        x = self.attention(x_tok + x_pos)     # [B,T,Ch]\n",
    "        x = self.head(x)                      # [B,T,vocab]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d038a565",
   "metadata": {},
   "source": [
    "Note that the positional and token embedding are simply added together element-wise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36298d06-55b5-48c4-aaca-baa72c326175",
   "metadata": {},
   "source": [
    "## Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f77dcbb-b0ff-473b-9d37-8de7b5f44cf0",
   "metadata": {},
   "source": [
    "When introducing self-attention, we discussed one specific query that the token could be interested in, we computed the affinity of that query with all the keys of the tokens. It seems natural to assume that there are multiple queries to be formulated which would help with the prediction of the next token (What is the subject of the sentence? What is the tense? etc). Each possible query could naturally have an affinity with different tokens. These considerations give rise to *multi-head attention*. Multi-head attention is simply the application of multiple attention-heads in parallel and the concatentation of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113bf9f7-f976-4b19-ba40-4b403472cc01",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_depth, head_size, n_heads, sequence_length):\n",
    "        super().__init__()\n",
    "        hs = head_size // n_heads\n",
    "        if head_size % n_heads != 0:\n",
    "            raise ValueError(\"head_size and n_heads don't match, make sure that head_size is divisible (without remainder) through n_heads\")\n",
    "        self.heads = nn.ModuleList([MaskedSelfAttentionHead(emb_depth, hs, sequence_length) for i in range(n_heads)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94419fe-d22b-446d-abdf-20f0addacf12",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_depth, head_size, n_heads, sequence_length):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(len(c2i), emb_depth)\n",
    "        self.position_embedding = nn.Embedding(sequence_length, emb_depth)\n",
    "        self.register_buffer('range', torch.arange(sequence_length))\n",
    "        self.attention = MultiHeadAttention(emb_depth, head_size, sequence_length)\n",
    "        self.head = nn.Linear(head_size, len(c2i))\n",
    "        \n",
    "    def forward(self, x):                     # x: [B,T]\n",
    "        x_tok = self.token_embedding(x)       # [B,T,Ce]\n",
    "        x_pos = self.position_embedding(self.range) # [B,T,Ce]\n",
    "        x = self.attention(x_tok + x_pos)     # [B,T,Ch]\n",
    "        x = self.head(x)                      # [B,T,vocab]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb33a9b-dc7c-404f-ae13-c143321c16b8",
   "metadata": {},
   "source": [
    "## Multiple transformer blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befdc4e1-fff6-4c83-bcd5-3cde32a60a9b",
   "metadata": {},
   "source": [
    "The next improvement has to do with adding computation following the attention. Currently we are doing one layer of self-attention, followed by the head which maps the activations to the correct number of outputs (the size of the vocabulary). We want to change this by adding a feed forward network right after the attention module. The idea being that we want to add computation which allows the transformer to *think* upon the results from an attention block. As mentioned, this will just be a simple Linear layer followed by a ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf9e76f-225e-4eab-b65b-97e0243932d9",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, emb_depth):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(*[\n",
    "            nn.Linear(emb_depth, emb_depth),\n",
    "            nn.ReLU(),\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177902bf-154c-4bf8-9ffc-839597f71760",
   "metadata": {},
   "source": [
    "And we create a `TransformerBlock` which combines the attention and computation phase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e3027d-1fb7-4c39-9e17-98f565ac5914",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_depth, n_heads, sequence_length):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(emb_depth, emb_depth, n_heads, sequence_length)\n",
    "        self.comp = FeedForward(emb_depth)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        att = self.att(x)\n",
    "        comp = self.comp(att)\n",
    "        return comp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b5c518-e085-4c7e-ae44-0d52a5fb551b",
   "metadata": {},
   "source": [
    "We can now add another parameter `n_blocks` signifying how many of these sequential transformer blocks we want to have in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f4ffb1-2303-461d-9ede-52da5bcc597d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_depth, n_heads, n_blocks, sequence_length):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(len(c2i), emb_depth)\n",
    "        self.position_embedding = nn.Embedding(sequence_length, emb_depth)\n",
    "        self.register_buffer('range', torch.arange(sequence_length))\n",
    "        self.tblocks = nn.Sequential(*[TransformerBlock(emb_depth, n_heads, sequence_length) for i in range(n_blocks)])\n",
    "        self.head = nn.Linear(emb_depth, len(c2i))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.token_embedding(x) + self.position_embedding(self.range)\n",
    "        x = self.tblocks(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c1f931-4f9c-44f5-9617-c2e4a5487f11",
   "metadata": {},
   "source": [
    "## Layernorm and Residual Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f4238a-3f79-437e-83fd-01a776af8331",
   "metadata": {},
   "source": [
    "And finally we will add the skip connections and layer normalization. The way these are added is somewhat different from the paper. After the paper, one of the few improvements that were made on this architecture was the use of something called Pre-LN transformer, in which two layernorms are applied in the residual path before the attention and the computation respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2c69b6-8a08-4932-bff9-98646086ff4f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_depth, n_heads, sequence_length):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(emb_depth, emb_depth, n_heads, sequence_length)\n",
    "        self.comp = FeedForward(emb_depth)\n",
    "        self.ln1 = nn.LayerNorm(emb_depth)\n",
    "        self.ln2 = nn.LayerNorm(emb_depth)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        att = x + self.att(self.ln1(x))\n",
    "        comp = x + self.comp(self.ln1(att))\n",
    "        return comp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bb6b85",
   "metadata": {},
   "source": [
    "Let's put it all together now and train the model on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352fcbb1-be50-4dc4-8f5c-10829476595b",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>mode</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>2.654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>2.371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>2.291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>2.243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "      <td>2.217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>2.203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>train</td>\n",
       "      <td>2.189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>train</td>\n",
       "      <td>2.173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "      <td>2.161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>train</td>\n",
       "      <td>2.147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>train</td>\n",
       "      <td>2.135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>train</td>\n",
       "      <td>2.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>train</td>\n",
       "      <td>2.111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>train</td>\n",
       "      <td>2.101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>train</td>\n",
       "      <td>2.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>2.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>train</td>\n",
       "      <td>2.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>train</td>\n",
       "      <td>2.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>train</td>\n",
       "      <td>2.043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>train</td>\n",
       "      <td>2.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>train</td>\n",
       "      <td>2.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>train</td>\n",
       "      <td>2.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>train</td>\n",
       "      <td>1.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>train</td>\n",
       "      <td>1.978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>train</td>\n",
       "      <td>1.966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>train</td>\n",
       "      <td>1.956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>train</td>\n",
       "      <td>1.947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>train</td>\n",
       "      <td>1.940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>train</td>\n",
       "      <td>1.937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>train</td>\n",
       "      <td>1.934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>eval</td>\n",
       "      <td>2.024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train parameters\n",
    "epochs = 30\n",
    "lr     = 1e-2\n",
    "bs     = 16\n",
    "\n",
    "# Hyperparameters\n",
    "sequence_length = 32\n",
    "emb_depth       = 64\n",
    "n_heads         = 4\n",
    "n_blocks        = 4\n",
    "\n",
    "dls = get_dls(sequence_length, bs)\n",
    "model = Transformer(emb_depth, n_heads, n_blocks, sequence_length)\n",
    "\n",
    "subs = [ProgressS(True),\n",
    "        MetricsS(),\n",
    "        Shuffle(),\n",
    "        DeviceS(device)]\n",
    "\n",
    "\n",
    "scheduler = partial(torch.optim.lr_scheduler.OneCycleLR, max_lr=lr, epochs=epochs, steps_per_epoch=len(dls.train))\n",
    "\n",
    "l = Learner(model, dls, multi_output_cross_entropy, torch.optim.Adam, None, subs=subs + [SchedulerS(scheduler)])\n",
    "l.fit(epochs, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca9667a-8358-4af5-8594-1444a691c119",
   "metadata": {},
   "source": [
    "## Sampling names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06925e46-f378-4932-b210-8373b1423b24",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "@fc.patch()\n",
    "def generate(self:Transformer, n_tokens=100):\n",
    "    idx = torch.zeros(1,sequence_length, dtype=torch.long).to(device) # B,T\n",
    "    for _ in range(n_tokens):\n",
    "        idx_s = idx[:, -sequence_length:]                  # B,T\n",
    "        logits = self(idx_s)                               # B,T,C\n",
    "        last_pred = logits[:,-1,:]                         # B,C\n",
    "        probs = F.softmax(last_pred, dim=-1)               # B,C\n",
    "        idx_next = torch.multinomial(probs, num_samples=1) # B,1\n",
    "        idx = torch.cat((idx, idx_next), dim=1)            # B,T+1\n",
    "    l = idx[0,sequence_length:].tolist()\n",
    "    return \"\".join([i2c[i] for i in l]).split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b681fd7c-fed9-4bcb-92f2-5bd70927fa6b",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['enthani',\n",
       " 'walynn',\n",
       " 'carkton',\n",
       " 'sulon',\n",
       " 'kanias',\n",
       " 'amour',\n",
       " 'katy',\n",
       " 'ayama',\n",
       " 'sylen',\n",
       " 'abexten',\n",
       " 'derriz',\n",
       " 'kacey',\n",
       " 'vonssen',\n",
       " 'tryver',\n",
       " 'amaru']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2dfade-2489-4017-84ad-391014475649",
   "metadata": {},
   "source": [
    "## Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62495d9d",
   "metadata": {},
   "source": [
    "The model is training fairly well, but it seems difficult to get the (validation) loss to the same level as what we saw for the [LSTM](https://lucasvw.github.io/posts/16_lstm/#pytorch-lstm) which was around 1.95. This bugged me for a bit, but then I realised this is comparing apples to oranges. The LSTM (and RNN) architecture, and more precisely the way we load the data and deal with passing the hidden state from batch to batch (see [here](https://lucasvw.github.io/posts/15_rnn/#optimizing-dataloading-and-model-for-sequential-data-bptt)), means that even for the first item in the sequence (`T=1`) we have information on the history from the previous batch. This is not the case for Transformers, where each batch starts out *fresh*, without any memory. \n",
    "\n",
    "I ran a small test to verify that this is indeed causing the difference in observed loss. I created a new loss function, which (during evaluation) only computes the loss on the last item in the sequence. I re-trained the transformer with a sequence length equal to what was used for the LSTM (5) and obtained a validation loss of 1.933. This shows indeed that a Transformer architecture is outperforming the LSTM on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f47fcc-11fb-4ab4-b428-d1f202a09517",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "Let's wrap-up by looking at the Transformer architecture diagram from the original paper. In this blog-post we have described the decoder-only variant of this architecture and only encompasses the right side of this diagram. The left side of the diagram is the encoder part and plays an important role in tasks which involve sequence to sequence problems, for example when translating sentences from English to German. As you can see, in that case the queries from the decoder are combined with the values and key vectors of the encoder. This allows the model to efficiently work with information from the input sequence.\n",
    "\n",
    "![Transformer architecture diagram from *Attention is all you need* by Vaswani et al](attention.png){width=400}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c9511e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
