<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Lucas van Walstijn">
<meta name="dcterms.date" content="2023-11-03">

<title>Lucas van Walstijn - Transformers</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../profile.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="../../site_libs/quarto-contrib/iconify-1.0.0-beta.2/iconify-icon.min.js"></script>
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Lucas van Walstijn</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/lucasvw" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/lvWal" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/lucasvanwalstijn/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://www.kaggle.com/lucasvw" rel="" target="">
 <span class="menu-text"><iconify-icon inline="" icon="fa6-brands:kaggle"></iconify-icon></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://lucasvw.github.io/index.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#summation-and-averaging-of-vectors" id="toc-summation-and-averaging-of-vectors" class="nav-link active" data-scroll-target="#summation-and-averaging-of-vectors">Summation and averaging of vectors</a></li>
  <li><a href="#summation-of-matrices" id="toc-summation-of-matrices" class="nav-link" data-scroll-target="#summation-of-matrices">Summation of matrices</a></li>
  <li><a href="#averaging-of-matrices" id="toc-averaging-of-matrices" class="nav-link" data-scroll-target="#averaging-of-matrices">Averaging of matrices</a></li>
  <li><a href="#another-way-of-creating-an-averaging-operator-matrix" id="toc-another-way-of-creating-an-averaging-operator-matrix" class="nav-link" data-scroll-target="#another-way-of-creating-an-averaging-operator-matrix">Another way of creating an averaging operator matrix</a></li>
  <li><a href="#affinities-and-the-dot-product" id="toc-affinities-and-the-dot-product" class="nav-link" data-scroll-target="#affinities-and-the-dot-product">Affinities and the dot-product</a></li>
  <li><a href="#queries-keys-and-values" id="toc-queries-keys-and-values" class="nav-link" data-scroll-target="#queries-keys-and-values">Queries, Keys and Values</a></li>
  <li><a href="#masked-self-attention" id="toc-masked-self-attention" class="nav-link" data-scroll-target="#masked-self-attention">Masked Self Attention</a></li>
  <li><a href="#positional-embeddings" id="toc-positional-embeddings" class="nav-link" data-scroll-target="#positional-embeddings">Positional embeddings</a></li>
  <li><a href="#multi-head-attention" id="toc-multi-head-attention" class="nav-link" data-scroll-target="#multi-head-attention">Multi-head attention</a></li>
  <li><a href="#multiple-transformer-blocks" id="toc-multiple-transformer-blocks" class="nav-link" data-scroll-target="#multiple-transformer-blocks">Multiple transformer blocks</a></li>
  <li><a href="#layernorm-and-residual-connections" id="toc-layernorm-and-residual-connections" class="nav-link" data-scroll-target="#layernorm-and-residual-connections">Layernorm and Residual Connections</a></li>
  <li><a href="#sampling-names" id="toc-sampling-names" class="nav-link" data-scroll-target="#sampling-names">Sampling names</a></li>
  <li><a href="#remarks" id="toc-remarks" class="nav-link" data-scroll-target="#remarks">Remarks</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Transformers</h1>
  <div class="quarto-categories">
    <div class="quarto-category">nlp</div>
    <div class="quarto-category">transformers</div>
    <div class="quarto-category">attention</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Lucas van Walstijn </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 3, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> <span class="bu">reduce</span>, partial</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> urllib.request <span class="im">import</span> urlretrieve</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torcheval.metrics <span class="im">as</span> tem</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> fastcore.<span class="bu">all</span> <span class="im">as</span> fc</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nntrain.dataloaders <span class="im">import</span> DataLoaders</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nntrain.learner <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nntrain.activations <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nntrain.acceleration <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nntrain.rnn <span class="im">import</span> <span class="op">*</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Transformers were introduced in the paper <em>Attention is all you need</em> in 2017 by Vaswani et al, and as the title suggests focusses heavily on a something called <em>attention</em>. This architecture is still very much in use today and is used in text applications applications like ChatGPT, but also in other fields such as vision, audio and other sequence based fields such as protein folding.</p>
<p>It’s an extremely important architecture, and in this post I would like to investigate how it works. Let’s discuss the transformer architecture in terms of a word level model trying to predict the next word in a sequence. With a vocabulary of size 100, some tokenized sentence of the training set might look like this:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>len_vocab <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">42</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>x_enc <span class="op">=</span> [random.randint(<span class="dv">0</span>, len_vocab<span class="op">-</span><span class="dv">1</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>)]</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>x_enc</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[81, 14, 3, 94]</code></pre>
</div>
</div>
<p>From this tokenized sequence we could naively create the following training samples:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span><span class="st">"input"</span><span class="sc">.</span>ljust(<span class="dv">5</span>)<span class="sc">}</span><span class="ss"> -&gt; output'</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span><span class="st">"-"</span><span class="op">*</span><span class="dv">15</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(x_enc) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>x_enc[i]<span class="sc">.</span><span class="fu">__repr__</span>()<span class="sc">.</span>ljust(<span class="dv">5</span>)<span class="sc">}</span><span class="ss"> -&gt; </span><span class="sc">{</span>x_enc[i<span class="op">+</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>input -&gt; output
---------------
81    -&gt; 14
14    -&gt; 3
3     -&gt; 94</code></pre>
</div>
</div>
<p>This is essentially the bigram language model, in which only the previous token is used to predict the next.</p>
<p>Now let’s consider the general case in which for each prediction we can use the full history of tokens, we then get the following samples:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span><span class="st">"input"</span><span class="sc">.</span>ljust(<span class="dv">12</span>)<span class="sc">}</span><span class="ss"> -&gt; output'</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span><span class="st">"-"</span><span class="op">*</span><span class="dv">22</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(x_enc) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>x_enc[:i<span class="op">+</span><span class="dv">1</span>]<span class="sc">.</span><span class="fu">__repr__</span>()<span class="sc">.</span>ljust(<span class="dv">12</span>)<span class="sc">}</span><span class="ss"> -&gt; </span><span class="sc">{</span>x_enc[i<span class="op">+</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>input        -&gt; output
----------------------
[81]         -&gt; 14
[81, 14]     -&gt; 3
[81, 14, 3]  -&gt; 94</code></pre>
</div>
</div>
<p>As we progress further into the sequence we can use more <em>context</em> from the sequence to predict the next token. For the first sample we just have one single token for the prediction, for the last sample we can use 3 tokens for our prediction. The question is then: <em>how do we use this varying length context in an optimal way to make the best prediction for the next word?</em></p>
<p>The initial idea we will pursue is to</p>
<ol type="1">
<li>create an embedding vector for each token</li>
<li>average the embeddings of the tokens in the context</li>
</ol>
<p>Arguably, this is a pretty rough way of dealing with the information contained in the individual embeddings. And indeed, we will see that self-attention is a bit more involved than this. However, it’s a good starting point to understand self-attention.</p>
<p>If we represent an embedding vector of token <span class="math inline">\(x\)</span> as <code>E_[x]</code> and the average of two embedding vectors <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> as <code>E_[x,y]</code> we can visualize this initial idea outlined above as follows:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span><span class="st">"raw input"</span><span class="sc">.</span>ljust(<span class="dv">10</span>)<span class="sc">}</span><span class="ss"> -&gt; </span><span class="sc">{</span><span class="st">"embedded input"</span><span class="sc">.</span>ljust(<span class="dv">16</span>)<span class="sc">}</span><span class="ss"> -&gt; </span><span class="sc">{</span><span class="st">"avg embedded input"</span><span class="sc">.</span>ljust(<span class="dv">19</span>)<span class="sc">}</span><span class="ss"> -&gt; output'</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span><span class="st">"-"</span><span class="op">*</span><span class="dv">62</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(x_enc) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>x_enc[i]<span class="sc">.</span><span class="fu">__repr__</span>()<span class="sc">.</span>ljust(<span class="dv">10</span>)<span class="sc">}</span><span class="ss"> -&gt; E_</span><span class="sc">{</span>x_enc[i]<span class="sc">.</span><span class="fu">__repr__</span>()<span class="sc">.</span>ljust(<span class="dv">14</span>)<span class="sc">}</span><span class="ss"> -&gt; E_</span><span class="sc">{</span>x_enc[:i<span class="op">+</span><span class="dv">1</span>]<span class="sc">.</span><span class="fu">__repr__</span>()<span class="sc">.</span>ljust(<span class="dv">17</span>)<span class="sc">}</span><span class="ss"> -&gt; </span><span class="sc">{</span>x_enc[i<span class="op">+</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>raw input  -&gt; embedded input   -&gt; avg embedded input  -&gt; output
--------------------------------------------------------------
81         -&gt; E_81             -&gt; E_[81]              -&gt; 14
14         -&gt; E_14             -&gt; E_[81, 14]          -&gt; 3
3          -&gt; E_3              -&gt; E_[81, 14, 3]       -&gt; 94</code></pre>
</div>
</div>
<p>Let’s discuss these transformation steps in detail, starting with the first: the creation of embedding vectors for the raw (integer) inputs.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>emb_depth <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>Emb <span class="op">=</span> nn.Embedding(num_embeddings<span class="op">=</span>len_vocab, embedding_dim<span class="op">=</span>emb_depth)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>With the help of this embedding layer, the inputs now look like this:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>embs <span class="op">=</span> Emb(torch.tensor(x_enc[:<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>embs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[-0.2531, -2.1761, -0.8211],
        [-1.1887, -0.3921,  0.7249],
        [-0.3795,  0.0908,  0.8146]], grad_fn=&lt;EmbeddingBackward0&gt;)</code></pre>
</div>
</div>
<p>In other words, we have the mapping:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span><span class="st">"raw input"</span><span class="sc">.</span>ljust(<span class="dv">13</span>)<span class="sc">}</span><span class="ss"> -&gt; embedding vector'</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span><span class="st">"-"</span><span class="op">*</span><span class="dv">43</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, v <span class="kw">in</span> <span class="bu">zip</span>(x_enc, embs):</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>i<span class="sc">.</span><span class="fu">__repr__</span>()<span class="sc">.</span>ljust(<span class="dv">13</span>)<span class="sc">}</span><span class="ss"> -&gt; </span><span class="sc">{</span>[<span class="bu">round</span>(i,<span class="dv">4</span>) <span class="cf">for</span> i <span class="kw">in</span> v.tolist()]<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>raw input     -&gt; embedding vector
-------------------------------------------
81            -&gt; [-0.2531, -2.1761, -0.8211]
14            -&gt; [-1.1887, -0.3921, 0.7249]
3             -&gt; [-0.3795, 0.0908, 0.8146]</code></pre>
</div>
</div>
<p>Now that we have a 2d tensor in which the individual embeddings are stacked vertically, let’s move to the second transformation: we want to create a new tensor that represents the <em>running average of all the embeddings in the rows that came before it</em>. In other words, we now want a vertically stacked tensor consisting of: <code>[E_[81], E_[81, 14], E_[81, 14, 3]]</code>:</p>
<p><code>E_[81]</code>:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>embs[<span class="dv">0</span>].data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<pre><code>tensor([-0.2531, -2.1761, -0.8211])</code></pre>
</div>
</div>
<p><code>E_[81, 14]</code>:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>embs[<span class="dv">0</span>:<span class="dv">2</span>].mean(dim<span class="op">=</span><span class="dv">0</span>).data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<pre><code>tensor([-0.7209, -1.2841, -0.0481])</code></pre>
</div>
</div>
<p><code>E_[81, 14, 3]</code>:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>embs[<span class="dv">0</span>:<span class="dv">3</span>].mean(dim<span class="op">=</span><span class="dv">0</span>).data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<pre><code>tensor([-0.6071, -0.8258,  0.2395])</code></pre>
</div>
</div>
<p>And the full tensor:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>torch.stack([embs[<span class="dv">0</span>], embs[:<span class="op">-</span><span class="dv">1</span>].mean(dim<span class="op">=</span><span class="dv">0</span>), embs.mean(dim<span class="op">=</span><span class="dv">0</span>)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<pre><code>tensor([[-0.2531, -2.1761, -0.8211],
        [-0.7209, -1.2841, -0.0481],
        [-0.6071, -0.8258,  0.2395]], grad_fn=&lt;StackBackward0&gt;)</code></pre>
</div>
</div>
<p>If you open the collapsed code-blocks above, you see that the logic is implemented by calling <code>.mean()</code> on a subset of the rows. Let’s see how we can get the same result, by using matrix multiplications.</p>
<section id="summation-and-averaging-of-vectors" class="level2">
<h2 class="anchored" data-anchor-id="summation-and-averaging-of-vectors">Summation and averaging of vectors</h2>
<p>Since <em>an average</em> involves taking the sum followed by a normalization, let’s start with the summation.</p>
<p>Instead of calling <code>.sum()</code> on a vector, we can do a matrix multiplication with a vector consisting of ones (1’s). This is essentially a <em>dot-product</em> of the two vectors, and since one of the vectors is filled with 1’s we are just simply summing over the elements in the other vector:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>arr <span class="op">=</span> torch.tensor(<span class="bu">range</span>(<span class="dv">5</span>), dtype<span class="op">=</span>torch.float32)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>arr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([0., 1., 2., 3., 4.])</code></pre>
</div>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>sum_op <span class="op">=</span> torch.ones(<span class="dv">5</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>sum_op</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([1., 1., 1., 1., 1.])</code></pre>
</div>
</div>
<p>The matrix multiplication <code>arr @ sum_op</code> (dot-product) between these vectors evaluates as:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"dot product between vectors:"</span>)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>[<span class="bu">round</span>(i) <span class="cf">for</span> i <span class="kw">in</span> arr.tolist()]<span class="sc">}</span><span class="ss"> and </span><span class="sc">{</span>[<span class="bu">round</span>(i) <span class="cf">for</span> i <span class="kw">in</span> sum_op.tolist()]<span class="sc">}</span><span class="ss">:'</span>, end<span class="op">=</span><span class="st">'</span><span class="ch">\n\n</span><span class="st">'</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> []</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> weight, value <span class="kw">in</span> <span class="bu">zip</span>(sum_op, arr):</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    out.append(<span class="ss">f'(</span><span class="sc">{</span>value<span class="sc">:.0f}</span><span class="ss"> x </span><span class="sc">{</span>weight<span class="sc">:.0f}</span><span class="ss">)'</span>)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" + "</span>.join(out) <span class="op">+</span> <span class="ss">f' = </span><span class="sc">{</span>arr<span class="op">@</span>sum_op<span class="sc">:.0f}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>dot product between vectors:
[0, 1, 2, 3, 4] and [1, 1, 1, 1, 1]:

(0 x 1) + (1 x 1) + (2 x 1) + (3 x 1) + (4 x 1) = 10</code></pre>
</div>
</div>
<p>With this sum, we compute the average by dividing through the number of elements <span class="math inline">\(n\)</span>:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>average <span class="op">=</span> (arr <span class="op">@</span> sum_op) <span class="op">/</span> sum_op.<span class="bu">sum</span>()</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Average of array: </span><span class="sc">{</span>average<span class="sc">:.0f}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Average of array: 2</code></pre>
</div>
</div>
<p>Equivalently, we could also pull the division into the <code>sum_op</code> itself, by doing:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>avg_op <span class="op">=</span> sum_op <span class="op">/</span> sum_op.<span class="bu">sum</span>()</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>avg_op</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])</code></pre>
</div>
</div>
<p>The matrix multiplication <code>arr @ avg_op</code> (dot-product) between these vectors evaluates as:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"dot product between vectors:"</span>)</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>[<span class="bu">round</span>(i) <span class="cf">for</span> i <span class="kw">in</span> arr.tolist()]<span class="sc">}</span><span class="ss"> and </span><span class="sc">{</span>[<span class="bu">round</span>(i,<span class="dv">1</span>) <span class="cf">for</span> i <span class="kw">in</span> avg_op.tolist()]<span class="sc">}</span><span class="ss">:'</span>, end<span class="op">=</span><span class="st">'</span><span class="ch">\n\n</span><span class="st">'</span>)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> []</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> weight, value <span class="kw">in</span> <span class="bu">zip</span>(avg_op, arr):</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>    out.append(<span class="ss">f'(</span><span class="sc">{</span>value<span class="sc">:.0f}</span><span class="ss"> x </span><span class="sc">{</span>weight<span class="sc">:.1f}</span><span class="ss">)'</span>)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" + "</span>.join(out) <span class="op">+</span> <span class="ss">f' = </span><span class="sc">{</span>arr<span class="op">@</span>avg_op<span class="sc">:.0f}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>dot product between vectors:
[0, 1, 2, 3, 4] and [0.2, 0.2, 0.2, 0.2, 0.2]:

(0 x 0.2) + (1 x 0.2) + (2 x 0.2) + (3 x 0.2) + (4 x 0.2) = 2</code></pre>
</div>
</div>
<p>This second formulation (<code>avg_op</code>) gives rise to the concept of <em>weighted averages</em>, which would occur when we manipulate the values in the <code>avg_op</code> and thereby giving a different <em>weight</em> to each item in the array. When doing so, we of course need to make sure the values sum to 1.</p>
</section>
<section id="summation-of-matrices" class="level2">
<h2 class="anchored" data-anchor-id="summation-of-matrices">Summation of matrices</h2>
<p>To take the sum over matrices we can do something similar, but depending on the multiplication order and the shape of the matrix we can either sum over the <em>columns</em> or over the <em>rows</em>. Let’s start with a matrix over which we want to compute the sum:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>matrix <span class="op">=</span> torch.tensor(<span class="bu">range</span>(<span class="dv">0</span>,<span class="dv">6</span>), dtype<span class="op">=</span>torch.float32).view(<span class="dv">3</span>,<span class="dv">2</span>)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>matrix</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[0., 1.],
        [2., 3.],
        [4., 5.]])</code></pre>
</div>
</div>
<p>And let’s define the following <code>column_sum</code> matrix:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>column_sum <span class="op">=</span> torch.ones(<span class="dv">3</span>,<span class="dv">3</span>)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>column_sum</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<pre><code>tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])</code></pre>
</div>
</div>
<p>The matrix multiplication <code>column_sum @ matrix</code> results in a matrix containing the column sum in <em>each row</em>, also note that the matrix has the same shape as the input matrix:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>column_sum <span class="op">@</span> matrix</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<pre><code>tensor([[6., 9.],
        [6., 9.],
        [6., 9.]])</code></pre>
</div>
</div>
<p>If we multiply instead with the following matrix, we can take the sum over the rows:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>row_sum <span class="op">=</span> torch.ones(<span class="dv">2</span>,<span class="dv">2</span>)</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>row_sum</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<pre><code>tensor([[1., 1.],
        [1., 1.]])</code></pre>
</div>
</div>
<p>This time, the output matrix of <code>matrix @ row_sum</code> will hold the row sum in <em>each column</em>:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>matrix <span class="op">@</span> row_sum</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<pre><code>tensor([[1., 1.],
        [5., 5.],
        [9., 9.]])</code></pre>
</div>
</div>
<p>Since we eventually want to average the embeddings (which are stacked vertically) column by column, we will need the <code>column_sum</code>.</p>
<p>Next, we need to make sure that the first sample (row) only averages over the first row, the second sample (row) averages over the first and second row and the third sample (row) averages over the first three rows.</p>
<p>To achieve that, we can make use of the following matrix:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>sum_up_to <span class="op">=</span> torch.tril(column_sum)</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>sum_up_to</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[1., 0., 0.],
        [1., 1., 0.],
        [1., 1., 1.]])</code></pre>
</div>
</div>
<p>This matrix is called a <em>lower triangular matrix</em>, as you can see all the values to the right of the diagonal are set to 0. When we multiply this with our matrix we get exactly what we need:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>matrix_sum_up_to <span class="op">=</span> sum_up_to <span class="op">@</span> matrix</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>matrix_sum_up_to</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[0., 1.],
        [2., 4.],
        [6., 9.]])</code></pre>
</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="sum_up_to.png" class="img-fluid figure-img" width="600"></p>
<figcaption class="figure-caption">matrix multiplication in detail</figcaption>
</figure>
</div>
<p>In the diagram above, we look at the definition of the yellow highlighted cell. We see that multiplying the matrix this lower triangular matrix is indeed doing exactly what we need: it takes the sum up to the second row, because of the zero value in cell <code>a23</code>, we dont add the value in the third row.</p>
</section>
<section id="averaging-of-matrices" class="level2">
<h2 class="anchored" data-anchor-id="averaging-of-matrices">Averaging of matrices</h2>
<p>To go from sums to averages, we can do the same trick as what we did with the vectors: division by the row sum:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>avg_up_to <span class="op">=</span> sum_up_to <span class="op">/</span> sum_up_to.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>avg_up_to</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[1.0000, 0.0000, 0.0000],
        [0.5000, 0.5000, 0.0000],
        [0.3333, 0.3333, 0.3333]])</code></pre>
</div>
</div>
<p>As you can see, the rows still sum up to 1, and the non-zero weights are all equal: this means that we are now taking an average instead of a sum.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>matrix_avg_up_to <span class="op">=</span> avg_up_to <span class="op">@</span> matrix</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>matrix_avg_up_to</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[0., 1.],
        [1., 2.],
        [2., 3.]])</code></pre>
</div>
</div>
</section>
<section id="another-way-of-creating-an-averaging-operator-matrix" class="level2">
<h2 class="anchored" data-anchor-id="another-way-of-creating-an-averaging-operator-matrix">Another way of creating an averaging operator matrix</h2>
<p>Above we did some matrix gymnastics to understand how we can use matrix multiplication for the purpose of summation and averaging. Most notably, we saw how we can use a lower triangular matrix, to create <em>running</em> sums and averages over the rows of a matrix. For auto-regressive problems this is important, since we want to only use information of <em>things that happened until now</em>. In other words, we don’t include information from the “future”, since that is exactly what we are trying to predict!</p>
<p>As we will shortly see, <em>self-attention</em> doesn’t just take simple averages over past embeddings, instead we want the model to be able to select <em>which parts of the past it finds interesting</em>. Mathematically this simply translates to the use of weighted averages, where the weights are going to be <em>data dependant</em>, e.g.&nbsp;learned by the model itself.</p>
<p>By doing so, the <code>avg_op</code> will still be lower triangular (zeros in the top right corner) to ensure the auto-regressive property but it will have non-unique weights per row and thus represent a weighted average instead of a simple average.</p>
<p>To incorporate this, we will use another trick to create this <code>avg_up_to</code> matrix. We start with the <code>column_sum</code> matrix and fill the upper right elements with the largest negative value available: <code>-inf</code>:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>sum_up_to</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[1., 0., 0.],
        [1., 1., 0.],
        [1., 1., 1.]])</code></pre>
</div>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> torch.ones(<span class="dv">3</span>,<span class="dv">3</span>) <span class="co"># --&gt; column_sum</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>t.masked_fill_(sum_up_to <span class="op">==</span> <span class="dv">0</span>, <span class="op">-</span>torch.inf)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[1., -inf, -inf],
        [1., 1., -inf],
        [1., 1., 1.]])</code></pre>
</div>
</div>
<p>Next, we take the softmax along the row dimension:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>t.softmax(dim<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[1.0000, 0.0000, 0.0000],
        [0.5000, 0.5000, 0.0000],
        [0.3333, 0.3333, 0.3333]])</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This works because taking the softmax means that we exponentiate everything and then normalize. Exponentiating <code>-torch.inf</code> returns 0, so the weight is distributed among the other values, since these other values are all the same (<span class="math inline">\(e^1\)</span>) we get an equal weight distribution.</p>
</div>
</div>
<p>So this results in the exact same matrix <code>avg_up_to</code> as we had before. This approach has the advantage however, that we can use a <em>general initial matrix</em>.</p>
<p>For example, for a <em>general initial matrix consisting of some random values</em>, we get to the following weighted averages:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">2</span>)</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> torch.randn(<span class="dv">3</span>,<span class="dv">3</span>)</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>t.masked_fill_(sum_up_to <span class="op">==</span> <span class="dv">0</span>, <span class="op">-</span>torch.inf).softmax(dim<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[1.0000, 0.0000, 0.0000],
        [0.0954, 0.9046, 0.0000],
        [0.4257, 0.4122, 0.1622]])</code></pre>
</div>
</div>
<p>If we consider the values in the above weighted average matrix: for the third sample (row 3) we see that a large weight is assigned to the first and second token, and less to the last token. The intuition being that for predicting the fourth token from the first three: the information of the first two tokens are more relevant then the information of the third token.</p>
</section>
<section id="affinities-and-the-dot-product" class="level2">
<h2 class="anchored" data-anchor-id="affinities-and-the-dot-product">Affinities and the dot-product</h2>
<p>The next thing we need to discuss is ofcourse this <em>general initial matrix</em>. Where is this coming from? How do we construct it? For that, we need to have another look at the dot-product. As we saw above, the dot-product between two vectors is simply an element-wise multiplication, followed by a sum.</p>
<p>We also saw that by carefully specifying one of the two vectors involved in the dot-product, we could make sure the output is either the sum or the average of the other vector.</p>
<p>However, in general the dot-product between 2 vectors represents something like the <em>affinity</em> between the two vectors. The reason for this, is that there is a geometric interpretation which states that the dot-product between two vectors is equal to the multiplication of the length of both vectors and the cosine of the angle <span class="math inline">\(\theta\)</span> between the two vectors: <span class="math inline">\(u \cdot v = |u| |v| \cos{\theta}\)</span></p>
<p>The best intuition for this, comes from looking at 2D space and the following extreme cases:</p>
<ol type="1">
<li>vectors: <code>(1,0)</code> and <code>(1,0)</code>. These two vectors couldn’t be more equal, the dot-product equals 1.</li>
<li>vectors: <code>(1,0)</code> and <code>(0,1)</code>. These two vectors are geometrically orthogonal to one another, their dot-product is zero because the cosine of 90 degrees is zero.</li>
<li>vectors: <code>(1,0)</code> and <code>(-1,0)</code>. These two vectors point in opposite directions, the dot product is -1 since the cosine of 180 degrees is -1.</li>
</ol>
<p>These intuitions translate to vectors of higher dimensionality, and we thus could get affinities betweeen tokens by taking the dot-product between embedding vectors. To do so, we matrix multiply the embeddings with it’s transpose, so that each embedding vector (a single row in the <code>embs</code> matrix) gets a dot-product with each other row:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="embs.png" class="img-fluid figure-img" width="700"></p>
<figcaption class="figure-caption">Affinities from matrix multiplication</figcaption>
</figure>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>affinity_matrix <span class="op">=</span> embs <span class="op">@</span> embs.T</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>affinity_matrix.data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[ 5.4735,  0.5589, -0.7704],
        [ 0.5589,  2.0921,  1.0061],
        [-0.7704,  1.0061,  0.8159]])</code></pre>
</div>
</div>
<p>You might guess where this is going: <em>we could use the affinity matrix as the general initial matrix</em> which we can use as a basis for creating the weighted averages matrix:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>sum_up_to <span class="op">=</span> torch.tril(torch.ones(<span class="dv">3</span>,<span class="dv">3</span>))</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>affinity_matrix.masked_fill_(sum_up_to <span class="op">==</span> <span class="dv">0</span>, <span class="op">-</span>torch.inf).softmax(dim<span class="op">=</span><span class="dv">1</span>).data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[1.0000, 0.0000, 0.0000],
        [0.1775, 0.8225, 0.0000],
        [0.0848, 0.5010, 0.4142]])</code></pre>
</div>
</div>
<p>And indeed, self-attention is doing almost exactly this. However, it’s not computing affinities on the embeddings themselves, but on something called the <em>query</em> and the <em>keys</em>.</p>
</section>
<section id="queries-keys-and-values" class="level2">
<h2 class="anchored" data-anchor-id="queries-keys-and-values">Queries, Keys and Values</h2>
<p>Now that we know how affinities work, let’s continue with how these concepts are implemented in self-attention. Instead of using the embedding vectors as we have done above, self-attention works with three additional vectors: the query, key and value vector. All three vectors are simple (independant) linear transformation of the embedding vector.</p>
<p>More specifically, the affinity matrix (previously computed as <code>embs @ embs.T</code>) is going to be replaced by the multiplication of the query with the key matrix. The intuition here is that the query represents <em>the information of a token with respect to a certain query or question</em> and the key vector represents <em>the information the tokens contains</em>.</p>
<p>As an example, let’s say that we need to predict what follows after encountering the single word “in”. When we just have this single word predicting the next word is extremely difficult, it could be something like a country, for example <em>in Spain</em>, or <em>in Italy</em>. Or perhaps something related to time: <em>in the morning</em> or <em>in the evening</em>. Or it could be something like <em>in the box</em> or <em>in the closet</em>. Knowing what <em>kind</em> of word comes next, all depends on the context.</p>
<p>If we add some context it becomes much easier: “Last year I was on vacation in …”. Now we immediately know that the first option (a country) is much more likely then the other two options (time and location/enclosure).</p>
<p>The intuition is now, that the <em>query vector</em> of the word <em>in</em> is a representation of the question related to whether we are talking about a country, a time or a location/enclosure. And the key vectors of all the tokens represent whether they have any information on this. For example, the word “vacation” could emit a key that is similar to the query vector of “in”, generating a high affinity between the tokens and thus resulting in a large weight.</p>
<p>The multiplication of the queries and the keys is thus what forms the affinity-matrix in self-attention. From this affinity-matrix the weighted averages matrix is computed in the way we have seen above (masking and softmax). You might expect that we then use this weighted averages matrix on our embeddings to get to weighted embeddings, but this is not the case. We don’t directly average over the embeddings, instead we average the <em>values</em>. Similarly as for the query and key, this is just a linear transformation of the embedding itself.</p>
<p>The last thing which concludes attention is a normalization. As we have seen in an earlier <a href="https://lucasvw.github.io/posts/11_nntrain_activations/#iterative-matrix-multiplications">post</a>, the distribution of activations can shrink or increase by doing matrix multiplications. In the attention module, there is a scaling of the activations <span class="math inline">\(1/\sqrt{d_k}\)</span>. This <span class="math inline">\(d_k\)</span> refers to the vector length of keys, values and queries, which is unique and is also referred to as the size of the (attention) head. after the multiplications of the keys and queries to make sure the distribution stays roughly standard normal.</p>
<p>We can now understand the full formula for masked self-attention:</p>
<p><span class="math display">\[
\textrm{Masked Self Attention}(Q, K, V) = \textrm{softmax} \left( \frac{\textrm{mask}(QK^T)}{\sqrt{d_k}} \right) V
\]</span></p>
</section>
<section id="masked-self-attention" class="level2">
<h2 class="anchored" data-anchor-id="masked-self-attention">Masked Self Attention</h2>
<p>Let’s make a start with formalizing these things in code. The data we are going to work with is the same data as we discussed in the previous posts on language: a dataset consisting of a list of (person) names.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="co">########### Load the data ###########</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> Path(<span class="st">'./data'</span>)</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>path.mkdir(parents<span class="op">=</span><span class="va">True</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> path <span class="op">/</span> <span class="st">'names.txt'</span></span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">'https://raw.githubusercontent.com/karpathy/makemore/master/names.txt'</span></span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a><span class="co"># _ = urlretrieve(url, path)</span></span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(path, <span class="st">'r'</span>) <span class="im">as</span> f:</span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a>    lines <span class="op">=</span> f.read().splitlines()</span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-12"><a href="#cb65-12" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">42</span>)</span>
<span id="cb65-13"><a href="#cb65-13" aria-hidden="true" tabindex="-1"></a>random.shuffle(lines)</span>
<span id="cb65-14"><a href="#cb65-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-15"><a href="#cb65-15" aria-hidden="true" tabindex="-1"></a>train_size<span class="op">=</span><span class="fl">0.8</span></span>
<span id="cb65-16"><a href="#cb65-16" aria-hidden="true" tabindex="-1"></a>val_size<span class="op">=</span><span class="fl">0.1</span></span>
<span id="cb65-17"><a href="#cb65-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-18"><a href="#cb65-18" aria-hidden="true" tabindex="-1"></a>train_lines <span class="op">=</span> lines[<span class="dv">0</span>:<span class="bu">int</span>(train_size <span class="op">*</span> <span class="bu">len</span>(lines))]</span>
<span id="cb65-19"><a href="#cb65-19" aria-hidden="true" tabindex="-1"></a>val_lines <span class="op">=</span> lines[<span class="bu">int</span>(train_size <span class="op">*</span> <span class="bu">len</span>(lines)): <span class="bu">int</span>((train_size <span class="op">+</span> val_size) <span class="op">*</span> <span class="bu">len</span>(lines))]</span>
<span id="cb65-20"><a href="#cb65-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-21"><a href="#cb65-21" aria-hidden="true" tabindex="-1"></a><span class="co">### Create vocabulary and mappings </span><span class="al">###</span></span>
<span id="cb65-22"><a href="#cb65-22" aria-hidden="true" tabindex="-1"></a>unique_chars <span class="op">=</span> <span class="bu">list</span>(<span class="bu">set</span>(<span class="st">""</span>.join(lines)))</span>
<span id="cb65-23"><a href="#cb65-23" aria-hidden="true" tabindex="-1"></a>unique_chars.sort()</span>
<span id="cb65-24"><a href="#cb65-24" aria-hidden="true" tabindex="-1"></a>vocabulary <span class="op">=</span> [<span class="st">'.'</span>] <span class="op">+</span> unique_chars</span>
<span id="cb65-25"><a href="#cb65-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-26"><a href="#cb65-26" aria-hidden="true" tabindex="-1"></a>c2i <span class="op">=</span> {c:i <span class="cf">for</span> i, c <span class="kw">in</span> <span class="bu">enumerate</span>(vocabulary)}</span>
<span id="cb65-27"><a href="#cb65-27" aria-hidden="true" tabindex="-1"></a>i2c <span class="op">=</span> {i:c <span class="cf">for</span> i, c <span class="kw">in</span> <span class="bu">enumerate</span>(vocabulary)}</span>
<span id="cb65-28"><a href="#cb65-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-29"><a href="#cb65-29" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SequentialDataset():</span>
<span id="cb65-30"><a href="#cb65-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, lines, c2i, sequence_length):</span>
<span id="cb65-31"><a href="#cb65-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lines <span class="op">=</span> lines</span>
<span id="cb65-32"><a href="#cb65-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.c2i <span class="op">=</span> c2i</span>
<span id="cb65-33"><a href="#cb65-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sequence_length <span class="op">=</span> sequence_length</span>
<span id="cb65-34"><a href="#cb65-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.shuffle_and_set_data()</span>
<span id="cb65-35"><a href="#cb65-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-36"><a href="#cb65-36" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> shuffle_and_set_data(<span class="va">self</span>):</span>
<span id="cb65-37"><a href="#cb65-37" aria-hidden="true" tabindex="-1"></a>        random.shuffle(<span class="va">self</span>.lines)</span>
<span id="cb65-38"><a href="#cb65-38" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> <span class="st">"."</span> <span class="op">+</span> <span class="st">"."</span>.join(<span class="va">self</span>.lines) <span class="op">+</span> <span class="st">"."</span></span>
<span id="cb65-39"><a href="#cb65-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.x <span class="op">=</span> []</span>
<span id="cb65-40"><a href="#cb65-40" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.y <span class="op">=</span> []</span>
<span id="cb65-41"><a href="#cb65-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(text) <span class="op">-</span> <span class="va">self</span>.sequence_length <span class="op">-</span> <span class="dv">1</span>, <span class="va">self</span>.sequence_length):</span>
<span id="cb65-42"><a href="#cb65-42" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.x.append([<span class="va">self</span>.c2i[xi] <span class="cf">for</span> xi <span class="kw">in</span> text[i: i<span class="op">+</span><span class="va">self</span>.sequence_length]])</span>
<span id="cb65-43"><a href="#cb65-43" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.y.append([<span class="va">self</span>.c2i[yi] <span class="cf">for</span> yi <span class="kw">in</span> text[i<span class="op">+</span><span class="dv">1</span>: i<span class="op">+</span><span class="va">self</span>.sequence_length<span class="op">+</span><span class="dv">1</span>]])</span>
<span id="cb65-44"><a href="#cb65-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.x <span class="op">=</span> torch.tensor(<span class="va">self</span>.x)</span>
<span id="cb65-45"><a href="#cb65-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.y <span class="op">=</span> torch.tensor(<span class="va">self</span>.y)</span>
<span id="cb65-46"><a href="#cb65-46" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-47"><a href="#cb65-47" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, i):</span>
<span id="cb65-48"><a href="#cb65-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.x[i], <span class="va">self</span>.y[i]</span>
<span id="cb65-49"><a href="#cb65-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-50"><a href="#cb65-50" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb65-51"><a href="#cb65-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.x)</span>
<span id="cb65-52"><a href="#cb65-52" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-53"><a href="#cb65-53" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Shuffle(Subscriber):</span>
<span id="cb65-54"><a href="#cb65-54" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> after_epoch(<span class="va">self</span>, learn):</span>
<span id="cb65-55"><a href="#cb65-55" aria-hidden="true" tabindex="-1"></a>        learn.dls.train.dataset.shuffle_and_set_data()</span>
<span id="cb65-56"><a href="#cb65-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-57"><a href="#cb65-57" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_dls(context_length, batch_size):</span>
<span id="cb65-58"><a href="#cb65-58" aria-hidden="true" tabindex="-1"></a>    train_ds <span class="op">=</span> SequentialDataset(train_lines, c2i, context_length)</span>
<span id="cb65-59"><a href="#cb65-59" aria-hidden="true" tabindex="-1"></a>    valid_ds <span class="op">=</span> SequentialDataset(val_lines, c2i, context_length)</span>
<span id="cb65-60"><a href="#cb65-60" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb65-61"><a href="#cb65-61" aria-hidden="true" tabindex="-1"></a>    train_dl <span class="op">=</span> torch.utils.data.DataLoader(train_ds, shuffle<span class="op">=</span><span class="va">False</span>, sampler<span class="op">=</span>VerticalSampler(train_ds, batch_size), batch_size<span class="op">=</span>batch_size, num_workers<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb65-62"><a href="#cb65-62" aria-hidden="true" tabindex="-1"></a>    valid_dl <span class="op">=</span> torch.utils.data.DataLoader(valid_ds, shuffle<span class="op">=</span><span class="va">False</span>, sampler<span class="op">=</span>VerticalSampler(valid_ds, batch_size), batch_size<span class="op">=</span>batch_size, num_workers<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb65-63"><a href="#cb65-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-64"><a href="#cb65-64" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> DataLoaders(train_dl, valid_dl)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>sequence_length <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>batch_size      <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> get_dls(sequence_length, batch_size)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>xb, yb <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(dls.train))</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>xb.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([8, 5])</code></pre>
</div>
</div>
<p>The samples are thus in the same structure as we used for the RNN (and LSTM) models:</p>
<ol type="1">
<li>We have two dimensions in our training data: the first one being the batch dimension <code>B</code> and the second refers to a sequence of characters <code>T</code>. The <code>T</code> stands here for timestep, which is a strange name but is used for historical reasons</li>
<li>The previous discussion on computing the rolling average over tensors only mentioned one of these dimensions: the sequence length dimension</li>
<li>Don’t be confused by the batch dimension, it simply means we are processing each items in the batch in parallel. All the items in the batch are processed independantly from one another</li>
<li>We will see in the code, we need to be careful with these dimensions, to make sure the broadcasting rules of matrix multiplications are correctly applied</li>
</ol>
<p>Let’s create a <code>MaskedSelfAttentionHead</code> module. We initialize this module by creating the linear layers by which we can create the query, key and value vectors from our embeddings. Remind yourself that a linear layer applied to an embedding vector is simply a linear transformation of that vector into a new space (of dimension <code>head_size</code>).</p>
<p>We will also initialize the <code>mask</code> tensor consisting of a lower triangular matrix as a <em>buffer</em>. This makes sure that PyTorch will move this tensor to the appropriate device when moving the module, eventhough it’s not treated as a module parameter (it’s values are not optimized during training).</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MaskedSelfAttentionHead(nn.Module):</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, emb_depth, head_size, sequence_length):</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.queries <span class="op">=</span> nn.Linear(emb_depth, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.keys    <span class="op">=</span> nn.Linear(emb_depth, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.values  <span class="op">=</span> nn.Linear(emb_depth, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'mask'</span>, torch.tril(torch.ones(sequence_length, sequence_length)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, let’s have a closer look at the <code>forward()</code> method of this module, starting with the argument: the data passed to this method consists of a <em>batch of embedding data</em>. We already saw that the raw input data is of shape <code>[B, T]</code>, once this data goes through an embedding layer it will be of shape: <code>[B, T, Ce]</code>, where <code>Ce</code> reflects the embedding depth.</p>
<p>From this data, we first compute the queries, keys and value vectors. Because of the shape of the linear layers, these three tensors are now of shape <code>[B, T, Ch]</code>.</p>
<p>As discussed, we want to compute the affinities between the queries and the keys which in the <em>single batch</em> case was computed as <span class="math inline">\(QK^T\)</span>. Since we now also have a batch dimension, we only transpose the last two dimensions and the matrix multiplication becomes a <em>batched matrix multiplication</em>.</p>
<p>These affinities are scaled, masked and softmaxed so that they represent weights. And finally the weights are applied to the batched values vector.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="at">@fc.patch</span>()</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>:MaskedSelfAttentionHead, x_emb):  <span class="co"># x_emb: [B, T, Ce]</span></span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>    q <span class="op">=</span> <span class="va">self</span>.queries(x_emb)  <span class="co"># [B,T,Ch]</span></span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> <span class="va">self</span>.keys(x_emb)     <span class="co"># [B,T,Ch]</span></span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> <span class="va">self</span>.values(x_emb)   <span class="co"># [B,T,Ch]</span></span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>    affinity <span class="op">=</span> q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)  <span class="co"># [B,T,Ch] @ [B, Ch, T] -&gt; [B, T, T]</span></span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>    head_size <span class="op">=</span> q.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a>    scaled_affinity <span class="op">=</span> affinity <span class="op">/</span> head_size<span class="op">**</span><span class="fl">0.5</span></span>
<span id="cb70-10"><a href="#cb70-10" aria-hidden="true" tabindex="-1"></a>    masked_scaled_affinity <span class="op">=</span> scaled_affinity.masked_fill_(<span class="va">self</span>.mask <span class="op">==</span> <span class="dv">0</span>, <span class="op">-</span>torch.inf)</span>
<span id="cb70-11"><a href="#cb70-11" aria-hidden="true" tabindex="-1"></a>    masked_weights <span class="op">=</span> masked_scaled_affinity.softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb70-12"><a href="#cb70-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-13"><a href="#cb70-13" aria-hidden="true" tabindex="-1"></a>    masked_self_attention <span class="op">=</span> masked_weights <span class="op">@</span> v <span class="co"># [B, T, T] @ [B,T,Ch] -&gt; [B,T,Ch]</span></span>
<span id="cb70-14"><a href="#cb70-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-15"><a href="#cb70-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> masked_self_attention <span class="co"># [B,T,Ch]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Our first transformer module is then very straight forward. Firstly, it will create embeddings for our input data, next it will compute the self-attention as we just discussed, and finally it will pass the self attention to a so-called <em>head</em>. This head is a simple linear layer which maps the activations to the right output size (the size of the vocabulary).</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Transformer(nn.Module):</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, emb_depth, head_size, sequence_length):</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.token_embedding <span class="op">=</span> nn.Embedding(<span class="bu">len</span>(c2i), emb_depth)</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> MaskedSelfAttentionHead(emb_depth, head_size, sequence_length)</span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head <span class="op">=</span> nn.Linear(head_size, <span class="bu">len</span>(c2i))</span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):           <span class="co"># x: [B, T]</span></span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.token_embedding(x) <span class="co"># [B,T,Ce]</span></span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.attention(x)       <span class="co"># [B,T,Ch]</span></span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.head(x)            <span class="co"># [B,T,vocab]</span></span>
<span id="cb71-13"><a href="#cb71-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>And let’s see how this model trains on our data:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train parameters</span></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>lr     <span class="op">=</span> <span class="fl">1e-1</span></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>bs     <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameters</span></span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a>sequence_length <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a>emb_depth       <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a>head_size       <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> get_dls(sequence_length, bs)</span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Transformer(emb_depth, head_size, sequence_length)</span>
<span id="cb72-13"><a href="#cb72-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-14"><a href="#cb72-14" aria-hidden="true" tabindex="-1"></a>subs <span class="op">=</span> [ProgressS(<span class="va">True</span>),</span>
<span id="cb72-15"><a href="#cb72-15" aria-hidden="true" tabindex="-1"></a>        MetricsS(),</span>
<span id="cb72-16"><a href="#cb72-16" aria-hidden="true" tabindex="-1"></a>        DeviceS(device),</span>
<span id="cb72-17"><a href="#cb72-17" aria-hidden="true" tabindex="-1"></a>        Shuffle()]</span>
<span id="cb72-18"><a href="#cb72-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-19"><a href="#cb72-19" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> partial(torch.optim.lr_scheduler.OneCycleLR, max_lr<span class="op">=</span>lr, epochs<span class="op">=</span>epochs, steps_per_epoch<span class="op">=</span><span class="bu">len</span>(dls.train))</span>
<span id="cb72-20"><a href="#cb72-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-21"><a href="#cb72-21" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> Learner(model, dls, multi_output_cross_entropy, torch.optim.Adam, <span class="va">None</span>, subs<span class="op">=</span>subs <span class="op">+</span>[SchedulerS(scheduler)])</span>
<span id="cb72-22"><a href="#cb72-22" aria-hidden="true" tabindex="-1"></a>l.fit(epochs, lr<span class="op">=</span>lr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">mode</th>
<th data-quarto-table-cell-role="th">loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>train</td>
<td>2.813</td>
</tr>
<tr class="even">
<td>0</td>
<td>eval</td>
<td>2.585</td>
</tr>
<tr class="odd">
<td>1</td>
<td>train</td>
<td>2.604</td>
</tr>
<tr class="even">
<td>1</td>
<td>eval</td>
<td>2.547</td>
</tr>
<tr class="odd">
<td>2</td>
<td>train</td>
<td>2.515</td>
</tr>
<tr class="even">
<td>2</td>
<td>eval</td>
<td>2.496</td>
</tr>
<tr class="odd">
<td>3</td>
<td>train</td>
<td>2.492</td>
</tr>
<tr class="even">
<td>3</td>
<td>eval</td>
<td>2.481</td>
</tr>
<tr class="odd">
<td>4</td>
<td>train</td>
<td>2.479</td>
</tr>
<tr class="even">
<td>4</td>
<td>eval</td>
<td>2.476</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-41-output-3.png" class="img-fluid"></p>
</div>
</div>
<p>The model seems to be training (the loss is going down), but it’s not performing very well. Let’s thus continue adding other components that are described in the paper:</p>
</section>
<section id="positional-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="positional-embeddings">Positional embeddings</h2>
<p>So far we created embeddings for our tokens and used them as the basis for the query, key and value vectors. However, in doing so we never explicitly encoded the <em>positional</em> information in the embeddings: we don’t have any mechanism of working with the position of the tokens. Consider the sentence: <em>I was working at home when my cat …</em>. Positional information is important to encode the fact that <em>cat</em> was mentioned after <em>I</em>. This way, the model might know that the words that will follow, will have to relate to my cat and not to me. There are multiple ways of encoding this positional information, but for now let’s just stick with another embedding layer which will simply embed the position of each token in the sentence:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Transformer(nn.Module):</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, emb_depth, head_size, sequence_length):</span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.token_embedding <span class="op">=</span> nn.Embedding(<span class="bu">len</span>(c2i), emb_depth)</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.position_embedding <span class="op">=</span> nn.Embedding(sequence_length, emb_depth)</span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'range'</span>, torch.arange(sequence_length))</span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> MaskedSelfAttentionHead(emb_depth, head_size, sequence_length)</span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head <span class="op">=</span> nn.Linear(head_size, <span class="bu">len</span>(c2i))</span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):                     <span class="co"># x: [B,T]</span></span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a>        x_tok <span class="op">=</span> <span class="va">self</span>.token_embedding(x)       <span class="co"># [B,T,Ce]</span></span>
<span id="cb73-13"><a href="#cb73-13" aria-hidden="true" tabindex="-1"></a>        x_pos <span class="op">=</span> <span class="va">self</span>.position_embedding(<span class="va">self</span>.<span class="bu">range</span>) <span class="co"># [B,T,Ce]</span></span>
<span id="cb73-14"><a href="#cb73-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.attention(x_tok <span class="op">+</span> x_pos)     <span class="co"># [B,T,Ch]</span></span>
<span id="cb73-15"><a href="#cb73-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.head(x)                      <span class="co"># [B,T,vocab]</span></span>
<span id="cb73-16"><a href="#cb73-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that the positional and token embedding are simply added together element-wise.</p>
</section>
<section id="multi-head-attention" class="level2">
<h2 class="anchored" data-anchor-id="multi-head-attention">Multi-head attention</h2>
<p>When introducing self-attention, we discussed one specific query that the token could be interested in, we computed the affinity of that query with all the keys of the tokens. It seems natural to assume that there are multiple queries to be formulated which would help with the prediction of the next token (What is the subject of the sentence? What is the tense? etc). Each possible query could naturally have an affinity with different tokens. These considerations give rise to <em>multi-head attention</em>. Multi-head attention is simply the application of multiple attention-heads in parallel and the concatentation of the results.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, emb_depth, head_size, n_heads, sequence_length):</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>        hs <span class="op">=</span> head_size <span class="op">//</span> n_heads</span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> head_size <span class="op">%</span> n_heads <span class="op">!=</span> <span class="dv">0</span>:</span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"head_size and n_heads don't match, make sure that head_size is divisible (without remainder) through n_heads"</span>)</span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.heads <span class="op">=</span> nn.ModuleList([MaskedSelfAttentionHead(emb_depth, hs, sequence_length) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_heads)])</span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.cat([head(x) <span class="cf">for</span> head <span class="kw">in</span> <span class="va">self</span>.heads], dim<span class="op">=-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Transformer(nn.Module):</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, emb_depth, head_size, n_heads, sequence_length):</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.token_embedding <span class="op">=</span> nn.Embedding(<span class="bu">len</span>(c2i), emb_depth)</span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.position_embedding <span class="op">=</span> nn.Embedding(sequence_length, emb_depth)</span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'range'</span>, torch.arange(sequence_length))</span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> MultiHeadAttention(emb_depth, head_size, sequence_length)</span>
<span id="cb75-9"><a href="#cb75-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head <span class="op">=</span> nn.Linear(head_size, <span class="bu">len</span>(c2i))</span>
<span id="cb75-10"><a href="#cb75-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb75-11"><a href="#cb75-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):                     <span class="co"># x: [B,T]</span></span>
<span id="cb75-12"><a href="#cb75-12" aria-hidden="true" tabindex="-1"></a>        x_tok <span class="op">=</span> <span class="va">self</span>.token_embedding(x)       <span class="co"># [B,T,Ce]</span></span>
<span id="cb75-13"><a href="#cb75-13" aria-hidden="true" tabindex="-1"></a>        x_pos <span class="op">=</span> <span class="va">self</span>.position_embedding(<span class="va">self</span>.<span class="bu">range</span>) <span class="co"># [B,T,Ce]</span></span>
<span id="cb75-14"><a href="#cb75-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.attention(x_tok <span class="op">+</span> x_pos)     <span class="co"># [B,T,Ch]</span></span>
<span id="cb75-15"><a href="#cb75-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.head(x)                      <span class="co"># [B,T,vocab]</span></span>
<span id="cb75-16"><a href="#cb75-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="multiple-transformer-blocks" class="level2">
<h2 class="anchored" data-anchor-id="multiple-transformer-blocks">Multiple transformer blocks</h2>
<p>The next improvement has to do with adding computation following the attention. Currently we are doing one layer of self-attention, followed by the head which maps the activations to the correct number of outputs (the size of the vocabulary). We want to change this by adding a feed forward network right after the attention module. The idea being that we want to add computation which allows the transformer to <em>think</em> upon the results from an attention block. As mentioned, this will just be a simple Linear layer followed by a ReLU:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FeedForward(nn.Module):</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, emb_depth):</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.net <span class="op">=</span> nn.Sequential(<span class="op">*</span>[</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>            nn.Linear(emb_depth, emb_depth),</span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.net(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>And we create a <code>TransformerBlock</code> which combines the attention and computation phase:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerBlock(nn.Module):</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, emb_depth, n_heads, sequence_length):</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.att <span class="op">=</span> MultiHeadAttention(emb_depth, emb_depth, n_heads, sequence_length)</span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.comp <span class="op">=</span> FeedForward(emb_depth)</span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>        att <span class="op">=</span> <span class="va">self</span>.att(x)</span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>        comp <span class="op">=</span> <span class="va">self</span>.comp(att)</span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> comp</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now add another parameter <code>n_blocks</code> signifying how many of these sequential transformer blocks we want to have in the model:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Transformer(nn.Module):</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, emb_depth, n_heads, n_blocks, sequence_length):</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.token_embedding <span class="op">=</span> nn.Embedding(<span class="bu">len</span>(c2i), emb_depth)</span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.position_embedding <span class="op">=</span> nn.Embedding(sequence_length, emb_depth)</span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'range'</span>, torch.arange(sequence_length))</span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tblocks <span class="op">=</span> nn.Sequential(<span class="op">*</span>[TransformerBlock(emb_depth, n_heads, sequence_length) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_blocks)])</span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head <span class="op">=</span> nn.Linear(emb_depth, <span class="bu">len</span>(c2i))</span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.token_embedding(x) <span class="op">+</span> <span class="va">self</span>.position_embedding(<span class="va">self</span>.<span class="bu">range</span>)</span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.tblocks(x)</span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.head(x)</span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="layernorm-and-residual-connections" class="level2">
<h2 class="anchored" data-anchor-id="layernorm-and-residual-connections">Layernorm and Residual Connections</h2>
<p>And finally we will add the skip connections and layer normalization. The way these are added is somewhat different from the paper. After the paper, one of the few improvements that were made on this architecture was the use of something called Pre-LN transformer, in which two layernorms are applied in the residual path before the attention and the computation respectively.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerBlock(nn.Module):</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, emb_depth, n_heads, sequence_length):</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.att <span class="op">=</span> MultiHeadAttention(emb_depth, emb_depth, n_heads, sequence_length)</span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.comp <span class="op">=</span> FeedForward(emb_depth)</span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln1 <span class="op">=</span> nn.LayerNorm(emb_depth)</span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln2 <span class="op">=</span> nn.LayerNorm(emb_depth)</span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb79-9"><a href="#cb79-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb79-10"><a href="#cb79-10" aria-hidden="true" tabindex="-1"></a>        att <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.att(<span class="va">self</span>.ln1(x))</span>
<span id="cb79-11"><a href="#cb79-11" aria-hidden="true" tabindex="-1"></a>        comp <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.comp(<span class="va">self</span>.ln1(att))</span>
<span id="cb79-12"><a href="#cb79-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> comp</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s put it all together now and train the model on the data</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train parameters</span></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>lr     <span class="op">=</span> <span class="fl">1e-2</span></span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>bs     <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameters</span></span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>sequence_length <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>emb_depth       <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a>n_heads         <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a>n_blocks        <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> get_dls(sequence_length, bs)</span>
<span id="cb80-13"><a href="#cb80-13" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Transformer(emb_depth, n_heads, n_blocks, sequence_length)</span>
<span id="cb80-14"><a href="#cb80-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-15"><a href="#cb80-15" aria-hidden="true" tabindex="-1"></a>subs <span class="op">=</span> [ProgressS(<span class="va">True</span>),</span>
<span id="cb80-16"><a href="#cb80-16" aria-hidden="true" tabindex="-1"></a>        MetricsS(),</span>
<span id="cb80-17"><a href="#cb80-17" aria-hidden="true" tabindex="-1"></a>        Shuffle(),</span>
<span id="cb80-18"><a href="#cb80-18" aria-hidden="true" tabindex="-1"></a>        DeviceS(device)]</span>
<span id="cb80-19"><a href="#cb80-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-20"><a href="#cb80-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-21"><a href="#cb80-21" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> partial(torch.optim.lr_scheduler.OneCycleLR, max_lr<span class="op">=</span>lr, epochs<span class="op">=</span>epochs, steps_per_epoch<span class="op">=</span><span class="bu">len</span>(dls.train))</span>
<span id="cb80-22"><a href="#cb80-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-23"><a href="#cb80-23" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> Learner(model, dls, multi_output_cross_entropy, torch.optim.Adam, <span class="va">None</span>, subs<span class="op">=</span>subs <span class="op">+</span> [SchedulerS(scheduler)])</span>
<span id="cb80-24"><a href="#cb80-24" aria-hidden="true" tabindex="-1"></a>l.fit(epochs, lr<span class="op">=</span>lr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">mode</th>
<th data-quarto-table-cell-role="th">loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>train</td>
<td>2.654</td>
</tr>
<tr class="even">
<td>0</td>
<td>eval</td>
<td>2.430</td>
</tr>
<tr class="odd">
<td>1</td>
<td>train</td>
<td>2.371</td>
</tr>
<tr class="even">
<td>1</td>
<td>eval</td>
<td>2.321</td>
</tr>
<tr class="odd">
<td>2</td>
<td>train</td>
<td>2.291</td>
</tr>
<tr class="even">
<td>2</td>
<td>eval</td>
<td>2.268</td>
</tr>
<tr class="odd">
<td>3</td>
<td>train</td>
<td>2.243</td>
</tr>
<tr class="even">
<td>3</td>
<td>eval</td>
<td>2.228</td>
</tr>
<tr class="odd">
<td>4</td>
<td>train</td>
<td>2.217</td>
</tr>
<tr class="even">
<td>4</td>
<td>eval</td>
<td>2.214</td>
</tr>
<tr class="odd">
<td>5</td>
<td>train</td>
<td>2.203</td>
</tr>
<tr class="even">
<td>5</td>
<td>eval</td>
<td>2.208</td>
</tr>
<tr class="odd">
<td>6</td>
<td>train</td>
<td>2.189</td>
</tr>
<tr class="even">
<td>6</td>
<td>eval</td>
<td>2.186</td>
</tr>
<tr class="odd">
<td>7</td>
<td>train</td>
<td>2.173</td>
</tr>
<tr class="even">
<td>7</td>
<td>eval</td>
<td>2.189</td>
</tr>
<tr class="odd">
<td>8</td>
<td>train</td>
<td>2.161</td>
</tr>
<tr class="even">
<td>8</td>
<td>eval</td>
<td>2.171</td>
</tr>
<tr class="odd">
<td>9</td>
<td>train</td>
<td>2.147</td>
</tr>
<tr class="even">
<td>9</td>
<td>eval</td>
<td>2.160</td>
</tr>
<tr class="odd">
<td>10</td>
<td>train</td>
<td>2.135</td>
</tr>
<tr class="even">
<td>10</td>
<td>eval</td>
<td>2.155</td>
</tr>
<tr class="odd">
<td>11</td>
<td>train</td>
<td>2.125</td>
</tr>
<tr class="even">
<td>11</td>
<td>eval</td>
<td>2.129</td>
</tr>
<tr class="odd">
<td>12</td>
<td>train</td>
<td>2.111</td>
</tr>
<tr class="even">
<td>12</td>
<td>eval</td>
<td>2.116</td>
</tr>
<tr class="odd">
<td>13</td>
<td>train</td>
<td>2.101</td>
</tr>
<tr class="even">
<td>13</td>
<td>eval</td>
<td>2.110</td>
</tr>
<tr class="odd">
<td>14</td>
<td>train</td>
<td>2.090</td>
</tr>
<tr class="even">
<td>14</td>
<td>eval</td>
<td>2.111</td>
</tr>
<tr class="odd">
<td>15</td>
<td>train</td>
<td>2.079</td>
</tr>
<tr class="even">
<td>15</td>
<td>eval</td>
<td>2.103</td>
</tr>
<tr class="odd">
<td>16</td>
<td>train</td>
<td>2.069</td>
</tr>
<tr class="even">
<td>16</td>
<td>eval</td>
<td>2.085</td>
</tr>
<tr class="odd">
<td>17</td>
<td>train</td>
<td>2.055</td>
</tr>
<tr class="even">
<td>17</td>
<td>eval</td>
<td>2.082</td>
</tr>
<tr class="odd">
<td>18</td>
<td>train</td>
<td>2.043</td>
</tr>
<tr class="even">
<td>18</td>
<td>eval</td>
<td>2.075</td>
</tr>
<tr class="odd">
<td>19</td>
<td>train</td>
<td>2.029</td>
</tr>
<tr class="even">
<td>19</td>
<td>eval</td>
<td>2.062</td>
</tr>
<tr class="odd">
<td>20</td>
<td>train</td>
<td>2.015</td>
</tr>
<tr class="even">
<td>20</td>
<td>eval</td>
<td>2.057</td>
</tr>
<tr class="odd">
<td>21</td>
<td>train</td>
<td>2.002</td>
</tr>
<tr class="even">
<td>21</td>
<td>eval</td>
<td>2.051</td>
</tr>
<tr class="odd">
<td>22</td>
<td>train</td>
<td>1.990</td>
</tr>
<tr class="even">
<td>22</td>
<td>eval</td>
<td>2.046</td>
</tr>
<tr class="odd">
<td>23</td>
<td>train</td>
<td>1.978</td>
</tr>
<tr class="even">
<td>23</td>
<td>eval</td>
<td>2.038</td>
</tr>
<tr class="odd">
<td>24</td>
<td>train</td>
<td>1.966</td>
</tr>
<tr class="even">
<td>24</td>
<td>eval</td>
<td>2.034</td>
</tr>
<tr class="odd">
<td>25</td>
<td>train</td>
<td>1.956</td>
</tr>
<tr class="even">
<td>25</td>
<td>eval</td>
<td>2.029</td>
</tr>
<tr class="odd">
<td>26</td>
<td>train</td>
<td>1.947</td>
</tr>
<tr class="even">
<td>26</td>
<td>eval</td>
<td>2.027</td>
</tr>
<tr class="odd">
<td>27</td>
<td>train</td>
<td>1.940</td>
</tr>
<tr class="even">
<td>27</td>
<td>eval</td>
<td>2.025</td>
</tr>
<tr class="odd">
<td>28</td>
<td>train</td>
<td>1.937</td>
</tr>
<tr class="even">
<td>28</td>
<td>eval</td>
<td>2.024</td>
</tr>
<tr class="odd">
<td>29</td>
<td>train</td>
<td>1.934</td>
</tr>
<tr class="even">
<td>29</td>
<td>eval</td>
<td>2.024</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-49-output-3.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="sampling-names" class="level2">
<h2 class="anchored" data-anchor-id="sampling-names">Sampling names</h2>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="at">@fc.patch</span>()</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate(<span class="va">self</span>:Transformer, n_tokens<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> torch.zeros(<span class="dv">1</span>,sequence_length, dtype<span class="op">=</span>torch.<span class="bu">long</span>).to(device) <span class="co"># B,T</span></span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_tokens):</span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a>        idx_s <span class="op">=</span> idx[:, <span class="op">-</span>sequence_length:]                  <span class="co"># B,T</span></span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>(idx_s)                               <span class="co"># B,T,C</span></span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a>        last_pred <span class="op">=</span> logits[:,<span class="op">-</span><span class="dv">1</span>,:]                         <span class="co"># B,C</span></span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a>        probs <span class="op">=</span> F.softmax(last_pred, dim<span class="op">=-</span><span class="dv">1</span>)               <span class="co"># B,C</span></span>
<span id="cb81-9"><a href="#cb81-9" aria-hidden="true" tabindex="-1"></a>        idx_next <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>) <span class="co"># B,1</span></span>
<span id="cb81-10"><a href="#cb81-10" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">=</span> torch.cat((idx, idx_next), dim<span class="op">=</span><span class="dv">1</span>)            <span class="co"># B,T+1</span></span>
<span id="cb81-11"><a href="#cb81-11" aria-hidden="true" tabindex="-1"></a>    l <span class="op">=</span> idx[<span class="dv">0</span>,sequence_length:].tolist()</span>
<span id="cb81-12"><a href="#cb81-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">""</span>.join([i2c[i] <span class="cf">for</span> i <span class="kw">in</span> l]).split(<span class="st">"."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>model.generate(<span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>['enthani',
 'walynn',
 'carkton',
 'sulon',
 'kanias',
 'amour',
 'katy',
 'ayama',
 'sylen',
 'abexten',
 'derriz',
 'kacey',
 'vonssen',
 'tryver',
 'amaru']</code></pre>
</div>
</div>
</section>
<section id="remarks" class="level2">
<h2 class="anchored" data-anchor-id="remarks">Remarks</h2>
<p>The model is training fairly well, but it seems difficult to get the (validation) loss to the same level as what we saw for the <a href="https://lucasvw.github.io/posts/16_lstm/#pytorch-lstm">LSTM</a> which was around 1.95. This bugged me for a bit, but then I realised this is comparing apples to oranges. The LSTM (and RNN) architecture, and more precisely the way we load the data and deal with passing the hidden state from batch to batch (see <a href="https://lucasvw.github.io/posts/15_rnn/#optimizing-dataloading-and-model-for-sequential-data-bptt">here</a>), means that even for the first item in the sequence (<code>T=1</code>) we have information on the history from the previous batch. This is not the case for Transformers, where each batch starts out <em>fresh</em>, without any memory.</p>
<p>I ran a small test to verify that this is indeed causing the difference in observed loss. I created a new loss function, which (during evaluation) only computes the loss on the last item in the sequence. I re-trained the transformer with a sequence length equal to what was used for the LSTM (5) and obtained a validation loss of 1.933. This shows indeed that a Transformer architecture is outperforming the LSTM on this dataset.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="lucasvw/BlogComments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>