<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Lucas van Walstijn">
<meta name="dcterms.date" content="2023-09-14">

<title>Lucas van Walstijn - Recurrent Neural Networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../profile.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="../../site_libs/quarto-contrib/iconify-1.0.0-beta.2/iconify-icon.min.js"></script>
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Lucas van Walstijn</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/lucasvw" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/lvWal" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/lucasvanwalstijn/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://www.kaggle.com/lucasvw" rel="" target="">
 <span class="menu-text"><iconify-icon inline="" icon="fa6-brands:kaggle"></iconify-icon></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://lucasvw.github.io/index.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#data" id="toc-data" class="nav-link active" data-scroll-target="#data">Data</a></li>
  <li><a href="#from-mlp-to-rnn-a-small-change" id="toc-from-mlp-to-rnn-a-small-change" class="nav-link" data-scroll-target="#from-mlp-to-rnn-a-small-change">From MLP to RNN, a small change</a></li>
  <li><a href="#combing-data-addition-or-concatenation" id="toc-combing-data-addition-or-concatenation" class="nav-link" data-scroll-target="#combing-data-addition-or-concatenation">Combing data: addition or concatenation</a></li>
  <li><a href="#pytorch-rnn" id="toc-pytorch-rnn" class="nav-link" data-scroll-target="#pytorch-rnn">PyTorch RNN</a></li>
  <li><a href="#optimizing-dataloading-and-model-for-sequential-data-bptt" id="toc-optimizing-dataloading-and-model-for-sequential-data-bptt" class="nav-link" data-scroll-target="#optimizing-dataloading-and-model-for-sequential-data-bptt">Optimizing dataloading and model for sequential data (BPTT)</a></li>
  <li><a href="#multilayer-rnns" id="toc-multilayer-rnns" class="nav-link" data-scroll-target="#multilayer-rnns">MultiLayer RNN’s</a></li>
  <li><a href="#sampling-names" id="toc-sampling-names" class="nav-link" data-scroll-target="#sampling-names">Sampling names</a></li>
  <li><a href="#outlook" id="toc-outlook" class="nav-link" data-scroll-target="#outlook">Outlook</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Recurrent Neural Networks</h1>
  <div class="quarto-categories">
    <div class="quarto-category">nlp</div>
    <div class="quarto-category">rnn</div>
    <div class="quarto-category">bptt</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Lucas van Walstijn </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 14, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>In the previous <a href="https://lucasvw.github.io/posts/14_mlp/">post</a> we went through a simple MLP language model. We looked in detail at embeddings, and how they can be used to overcome the <em>curse of dimensionality</em> encountered for n-gram based models.</p>
<p>In this post we are going to have a look at recurrent neural networks and how they are different from the MLP developed earlier. As we will see, the concept of an rnn is remarkably similar.</p>
<section id="data" class="level2">
<h2 class="anchored" data-anchor-id="data">Data</h2>
<p>Everything starts with training data, for a description see the earlier <a href="https://lucasvw.github.io/posts/13_ngram/">post</a></p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> <span class="bu">reduce</span>, partial</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> urllib.request <span class="im">import</span> urlretrieve</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torcheval.metrics <span class="im">as</span> tem</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> fastcore.<span class="bu">all</span> <span class="im">as</span> fc</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nntrain.dataloaders <span class="im">import</span> DataLoaders</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nntrain.learner <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nntrain.activations <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nntrain.acceleration <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nntrain.ngram <span class="im">import</span> <span class="op">*</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">########### Load the data ###########</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> Path(<span class="st">'./data'</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>path.mkdir(parents<span class="op">=</span><span class="va">True</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> path <span class="op">/</span> <span class="st">'names.txt'</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">'https://raw.githubusercontent.com/karpathy/makemore/master/names.txt'</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> urlretrieve(url, path)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(path, <span class="st">'r'</span>) <span class="im">as</span> f:</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    lines <span class="op">=</span> f.read().splitlines()</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">42</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>random.shuffle(lines)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>train_size<span class="op">=</span><span class="fl">0.8</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>val_size<span class="op">=</span><span class="fl">0.1</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>train_lines <span class="op">=</span> lines[<span class="dv">0</span>:<span class="bu">int</span>(train_size <span class="op">*</span> <span class="bu">len</span>(lines))]</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>val_lines <span class="op">=</span> lines[<span class="bu">int</span>(train_size <span class="op">*</span> <span class="bu">len</span>(lines)): <span class="bu">int</span>((train_size <span class="op">+</span> val_size) <span class="op">*</span> <span class="bu">len</span>(lines))]</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co">### Create vocabulary and mappings </span><span class="al">###</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>unique_chars <span class="op">=</span> <span class="bu">list</span>(<span class="bu">set</span>(<span class="st">""</span>.join(lines)))</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>unique_chars.sort()</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>vocabulary <span class="op">=</span> [<span class="st">'.'</span>] <span class="op">+</span> unique_chars</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>c2i <span class="op">=</span> {c:i <span class="cf">for</span> i, c <span class="kw">in</span> <span class="bu">enumerate</span>(vocabulary)}</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>i2c <span class="op">=</span> {i:c <span class="cf">for</span> i, c <span class="kw">in</span> <span class="bu">enumerate</span>(vocabulary)}</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_dls(bs, context_length):</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    train_ds <span class="op">=</span> NgramDataset(train_lines, c2i, n<span class="op">=</span>context_length<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    val_ds <span class="op">=</span> NgramDataset(val_lines, c2i, n<span class="op">=</span>context_length<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    train_loader <span class="op">=</span> torch.utils.data.DataLoader(train_ds, shuffle<span class="op">=</span><span class="va">True</span>, batch_size<span class="op">=</span>bs, num_workers<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>    val_loader <span class="op">=</span> torch.utils.data.DataLoader(val_ds, batch_size<span class="op">=</span>bs<span class="op">*</span><span class="dv">2</span>, num_workers<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    dls <span class="op">=</span> DataLoaders(train_loader, val_loader)</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dls</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="from-mlp-to-rnn-a-small-change" class="level2">
<h2 class="anchored" data-anchor-id="from-mlp-to-rnn-a-small-change">From MLP to RNN, a small change</h2>
<p>The MLP model encountered in the last post, used an embedding layer to turn the tokens from the context (in the image below of length 3) into vectors. These vectors then got concatenated, and passed through a hidden layer. After that, the activations were mapped to the size of the vocabulary in the output layer:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="arch.png" class="img-fluid figure-img" width="600"></p>
<figcaption class="figure-caption">MLP architecture</figcaption>
</figure>
</div>
<p>For an RNN, we also start-off by turning our integers into vectors making use of an embedding layer. However, instead of concatenating the result and passing it through a hidden layer, we <em>iteratively feed the embedding activations through a single hidden layer</em>. Each time moving on to (the embedding activations of) the next token and adding the hidden activations obtained from the previous iteration. Once all the context has been processed in this way, we finalize with an output layer similar to what we did for the MLP model.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="rnn.drawio.png" class="img-fluid figure-img" width="400"></p>
<figcaption class="figure-caption">RNN architecture</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The first time I learned about the summing of embedding activations and hidden activations, I was pretty surprised: how can you expect to get anything meaningful by simple adding activations from two layers together? As it turns out, in neural network literature it happens quite often that the combination / merging of activations simply happens by element-wise addition. In a section below, we will also shortly discuss another way of combining information: by concatenation.</p>
</div>
</div>
<p>Let’s put this RNN architecture into code:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FixedContextRNN(nn.Module):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, c2i, hidden_size<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.c2i <span class="op">=</span> c2i</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input2hidden   <span class="op">=</span> nn.Embedding(<span class="bu">len</span>(c2i), hidden_size)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden2hidden  <span class="op">=</span> nn.Linear(hidden_size, hidden_size)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden2out     <span class="op">=</span> nn.Linear(hidden_size, <span class="bu">len</span>(c2i))</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        input1 <span class="op">=</span> F.relu(<span class="va">self</span>.input2hidden(x[:,<span class="dv">0</span>]))</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        input2 <span class="op">=</span> F.relu(<span class="va">self</span>.input2hidden(x[:,<span class="dv">1</span>]))</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        input3 <span class="op">=</span> F.relu(<span class="va">self</span>.input2hidden(x[:,<span class="dv">2</span>]))</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> F.relu(<span class="va">self</span>.hidden2hidden(input1))</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> F.relu(<span class="va">self</span>.hidden2hidden(out <span class="op">+</span> input2))</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> F.relu(<span class="va">self</span>.hidden2hidden(out <span class="op">+</span> input3))</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.hidden2out(out)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Observe that both the embedding dimension and the size of the hidden layer need to be equal (<code>hidden_size</code>) because the activations from both these layers are added together and thus need to be of equal shape. This also means that there is just one hyperparameter influencing the amount of parameters in this network.</p>
<p>We can generalize this model, so that it accepts data of arbitrary context length. Also, we will make use of a <em>dummy hidden-layer activation tensor</em> (initialized to zeros) so that we can do the addition for all iterations (and not just for the second iteration onward):</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VariableContextRNN(nn.Module):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, c2i, hidden_size):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.c2i <span class="op">=</span> c2i</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_size <span class="op">=</span> hidden_size</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input2hidden   <span class="op">=</span> nn.Embedding(<span class="bu">len</span>(c2i), hidden_size)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden2hidden  <span class="op">=</span> nn.Linear(hidden_size, hidden_size)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden2out     <span class="op">=</span> nn.Linear(hidden_size, <span class="bu">len</span>(c2i))</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> <span class="va">self</span>.input2hidden(x)           <span class="co"># [bs, context_length, hidden_size]</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># create dummy hidden-layer activations for the very first token </span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> torch.zeros((x.shape[<span class="dv">0</span>], <span class="va">self</span>.hidden_size)).to(device)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(x.shape[<span class="dv">1</span>]):             <span class="co"># iterate through the tokens (context length)</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>            h <span class="op">=</span> F.relu(<span class="va">self</span>.hidden2hidden(h <span class="op">+</span> inputs[:,i,:])) <span class="co"># [bs, hidden_size]</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.hidden2out(h)               <span class="co"># [bs, classes]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s see how this model performs with a similar configuration to the first MLP we build in the previous <a href="https://lucasvw.github.io/posts/14_mlp/#using-embeddings-effectively">post</a> (context length of 3 and a hidden size of 50):</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train parameters</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>lr     <span class="op">=</span> <span class="fl">1e-2</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>bs     <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameters</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>context_length <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>n_h            <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> get_dls(bs, context_length)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>rnn <span class="op">=</span> VariableContextRNN(c2i, n_h)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>subs <span class="op">=</span> [ProgressS(<span class="va">True</span>),</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        MetricsS(),</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        DeviceS(device)]</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> partial(torch.optim.lr_scheduler.OneCycleLR, max_lr<span class="op">=</span>lr, epochs<span class="op">=</span>epochs, steps_per_epoch<span class="op">=</span><span class="bu">len</span>(dls.train))</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> Learner(rnn, dls, F.cross_entropy, torch.optim.Adam, <span class="va">None</span>, subs<span class="op">=</span>subs <span class="op">+</span> [SchedulerS(scheduler)])</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>l.fit(epochs, lr<span class="op">=</span>lr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">mode</th>
<th data-quarto-table-cell-role="th">loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>train</td>
<td>2.634</td>
</tr>
<tr class="even">
<td>0</td>
<td>eval</td>
<td>2.303</td>
</tr>
<tr class="odd">
<td>1</td>
<td>train</td>
<td>2.275</td>
</tr>
<tr class="even">
<td>1</td>
<td>eval</td>
<td>2.244</td>
</tr>
<tr class="odd">
<td>2</td>
<td>train</td>
<td>2.231</td>
</tr>
<tr class="even">
<td>2</td>
<td>eval</td>
<td>2.213</td>
</tr>
<tr class="odd">
<td>3</td>
<td>train</td>
<td>2.200</td>
</tr>
<tr class="even">
<td>3</td>
<td>eval</td>
<td>2.194</td>
</tr>
<tr class="odd">
<td>4</td>
<td>train</td>
<td>2.178</td>
</tr>
<tr class="even">
<td>4</td>
<td>eval</td>
<td>2.184</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-6-output-3.png" class="img-fluid"></p>
</div>
</div>
<p>So it’s doing a bit better (2.280 for the MLP, 2.178 for the RNN)</p>
</section>
<section id="combing-data-addition-or-concatenation" class="level2">
<h2 class="anchored" data-anchor-id="combing-data-addition-or-concatenation">Combing data: addition or concatenation</h2>
<p>As mentioned, the first time I learned about the fact that we are simply adding activations from the hidden layer and the embedding layer, I was pretty surprised. Instead of simple addition of the activations we could also concatenate these activations instead. This means that our <code>hidden2hidden</code> layer shape needs to be updated:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VariableContextRNNConcat(nn.Module):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, c2i, hidden_size):</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.c2i <span class="op">=</span> c2i</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_size <span class="op">=</span> hidden_size</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input2hidden   <span class="op">=</span> nn.Embedding(<span class="bu">len</span>(c2i), hidden_size)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden2hidden  <span class="op">=</span> nn.Linear(<span class="dv">2</span><span class="op">*</span>hidden_size, hidden_size)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden2out     <span class="op">=</span> nn.Linear(hidden_size, <span class="bu">len</span>(c2i))</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> <span class="va">self</span>.input2hidden(x)                <span class="co"># [bs, context_length, hidden_size]</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> torch.zeros((x.shape[<span class="dv">0</span>], <span class="va">self</span>.hidden_size)).to(device)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(x.shape[<span class="dv">1</span>]):</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>            h <span class="op">=</span> torch.cat([h, inputs[:,i,:]], dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># [bs, 2 x hidden_size]</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>            h <span class="op">=</span> F.relu(<span class="va">self</span>.hidden2hidden(h))        <span class="co"># [bs, hidden_size]</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.hidden2out(h)                    <span class="co"># [bs, classes]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>rnn <span class="op">=</span> VariableContextRNNConcat(c2i, n_h)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> Learner(rnn, dls, F.cross_entropy, torch.optim.Adam, <span class="va">None</span>, subs<span class="op">=</span>subs <span class="op">+</span> [SchedulerS(scheduler)])</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>l.fit(epochs, lr<span class="op">=</span>lr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">mode</th>
<th data-quarto-table-cell-role="th">loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>train</td>
<td>2.621</td>
</tr>
<tr class="even">
<td>0</td>
<td>eval</td>
<td>2.285</td>
</tr>
<tr class="odd">
<td>1</td>
<td>train</td>
<td>2.262</td>
</tr>
<tr class="even">
<td>1</td>
<td>eval</td>
<td>2.230</td>
</tr>
<tr class="odd">
<td>2</td>
<td>train</td>
<td>2.216</td>
</tr>
<tr class="even">
<td>2</td>
<td>eval</td>
<td>2.196</td>
</tr>
<tr class="odd">
<td>3</td>
<td>train</td>
<td>2.180</td>
</tr>
<tr class="even">
<td>3</td>
<td>eval</td>
<td>2.173</td>
</tr>
<tr class="odd">
<td>4</td>
<td>train</td>
<td>2.156</td>
</tr>
<tr class="even">
<td>4</td>
<td>eval</td>
<td>2.163</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-8-output-3.png" class="img-fluid"></p>
</div>
</div>
<p>In this configuration, this seems to give a slightly better performance</p>
</section>
<section id="pytorch-rnn" class="level2">
<h2 class="anchored" data-anchor-id="pytorch-rnn">PyTorch RNN</h2>
<p>Let’s use PyTorch’s <code>nn.RNN</code>, in the <code>PyTorchRNN</code> class defined below, we can see that this module replaces our <code>hidden2hidden</code> layer, and also takes care of the loop over the tokens in the context in our forward pass.</p>
<p>Some notes:</p>
<ul>
<li><code>nn.RNN</code> uses a tanh activation by default, we can use a relu activation instead by setting the <code>nonlinearity</code> argument</li>
<li>the (dummy) activations that we add for the processing of the first token need to have an additional empty dimension in first position (for the non bidirectional case) according to the <a href="https://pytorch.org/docs/stable/generated/torch.nn.RNN.html">documentation</a></li>
<li>when forwarding data through the rnn module, it expects the input data to have the context_length as first dimension. We can alter this by setting <code>batch_first=True</code></li>
<li><code>nn.RNN</code> returns two parameters. The first parameter is a tensor containing the hidden activation from each iteration, the second parameter has the final hidden activation. For the moment we are only interested in the last hidden activations and pass it to the output layer but we will use the aggregated hidden activations in a section below</li>
</ul>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PyTorchRNN(nn.Module):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, c2i, context_length, hidden_size):</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.c2i <span class="op">=</span> c2i</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_size <span class="op">=</span> hidden_size</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input2hidden   <span class="op">=</span> nn.Embedding(<span class="bu">len</span>(c2i), hidden_size)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rnn            <span class="op">=</span> nn.RNN(hidden_size, hidden_size, nonlinearity<span class="op">=</span><span class="st">'relu'</span>, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden2out     <span class="op">=</span> nn.Linear(hidden_size, <span class="bu">len</span>(c2i))</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> <span class="va">self</span>.input2hidden(x) <span class="co"># [bs, context_length, hidden_size]</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> torch.zeros((<span class="dv">1</span>, x.shape[<span class="dv">0</span>], <span class="va">self</span>.hidden_size)).to(device)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>        hs, h <span class="op">=</span> <span class="va">self</span>.rnn(inputs, h)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hs : [bs, context_length, hidden_size] -&gt; all hidden states</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># h  : [1, bs, hidden_size]              -&gt; final hidden state</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.hidden2out(h.squeeze(<span class="dv">0</span>)) <span class="co"># squeeze out the first empty dimension</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>rnn <span class="op">=</span> PyTorchRNN(c2i, context_length, n_h)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> Learner(rnn, dls, F.cross_entropy, torch.optim.Adam, <span class="va">None</span>, subs<span class="op">=</span>subs <span class="op">+</span> [SchedulerS(scheduler)])</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>l.fit(epochs, lr<span class="op">=</span>lr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">mode</th>
<th data-quarto-table-cell-role="th">loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>train</td>
<td>2.570</td>
</tr>
<tr class="even">
<td>0</td>
<td>eval</td>
<td>2.285</td>
</tr>
<tr class="odd">
<td>1</td>
<td>train</td>
<td>2.261</td>
</tr>
<tr class="even">
<td>1</td>
<td>eval</td>
<td>2.240</td>
</tr>
<tr class="odd">
<td>2</td>
<td>train</td>
<td>2.217</td>
</tr>
<tr class="even">
<td>2</td>
<td>eval</td>
<td>2.200</td>
</tr>
<tr class="odd">
<td>3</td>
<td>train</td>
<td>2.180</td>
</tr>
<tr class="even">
<td>3</td>
<td>eval</td>
<td>2.176</td>
</tr>
<tr class="odd">
<td>4</td>
<td>train</td>
<td>2.156</td>
</tr>
<tr class="even">
<td>4</td>
<td>eval</td>
<td>2.166</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-10-output-3.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="optimizing-dataloading-and-model-for-sequential-data-bptt" class="level2">
<h2 class="anchored" data-anchor-id="optimizing-dataloading-and-model-for-sequential-data-bptt">Optimizing dataloading and model for sequential data (BPTT)</h2>
<p>Although the previous RNN’s are perfectly valid models, there is a pretty big improvement we can make for the training of these models. To realize that, let’s have a look at the samples for the first name <em>Yuheng</em> in our dataset:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>xb, yb <span class="op">=</span> dls.train.dataset[<span class="dv">0</span>:<span class="dv">6</span>]</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> x,y <span class="kw">in</span> <span class="bu">zip</span>(xb, yb):</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="bu">list</span>(i2c[i.item()] <span class="cf">for</span> i <span class="kw">in</span> x), <span class="st">'--&gt;'</span> ,i2c[y.item()])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['.', '.', '.'] --&gt; y
['.', '.', 'y'] --&gt; u
['.', 'y', 'u'] --&gt; h
['y', 'u', 'h'] --&gt; e
['u', 'h', 'e'] --&gt; n
['h', 'e', 'n'] --&gt; g</code></pre>
</div>
</div>
<p>Consider what happens when we process this data <em>in order</em> with a batch size of 1. In the very first batch (forward pass) we start with an empty hidden state and compute the final hidden state <span class="math inline">\(h_0\)</span> from processing the three <code>'.'</code> tokens sequentially. These final activations are mapped to output activations and the loss is computed by comparing to the label <code>'y'</code>.</p>
<p>When moving to the second batch we start again with an empty hidden state, and since we have moved one letter forward we will process 2 <code>'.'</code> tokens and the <code>'y</code> token sequentially to get to the final hidden activation <span class="math inline">\(h_1\)</span> of this sample. But this is a bit silly, we could have just remembered our hidden activations <span class="math inline">\(h_0\)</span> from the first batch and process the next token (<code>'y'</code>) to get to the same result!</p>
<p>In other words: for each forward pass, we reinitialize the dummy hidden layer activations, and the first two tokens we process are in fact already processed in the previous sample. We could be more efficient by just using the hidden activations we computed on the previous sample and have one iteration over the new token in the context:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="rnn2.drawio.png" class="img-fluid figure-img" width="600"></p>
<figcaption class="figure-caption">More efficient data-processing for sequential data</figcaption>
</figure>
</div>
<p>Let’s simplify things and make the architecture symmetrical by not treating the very first sample in a different way. This also means we can get rid of the triple start symbol (<code>...</code>) and replace it with a single start symbol (<code>.</code>).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="rnn3.drawio.png" class="img-fluid figure-img" width="600"></p>
<figcaption class="figure-caption">Symmetric architecture for efficient data-processing</figcaption>
</figure>
</div>
<p>With these changes, there are two practical problems we need to address:</p>
<ol type="1">
<li>How are we going to change the dataloader and dataset to feed this into a neural network?</li>
<li>Feeding the hidden activations into the processing of the next token means that the computational graph of the loss is getting more and more complex. If we do this for the entirety of our dataset, the backward pass will become intractable. We thus have to cut it off at some point.</li>
</ol>
<p>We need to go from samples (combinations of <code>X</code> and <code>y</code>) that look like this (and don’t pass over the hidden state from sample to sample):</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>xb, yb <span class="op">=</span> dls.train.dataset[<span class="dv">0</span>:<span class="dv">7</span>]</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> x,y <span class="kw">in</span> <span class="bu">zip</span>(xb, yb):</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="bu">list</span>(i2c[i.item()] <span class="cf">for</span> i <span class="kw">in</span> x), <span class="st">'--&gt;'</span> ,i2c[y.item()])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>['.', '.', '.'] --&gt; y
['.', '.', 'y'] --&gt; u
['.', 'y', 'u'] --&gt; h
['y', 'u', 'h'] --&gt; e
['u', 'h', 'e'] --&gt; n
['h', 'e', 'n'] --&gt; g
['e', 'n', 'g'] --&gt; .</code></pre>
</div>
</div>
<p>to samples that simply look like this (and <em>do</em> pass the hidden state from sample to sample):</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>name <span class="op">=</span> <span class="st">'.yuheng.'</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(name)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"['</span><span class="sc">{</span>name[i]<span class="sc">}</span><span class="ss">'] --&gt; </span><span class="sc">{</span>name[i<span class="op">+</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>['.'] --&gt; y
['y'] --&gt; u
['u'] --&gt; h
['h'] --&gt; e
['e'] --&gt; n
['n'] --&gt; g
['g'] --&gt; .</code></pre>
</div>
</div>
<p>However, we want to keep the sequence length dimension since we will use that as the maximal amount of tokens through which we will backpropagate. This addresses the second issue mentioned above. We thus need a dataset that looks like this (for a sequence length of 3):</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"['.', 'y', 'u']"</span>, <span class="st">'--&gt;'</span>, <span class="st">"['y', 'u', 'h']"</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"['h', 'e', 'n']"</span>, <span class="st">'--&gt;'</span>, <span class="st">"['e', 'n', 'g']"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>['.', 'y', 'u'] --&gt; ['y', 'u', 'h']
['h', 'e', 'n'] --&gt; ['e', 'n', 'g']</code></pre>
</div>
</div>
<p>For the above example, we would still pass the hidden state computed in the first sample to the second sample, but only the values are passed and not the computational history. This is essentially what’s referred to as (truncated) backpropagation through time. Also, note that this means that one single sample (<code>['.', 'y', 'u']</code>) will create 3 outputs (for the 3 labels <code>['y', 'u', 'h']</code>). This means we will have to update our loss function as well.</p>
<p>Let’s create a new dataset for this purpose:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SequentialDataset():</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, lines, c2i, sequence_length):</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> <span class="st">"."</span> <span class="op">+</span> <span class="st">"."</span>.join(lines) <span class="op">+</span> <span class="st">"."</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.x <span class="op">=</span> []</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.y <span class="op">=</span> []</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(text) <span class="op">-</span> sequence_length <span class="op">-</span> <span class="dv">1</span>, sequence_length):</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.x.append([c2i[xi] <span class="cf">for</span> xi <span class="kw">in</span> text[i: i<span class="op">+</span>sequence_length]])</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.y.append([c2i[yi] <span class="cf">for</span> yi <span class="kw">in</span> text[i<span class="op">+</span><span class="dv">1</span>: i<span class="op">+</span>sequence_length<span class="op">+</span><span class="dv">1</span>]])</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.x <span class="op">=</span> torch.tensor(<span class="va">self</span>.x)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.y <span class="op">=</span> torch.tensor(<span class="va">self</span>.y)</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, i):</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.x[i], <span class="va">self</span>.y[i]</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.x)</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>train_ds <span class="op">=</span> SequentialDataset(train_lines, c2i, <span class="dv">3</span>)</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>valid_ds <span class="op">=</span> SequentialDataset(val_lines, c2i, <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>([i2c[xi.item()] <span class="cf">for</span> xi <span class="kw">in</span> train_ds[i][<span class="dv">0</span>]], <span class="st">'--&gt;'</span>, [i2c[xi.item()] <span class="cf">for</span> xi <span class="kw">in</span> train_ds[i][<span class="dv">1</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['.', 'y', 'u'] --&gt; ['y', 'u', 'h']
['h', 'e', 'n'] --&gt; ['e', 'n', 'g']</code></pre>
</div>
</div>
<p>Furthermore, we need to order our data in a different way. We need to be very careful with shuffling: we certainly can’t shuffle individual samples anymore in our Dataloader, since we need to maintain the sequence of our data.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>We could however shuffle the order of the <strong>complete</strong> names, at the beginning of each epoch. But let’s not be concerned with that for now, and just disable shuffling altogether.</p>
</div>
</div>
<p>Also, we need to make sure that the stream <em>continues from sample to sample across batches</em>, so that we can pass the (final) hidden states from the samples in a batch to the samples in the next batch. Consider a dataset consisting of 100 sequential samples:</p>
<p><img src="rnn5.drawio.png" class="img-fluid" width="600"></p>
<p>If we order the data in this way, only the very first batch of each epoch will be using a dummy hidden state equal to zero, all other iterations in all the batches will be using the correct (previously computed and sequentially passed) hidden state.</p>
<p>To get these batches into the model during training, we need to order the data <em>vertically</em>, so 1, 21, 41, 61, 81, 2, 22, 42, etc. Let’s do so with a custom Sampler (for details on samplers, check out this earlier blog <a href="https://lucasvw.github.io/posts/09_nntrain_ds/#next-up-shuffling-the-data">post</a>):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="sampler_order.drawio.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">“Vertical” sampling order</figcaption>
</figure>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VerticalSampler():</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, ds, batch_size):</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.batch_size <span class="op">=</span> batch_size</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.batches <span class="op">=</span> <span class="bu">len</span>(ds) <span class="op">//</span> <span class="va">self</span>.batch_size</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__iter__</span>(<span class="va">self</span>):</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.batches):</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.batch_size):</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>                <span class="cf">yield</span> i <span class="op">+</span> <span class="va">self</span>.batches<span class="op">*</span>j</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.batches <span class="op">*</span> <span class="va">self</span>.batch_size</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>With this sampler we can then create a dataloader with the associated batch size to load exactly the batches into the model as described!</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>train_dl <span class="op">=</span> torch.utils.data.DataLoader(train_ds, shuffle<span class="op">=</span><span class="va">False</span>, sampler<span class="op">=</span>VerticalSampler(train_ds, bs), batch_size<span class="op">=</span>bs)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>valid_dl <span class="op">=</span> torch.utils.data.DataLoader(valid_ds, shuffle<span class="op">=</span><span class="va">False</span>, sampler<span class="op">=</span>VerticalSampler(valid_ds, bs), batch_size<span class="op">=</span>bs)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> DataLoaders(train_dl, valid_dl)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let’s update our model to pass all the hidden states to the output layer:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PyTorchRNNMultiOutput(nn.Module):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, c2i, hidden_size, bs):</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.c2i <span class="op">=</span> c2i</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bs <span class="op">=</span> bs</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_size <span class="op">=</span> hidden_size</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'h'</span>, torch.zeros((<span class="dv">1</span>, bs, <span class="va">self</span>.hidden_size)))</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input2hidden   <span class="op">=</span> nn.Embedding(<span class="bu">len</span>(c2i), hidden_size)</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rnn            <span class="op">=</span> nn.RNN(hidden_size, hidden_size, nonlinearity<span class="op">=</span><span class="st">'relu'</span>, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden2out     <span class="op">=</span> nn.Linear(hidden_size, <span class="bu">len</span>(c2i))</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> <span class="va">self</span>.input2hidden(x)</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>        hs, h <span class="op">=</span> <span class="va">self</span>.rnn(inputs, <span class="va">self</span>.h)</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hs : [bs, context_length, hidden_size]</span></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># h  : [1, bs, hidden_size]</span></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> h.detach()                      <span class="co"># detach the computational graph</span></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.hidden2out(hs)               <span class="co"># pass all hidden states to the output layer</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>And finally, we have to create a custom loss function that can deal with the multiple output activations and multiple labels:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> multi_output_cross_entropy(logits, targets):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># logits = [bs, context_length, output_classes]</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># targets = [bs, context_length]</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    targets <span class="op">=</span> targets.view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>    bs, context_length, output_classes <span class="op">=</span> logits.shape</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> logits.view(<span class="op">-</span><span class="dv">1</span>, output_classes)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> F.cross_entropy(logits, targets)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_dls(context_length, batch_size):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    train_ds <span class="op">=</span> SequentialDataset(train_lines, c2i, context_length)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    valid_ds <span class="op">=</span> SequentialDataset(val_lines, c2i, context_length)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    train_dl <span class="op">=</span> torch.utils.data.DataLoader(train_ds, shuffle<span class="op">=</span><span class="va">False</span>, sampler<span class="op">=</span>VerticalSampler(train_ds, batch_size), batch_size<span class="op">=</span>batch_size)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    valid_dl <span class="op">=</span> torch.utils.data.DataLoader(valid_ds, shuffle<span class="op">=</span><span class="va">False</span>, sampler<span class="op">=</span>VerticalSampler(valid_ds, batch_size), batch_size<span class="op">=</span>batch_size)</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> DataLoaders(train_dl, valid_dl)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that we drop the batch size to a value of 300 (1/3 of 1000). We do this, to roughly keep the amount of weight updates per batch constant. Previously, we had 1000 sequences in the batch, and from each sequence we would receive one “weight update”. Now, we have 3 times the amount of sequences in a batch, so we have to reduce the batch size by 3 to keep the amount of “weight updates” roughly equal.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train parameters</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>lr     <span class="op">=</span> <span class="fl">1e-2</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>bs     <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameters</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>context_length <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>n_h            <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> get_dls(context_length, bs)</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>rnn <span class="op">=</span> PyTorchRNNMultiOutput(c2i, n_h, bs)</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>subs <span class="op">=</span> [ProgressS(<span class="va">True</span>),</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>        MetricsS(),</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>        DeviceS(device)]</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> partial(torch.optim.lr_scheduler.OneCycleLR, max_lr<span class="op">=</span>lr, epochs<span class="op">=</span>epochs, steps_per_epoch<span class="op">=</span><span class="bu">len</span>(dls.train))</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> Learner(rnn, dls, multi_output_cross_entropy, torch.optim.Adam, <span class="va">None</span>, subs<span class="op">=</span>subs <span class="op">+</span> [SchedulerS(scheduler)])</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>l.fit(epochs, lr<span class="op">=</span>lr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">mode</th>
<th data-quarto-table-cell-role="th">loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>train</td>
<td>2.528</td>
</tr>
<tr class="even">
<td>0</td>
<td>eval</td>
<td>2.224</td>
</tr>
<tr class="odd">
<td>1</td>
<td>train</td>
<td>2.176</td>
</tr>
<tr class="even">
<td>1</td>
<td>eval</td>
<td>2.174</td>
</tr>
<tr class="odd">
<td>2</td>
<td>train</td>
<td>2.117</td>
</tr>
<tr class="even">
<td>2</td>
<td>eval</td>
<td>2.130</td>
</tr>
<tr class="odd">
<td>3</td>
<td>train</td>
<td>2.064</td>
</tr>
<tr class="even">
<td>3</td>
<td>eval</td>
<td>2.104</td>
</tr>
<tr class="odd">
<td>4</td>
<td>train</td>
<td>2.026</td>
</tr>
<tr class="even">
<td>4</td>
<td>eval</td>
<td>2.093</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-22-output-3.png" class="img-fluid"></p>
</div>
</div>
<p>Performance is pretty good, however what’s up with the spikes in the loss? These spikes occur at the beginning of every epoch and they happen because moving from the last batch in an epoch, to the first batch in the next epoch is not a sequential step in the data. Looking at the image above in which we laid out the “vertical ordering of the data”, wee see that for the first batch in any epoch &gt; 1, we would feed the hidden state originating from samples (20, 40, 60, 80, 100) into the samples (1, 21, 41, 61, 81), e.g.:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>frm <span class="op">=</span> (<span class="dv">20</span>, <span class="dv">40</span>, <span class="dv">60</span>, <span class="dv">80</span>, <span class="dv">100</span>)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>to  <span class="op">=</span> (<span class="dv">1</span>, <span class="dv">21</span>, <span class="dv">41</span>, <span class="dv">61</span>, <span class="dv">81</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> f, t <span class="kw">in</span> <span class="bu">zip</span>(frm,to):</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'from: </span><span class="sc">{</span>f<span class="sc">:3d}</span><span class="ss"> to: </span><span class="sc">{</span>t<span class="sc">:2d}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>from:  20 to:  1
from:  40 to: 21
from:  60 to: 41
from:  80 to: 61
from: 100 to: 81</code></pre>
</div>
</div>
<p>And this is obviously not sequential. The easiest solution is to reset the hidden state to zero at the beginning of each epoch, let’s do this with a very small <code>Subscriber</code>:</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Based on the observation that we could realign the hidden activations by moving them one row down, I implemented this solution but was surprised to see that this doesn’t improve things at all. I then found that we generally don’t have a fully connected sequential dataset that fits exactly into a whole number of batches, we thus lose some data in the end, and the data doesn’t connect easily by shifting it “one row down”. In terms of the sample ordering diagram above: we generally don’t have (20, 40, 60, 80, 100) as final batch of an epoch but instead something like (18, 38, 58, 78, 98).</p>
</div>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> HiddenStateResetterS(Subscriber):</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> before_epoch(<span class="va">self</span>, learn):</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>        learn.model.reset_hidden_state()</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="at">@fc.patch</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> reset_hidden_state(<span class="va">self</span>:PyTorchRNNMultiOutput):</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> <span class="va">self</span>.h.get_device()</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.h <span class="op">=</span> torch.zeros_like(<span class="va">self</span>.h).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train parameters</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>lr     <span class="op">=</span> <span class="fl">1e-2</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>bs     <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameters</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>context_length <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>n_h            <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> get_dls(context_length, bs)</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>rnn <span class="op">=</span> PyTorchRNNMultiOutput(c2i, n_h, bs)</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>subs <span class="op">=</span> [ProgressS(<span class="va">True</span>),</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>        MetricsS(),</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>        DeviceS(device),</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>        HiddenStateResetterS()]</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> partial(torch.optim.lr_scheduler.OneCycleLR, max_lr<span class="op">=</span>lr, epochs<span class="op">=</span>epochs, steps_per_epoch<span class="op">=</span><span class="bu">len</span>(dls.train))</span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> Learner(rnn, dls, multi_output_cross_entropy, torch.optim.Adam, <span class="va">None</span>, subs<span class="op">=</span>subs <span class="op">+</span> [SchedulerS(scheduler)])</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>l.fit(epochs, lr<span class="op">=</span>lr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">mode</th>
<th data-quarto-table-cell-role="th">loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>train</td>
<td>2.532</td>
</tr>
<tr class="even">
<td>0</td>
<td>eval</td>
<td>2.221</td>
</tr>
<tr class="odd">
<td>1</td>
<td>train</td>
<td>2.177</td>
</tr>
<tr class="even">
<td>1</td>
<td>eval</td>
<td>2.159</td>
</tr>
<tr class="odd">
<td>2</td>
<td>train</td>
<td>2.114</td>
</tr>
<tr class="even">
<td>2</td>
<td>eval</td>
<td>2.118</td>
</tr>
<tr class="odd">
<td>3</td>
<td>train</td>
<td>2.065</td>
</tr>
<tr class="even">
<td>3</td>
<td>eval</td>
<td>2.086</td>
</tr>
<tr class="odd">
<td>4</td>
<td>train</td>
<td>2.025</td>
</tr>
<tr class="even">
<td>4</td>
<td>eval</td>
<td>2.076</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-25-output-3.png" class="img-fluid"></p>
</div>
</div>
<p>The loss improved a bit from this fix, and the spikes are smaller, but still visible. This is not surprising since the very first batch in each epoch still has the difficulty of working with an empty hidden state. This is not a big issue however, since the hidden state is quickly filling up. Accordingly, the following batches show again a loss that is in-line with the loss of the end of the previous epoch.</p>
<p>Last but not least, PyTorch <code>nn.RNN</code> accepts two parameters, one for the features in the inputs and one for the features in the hidden state. Previously we used one single parameter for this to make sure we can add the activations. Apparently, <code>nn.RNN</code> is doing another multiplication to align them. So let’s use these two values as separate arguments to see whether we can further improve performance through tweaking them.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PyTorchRNNMultiOutput(nn.Module):</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, c2i, embedding_dim, hidden_size, bs):</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.c2i <span class="op">=</span> c2i</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bs <span class="op">=</span> bs</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding_dim <span class="op">=</span> embedding_dim</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_size <span class="op">=</span> hidden_size</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># register as buffer so that its moved to the device by the DeviceS Subscriber</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'h'</span>, torch.zeros((<span class="dv">1</span>, bs, <span class="va">self</span>.hidden_size))) </span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input2hidden   <span class="op">=</span> nn.Embedding(<span class="bu">len</span>(c2i), embedding_dim)</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rnn            <span class="op">=</span> nn.RNN(embedding_dim, hidden_size, nonlinearity<span class="op">=</span><span class="st">'relu'</span>, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden2out     <span class="op">=</span> nn.Linear(hidden_size, <span class="bu">len</span>(c2i))</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> <span class="va">self</span>.input2hidden(x)</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>        hs, h <span class="op">=</span> <span class="va">self</span>.rnn(inputs, <span class="va">self</span>.h)</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hs : [bs, context_length, hidden_size]</span></span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># h  : [1, bs, hidden_size]</span></span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> h.detach()</span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.hidden2out(hs)</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reset_hidden_state(<span class="va">self</span>):</span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> <span class="va">self</span>.h.get_device()</span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> torch.zeros_like(<span class="va">self</span>.h).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Trying for around 10 minutes with different settings shows me that performance can be slightly improved by increasing the size of the hidden state by quite a bit, and keeping the embedding dimension smaller:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train parameters</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>lr     <span class="op">=</span> <span class="fl">1e-2</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>bs     <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameters</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>context_length <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>n_h            <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>embedding_dim  <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> get_dls(context_length, bs)</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>rnn <span class="op">=</span> PyTorchRNNMultiOutput(c2i, embedding_dim, n_h, bs)</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>subs <span class="op">=</span> [ProgressS(<span class="va">True</span>),</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>        MetricsS(),</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>        DeviceS(device),</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>        HiddenStateResetterS()]</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> partial(torch.optim.lr_scheduler.OneCycleLR, max_lr<span class="op">=</span>lr, epochs<span class="op">=</span>epochs, steps_per_epoch<span class="op">=</span><span class="bu">len</span>(dls.train))</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> Learner(rnn, dls, multi_output_cross_entropy, torch.optim.Adam, <span class="va">None</span>, subs<span class="op">=</span>subs <span class="op">+</span> [SchedulerS(scheduler)])</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>l.fit(epochs, lr<span class="op">=</span>lr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">mode</th>
<th data-quarto-table-cell-role="th">loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>train</td>
<td>2.391</td>
</tr>
<tr class="even">
<td>0</td>
<td>eval</td>
<td>2.184</td>
</tr>
<tr class="odd">
<td>1</td>
<td>train</td>
<td>2.130</td>
</tr>
<tr class="even">
<td>1</td>
<td>eval</td>
<td>2.126</td>
</tr>
<tr class="odd">
<td>2</td>
<td>train</td>
<td>2.042</td>
</tr>
<tr class="even">
<td>2</td>
<td>eval</td>
<td>2.055</td>
</tr>
<tr class="odd">
<td>3</td>
<td>train</td>
<td>1.934</td>
</tr>
<tr class="even">
<td>3</td>
<td>eval</td>
<td>2.007</td>
</tr>
<tr class="odd">
<td>4</td>
<td>train</td>
<td>1.825</td>
</tr>
<tr class="even">
<td>4</td>
<td>eval</td>
<td>1.985</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-27-output-3.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="multilayer-rnns" class="level2">
<h2 class="anchored" data-anchor-id="multilayer-rnns">MultiLayer RNN’s</h2>
<p>Last but not least, RNN’s can be stacked on top of eachother. In the diagram below, a 2-layer RNN with a sequence length of 3 is shown. The only difference is that instead of passing the hidden activations of the first layer directly through the hidden-to-out layer to get to our output activations, we pass it through an additional hidden-to-hidden layer (with it’s own weights).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="rnnmulti.drawio.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Multilayer RNN</figcaption>
</figure>
</div>
<p>Two implement this in our model is extremely easy, the only thing we have to change is adding a <code>layers</code> argument into the model which is passed both to the creation of the dummy activations as well as to the <code>nn.RNN</code> module:</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiLayerRNN(nn.Module):</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, c2i, embedding_dim, hidden_size, bs, layers<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.c2i <span class="op">=</span> c2i</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bs <span class="op">=</span> bs</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding_dim <span class="op">=</span> embedding_dim</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_size <span class="op">=</span> hidden_size</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'h'</span>, torch.zeros((layers, bs, <span class="va">self</span>.hidden_size)))    <span class="co"># &lt;- initialize dummy activations for n layers</span></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input2hidden   <span class="op">=</span> nn.Embedding(<span class="bu">len</span>(c2i), embedding_dim)               <span class="co"># multiple layers in the rnn module ↓ </span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rnn            <span class="op">=</span> nn.RNN(embedding_dim, hidden_size, nonlinearity<span class="op">=</span><span class="st">'relu'</span>, batch_first<span class="op">=</span><span class="va">True</span>, num_layers<span class="op">=</span>layers)</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden2out     <span class="op">=</span> nn.Linear(hidden_size, <span class="bu">len</span>(c2i))</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> <span class="va">self</span>.input2hidden(x)</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>        hs, h <span class="op">=</span> <span class="va">self</span>.rnn(inputs, <span class="va">self</span>.h)</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hs : [bs, context_length, hidden_size]</span></span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># h  : [layers, bs, hidden_size]</span></span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> h.detach()</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.hidden2out(hs)</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reset_hidden_state(<span class="va">self</span>):</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> <span class="va">self</span>.h.get_device()</span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.h <span class="op">=</span> torch.zeros_like(<span class="va">self</span>.h).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train parameters</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>lr     <span class="op">=</span> <span class="fl">1e-2</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>bs     <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameters</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>context_length <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>n_h            <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>embedding_dim  <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>layers         <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> get_dls(context_length, bs)</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>rnn <span class="op">=</span> MultiLayerRNN(c2i, embedding_dim, n_h, bs, layers)</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>subs <span class="op">=</span> [ProgressS(<span class="va">True</span>),</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>        MetricsS(),</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>        DeviceS(device),</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>        HiddenStateResetterS()]</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> partial(torch.optim.lr_scheduler.OneCycleLR, max_lr<span class="op">=</span>lr, epochs<span class="op">=</span>epochs, steps_per_epoch<span class="op">=</span><span class="bu">len</span>(dls.train))</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> Learner(rnn, dls, multi_output_cross_entropy, torch.optim.Adam, <span class="va">None</span>, subs<span class="op">=</span>subs <span class="op">+</span> [SchedulerS(scheduler)])</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>l.fit(epochs, lr<span class="op">=</span>lr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">mode</th>
<th data-quarto-table-cell-role="th">loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>train</td>
<td>2.363</td>
</tr>
<tr class="even">
<td>0</td>
<td>eval</td>
<td>2.207</td>
</tr>
<tr class="odd">
<td>1</td>
<td>train</td>
<td>2.160</td>
</tr>
<tr class="even">
<td>1</td>
<td>eval</td>
<td>2.135</td>
</tr>
<tr class="odd">
<td>2</td>
<td>train</td>
<td>2.066</td>
</tr>
<tr class="even">
<td>2</td>
<td>eval</td>
<td>2.064</td>
</tr>
<tr class="odd">
<td>3</td>
<td>train</td>
<td>1.948</td>
</tr>
<tr class="even">
<td>3</td>
<td>eval</td>
<td>2.002</td>
</tr>
<tr class="odd">
<td>4</td>
<td>train</td>
<td>1.827</td>
</tr>
<tr class="even">
<td>4</td>
<td>eval</td>
<td>1.976</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-29-output-3.png" class="img-fluid"></p>
</div>
</div>
<p>And this performs even better then the model we obtained with a single layer!</p>
</section>
<section id="sampling-names" class="level2">
<h2 class="anchored" data-anchor-id="sampling-names">Sampling names</h2>
<p>And as we did with the MLP, let’s conclude with some sampled names from this model</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="at">@fc.patch</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate(<span class="va">self</span>:MultiLayerRNN, n<span class="op">=</span><span class="dv">10</span>, generator<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For unbatched input we need a 2D hidden state tensor of size [1, hidden_size]</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.h <span class="op">=</span> torch.zeros((<span class="va">self</span>.h.shape[<span class="dv">0</span>], <span class="va">self</span>.hidden_size)).cuda()</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>    names <span class="op">=</span> []</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>        name <span class="op">=</span> <span class="st">'.'</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>            idx <span class="op">=</span> torch.tensor([c2i[name[<span class="op">-</span><span class="dv">1</span>]]]).cuda()</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> <span class="va">self</span>.forward(idx)</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>            s <span class="op">=</span> torch.multinomial(F.softmax(logits, dim<span class="op">=</span><span class="dv">1</span>), <span class="dv">1</span>, generator<span class="op">=</span>generator)</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>            c <span class="op">=</span> i2c[s.item()]</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>            name <span class="op">+=</span> c</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> c <span class="op">==</span> <span class="st">'.'</span>:</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>                names.append(name)</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> names</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>rnn.generate()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>['.erwan.',
 '.isabel.',
 '.jestelyn.',
 '.hubri.',
 '.coltyn.',
 '.hasseem.',
 '.ramarion.',
 '.ariaad.',
 '.rayron.',
 '.ciron.']</code></pre>
</div>
</div>
</section>
<section id="outlook" class="level2">
<h2 class="anchored" data-anchor-id="outlook">Outlook</h2>
<p>We have come quite a long way, and we have seen that an RNN is in essence not very different from the MLP created earlier. The complexity in my opinion only has to do with the different way of processing and loading the data, which greatly reduces the time to train such models. In the next post we are going to have a look at long short-term memory (LSTM) networks and gated recurrent unit networks (with lot’s of fancy diagrams 😏).</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="lucasvw/BlogComments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>