{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5a706372-b4f8-4d32-9470-24b837947fa5",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"`nntrain`: a small library for training neural networks (1/n)\"\n",
    "author: \"Lucas van Walstijn\"\n",
    "date: \"2023-08-09\"\n",
    "categories: [Code, Neural Network, Deep Learning]\n",
    "image: \"image.jpeg\"\n",
    "comments:\n",
    "  utterances:\n",
    "    repo: lucasvw/BlogComments\n",
    "format:\n",
    "  html:\n",
    "    code-overflow: wrap\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438a6911-402a-4234-9a58-7e1530bcb1bb",
   "metadata": {},
   "source": [
    "In this series, I want to discuss the creation of a small library for training neural networks. It's based off the excellent [part 2](https://course.fast.ai/) of Practical Deep Learning for Coders by Jeremy Howard, in which from lessons 13 to 18 (roughly) the development of the `miniai` library is discussed.\n",
    "\n",
    "The library will build upon PyTorch. However, we'll create things as much as possible from scratch to understand how it all works. Once the main functionality of components are implemented and verified, we can use PyTorch's version, mainly for performance reasons and interoperability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc98b7c4-9002-407d-90f6-ea9d8c07b8ae",
   "metadata": {},
   "source": [
    "As we'll see, the library will be built using [`nb_dev`](https://nbdev.fast.ai/), another great project from the fastai community. With this software, it becomes very straight forward to create python libraries which are exported from jupyter notebooks. This may sound a bit weird, but it has the advantage that we can create the sourcecode for our library in the very same environment in which we want to experiment and interact with our methods, objects and structure **while we are building the library**. For more details on why this is a good idea, see [here](https://www.fast.ai/posts/2022-07-28-nbdev2.html). In fact, I am going to write the library in notebooks that are also the sourcecode for this very blog, kind of crazyðŸ¤ª."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a4b6e1-9737-470a-980b-1aa3df5f5268",
   "metadata": {},
   "source": [
    "So without further ado, let's start with some data! To keep things simple, let's use the fashion-mnist dataset. We can get the data from the huggingface datasets library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4de5add-ae69-46ee-aa4f-a38487830fa9",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a training set of\n",
      "60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image,\n",
      "associated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in\n",
      "replacement for the original MNIST dataset for benchmarking machine learning algorithms.\n",
      "It shares the same image size and structure of training and testing splits.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset,load_dataset_builder\n",
    "\n",
    "name = \"fashion_mnist\"\n",
    "ds_builder = load_dataset_builder(name)\n",
    "print(ds_builder.info.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a70f9b3-fef1-4a54-bf22-581c9f651a7e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset fashion_mnist (/root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1)\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(name, split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba93a563-aac7-4015-9a95-c42d8d3d3749",
   "metadata": {},
   "source": [
    "`ds` is a `Dataset` object. These kind of objects appear in many Deep Learning libraries and have two main functionalities: you can index into them and they have a length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3960543f-0fdb-4316-aef8-a3d51a42e287",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n",
       " 'label': 9}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71add5de-4c4f-45bb-a694-606dd45603b2",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ea37f9-7d1e-45e0-9840-dd79d47f828d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label'],\n",
       "    num_rows: 60000\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ade5bd0-9411-4f7c-9f18-634f84931e72",
   "metadata": {},
   "source": [
    "Hugginface datasets also have some properties, in this case `num_rows`, which is the length of the dataset (60000) and `features`, a dictionary giving metadata on what is returned when we index into the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4059340-2d94-4fba-be7a-2dae8760bc04",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': Image(decode=True, id=None),\n",
       " 'label': ClassLabel(num_classes=10, names=['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], id=None)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5524187-43f6-484b-ac44-ae581ba3dec7",
   "metadata": {},
   "source": [
    "Let's visualize one single item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b83ab5-7852-4b6d-91c6-30e7403dead2",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARwAAAD3CAYAAADPAOsVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAVmElEQVR4nO3de5RdZXnH8d+Te2aSkIBAyJVLyIUQ0mIRIVFiDVRQEFdrLUSusqR2tah4qVgpUUSpttSuWkMrFgygSLUGA1rAQqCaxGSxkrUCJBAJM7kRck8m99vbP/Y7cBhmP++ZOck7k+T7WWvWOie/8+7z7r3Pec4++7x5t4UQBAA5dOnoDgA4elBwAGRDwQGQDQUHQDYUHADZUHAAZJMsOGbWYGaTq1mYmQUzG9GejtTS9lAys6lm9oCTv2Bmk6pYTtXrZ2ZfNrN7qu/lkc3MTo7br1tH9+VgM7Nrzew3bc2qWO4sM7uhtt4dfIf1EY6Z9TezH5rZ2vg3NXcfQghjQwizDvIyvxFC6HQvllza8iGXU3wTT4ofQlNbZKeY2QEzm9ZB3cvCzO4zs6+3+LeG+KFwn5ld67U/rAuOpH+WVCfpZEnvknSVmV3XoT2CzKxrR/ehA1wtaZOkj5lZz47uTGfVpoJjZu8yszlmttnMXjOz75pZjxYPu8TMlpnZejP7tpl1qWh/vZktNrNNZva4mQ2vsf+XSvpWCGFHCKFB0g8kXV/lujSa2Tvj7SnxkH1svP8JM5tR8fAeZjbdzJriV6g/qljOG5/GZtY1fh16JT72OTMbWrGcyWa2NG6/fzMzK+nbG1/jzKyXmT1gZhtiu/lmdmJJuy9VPPeLZvaRiuxaM/tt3GdbzGyJmb2/Ip9lZt80s3lmttXMHjGzYyvy/zKzNbHts83bKmb3mdk0M/ulmW2X9D4zG2RmPzOzdWb2qpnd1GL9Hm5tm5rZ/ZKGSZppZtvM7Ist1vGjZvZci3+72cweKdkm18XXXFN8Xd5YkU0ys5Vm9rl4hPxaez6w4n68WtJXJO1V8bqszIOZ/WWV+/7bZvYbMzumlWy0mT1pZhvN7CUz+/NE105z9udlcbtvjvt+TEU2Jv7b5viYy+K/f1LSFElfjPtmZpWb6E0hBPdPUoOkyfH2OyW9W1I3FUcViyV9puKxQdLTko5V8aJ5WdINMfuwpN9LGhPbf0XS7BZtR8TbX5K0ueyvos16Se+quP93kjal1ik+drqkz8Xb/yHpFUmfqsg+G29PlbRL0iWSukr6pqS5JdvnC5IWSRolySSNl3Rcxfo9Kql/3DbrJH2gpG9TJT0Qb98oaaaKI7mucR/0K2n3UUmDVHyQfEzSdkknxexaSfskfVZS95hvkXRszGdJWiXpTEn1kn7W3IeYXy+pr6Sekr4jaWFFdl9c1oT43HWSnpP095J6SDpV0jJJf9LWbRrvnxy3X7f4/BsljanIF0j605Jt8kFJp8X9cYGkHZLOjtmkuE2+FrfJJTEfUM1rqOI53iNpt6QBkv5V0swWeem+j/vlN3G7fV/S45LqKrN4u17SCknXxe3whype/2eU9Kl0f0oaGV8bF8b1/qKK92aPeP/3kr4c7/+xpCZJoyr29dfbsn3e0q+2FJxWss9I+nmLDfuBivt/Jel/4+1fSfpERdYl7tzhLQtOG3b0A5L+W8UbYYSKorG7yrafkPSLeHuxpBskPRTvN1a8KKdK+nVFuzMk7SwpOC9J+nDJ8wVJEyvuPyzpS1UUnOslzZZ0Vpt3rrSwuT/xxbtaklXk8yRdVfECvbPFeu6R1LWV5faP63NMxYtwekV+rqTlLdrcIunetm7TeP/k+Hzd4v1pku6It8eq+CrTs8ptMkPSp+PtSZJ2Ni83/ttaSe9u43a+R9KMePs8FUc5J1Sz7+N++Z2kn6goCj0qHnet3iw4H5P0fy2e998l3VbSp9L9KelWSQ9XZF1UFKdJKornGkldKvIfS5pasa/bXXDa+pVqpJk9Gg+tt0r6hqR3tHjYiorbjSo+cSVpuKR/iYdpm1V8SpmkwW3pQws3qXjBLJX0iIoNs7LKts9Ieo+ZnaRiJzwsaYKZnSzpGBVv1mZrKm7vkNTLWv/FZKiKolem5XL6VNHP+1V86j1kZqvN7Ftm1r21B5rZ1Wa2sGIbn6m37p9VIb5qosr9I71933WX9A4rvireGb+ubVVRENRi2ZVth0sa1NyP2JcvS6r8KljtNm3NDyVdGb+WXKXizbO7tQea2cVmNjd+Ddms4iimst8bQgj7WvSlmv3SvPzeKo4sH5SkEMIcScslXdniod6+H6HiG8BXQwh7Sp5quKRzW2zTKZIGOt1rdX+q2OeNzUEI4UB87OCYrYj/Vtm2lvfpG9p60niapCWSTg8h9FPxImr5XbTynMUwFZ+qUrFCN4YQ+lf89Q4hzG75JFacB9lW9tf8uBDCxhDClBDCwBDC2Lg+86pZkRDC71Xs+L+R9GwIYauKF8UnVXyqHPDal1ih4vD9oAkh7A0hfDWEcIak8yV9SMX5grew4nzY9yX9tYqvcf0lPa+37p/BLc4dVO4f6e37bq+Kw/YrVbwhJqsoxic3P21lVytur5D0aot93TeEcEl1a/2WZb09DGGuik/r98S+3d/a46w4efszSf8o6cS4TX6pt79ma/ERSf0kfS9+EK9R8ea8pg3LWKziq9KvzGxUyWNWSHqmxTbtE0L4lLPcsv25WkUBk/TGOaihKo5yVksaahXnXmPbVfF2TdNLtLXg9JW0VdI2MxstqbWV/YKZDbDiZOmnVRwqStLdkm6xN0/MHmNmH23tSULxs3Cfsr/mx5nZaWZ2XPwEvlhFsfh6RT7L/J/Kn1HxBn0m3p/V4n5b3SPpdjM73Qpnmdlx7VyWJMnM3mdm46z45WerihdNa8WwXsWLYV1sd52KI5xKJ0i6ycy6x20/RsUbsNnHzewMM6tTcV7jpyGE/Sr2+25JG1Scn/lGotvzJDWZ2d+aWe+4f840s3OqXO3XVZz38UyX9F1Je0MIZWNVeqg457NO0r74Grmoyj5U6xpJ/ylpnKQ/iH8TJI03s3HVLiSE8GMVH+C/NrPWPrQelTTSzK6K+6+7mZ1TebK3FWX782FJHzSz98ej5c+p2L+zVXy926HixHB3K8aYXSrpobjMavZNqbYWnM+r+ERpUvFp+pNWHvOIihOGCyU9puKXI4UQfi7pH1R8Ndiq4tP34nb1+k3vVHGStknFiccpIYQXKvKhkn7rtH9GxZvp2ZL7bXWXip35hIri8ANJvdu5rGYDJf00Lm9x7OPbPtFDCC9K+idJc1S8KMbp7ev+O0mnq/iUu0PSn4UQNlTk96v4jr5GUi8VX1ml4s3dqOJT7kVJc70Oxxf1h1S8+V6Nz3ePiqOjanxT0lfiV4fPlzzmfhUFtXRQZgihKa7DwyrO81wp6RdV9iHJzAZLer+k74QQ1lT8PSfpf9S2oxyFEH6oojA8Fb/aV2ZNKorlX6g4Clmj4v3k/QTf6v4MIbwk6eMqTnCvV1FQLg0h7Ilf6S5V8d5cL+l7kq4OISyJy/yBpDPivpnRlvWT4gnEI5GZDVHx3f78ju5LZ2DFgKwbQggTS/JZKk5UHxYjnOO5k7UqTu4v7ej+oDpH3FDxZiGElSrOeeDI9ClJ8yk2h5cjtuDgyGVmDSpO/F7esT1BWx2xX6kAdD6H+/+lAnAY6UxfqTjUAg69gzkGqc04wgGQDQUHQDYUHADZUHAAZEPBAZANBQdANhQcANlQcABkQ8EBkA0FB0A2FBwA2VBwAGRDwQGQDQUHQDYUHADZUHAAZEPBAZANBQdANhQcANlQcABkQ8EBkA0FB0A2nekyMcgsdRFEs9quKLJ79243X7JkSWk2fvz4mp47tW5e3qVLx34O13Jxylr32aHGEQ6AbCg4ALKh4ADIhoIDIBsKDoBsKDgAsqHgAMiGcThHsVrH4WzcuNHN7733Xjevq6trVyZJPXr0cPPhw4e7eS3jVWoZ41ONWsYBHThw4JAt+2DgCAdANhQcANlQcABkQ8EBkA0FB0A2FBwA2VBwAGTDOJyjWK3jRebOnevmjz76qJufcsoppdmuXbvcttu3b3fzgQMHuvkVV1xRmtXX17ttU2N4ap2TZs+ePe1edvfu3Wt67kONIxwA2VBwAGRDwQGQDQUHQDYUHADZUHAAZEPBAZAN43COYl27dq2p/bPPPuvmL774opvv3bu3NEvN63L55Ze7+Zw5c9z81ltvLc0mTJjgtj3zzDPdfMiQIW7+0ksvufns2bNLs/e+971u25EjR7p5r1693PxQ4wgHQDYUHADZUHAAZEPBAZANBQdANhQcANlYrVMUHESdpiNHEm//pqY6eOGFF9z8xhtvdPO1a9e6ec+ePUuzWn+ynzRpkpuPGjWqNPP6JaWn9Vi1apWbpy5xM3HixNJs+vTpbtubb77ZzUePHl3b3Bk14ggHQDYUHADZUHAAZEPBAZANBQdANhQcANlQcABkwzicTu5Q7p/UOJyLLrrIzVPjdFK8dUtd7iQ1VibFuxRMagxQavqK0aNHu3lq3WbMmFGaLVq0yG3b2Njo5pIYhwPg6EDBAZANBQdANhQcANlQcABkQ8EBkA0FB0A2XCamk0uNlTmUjj/+eDdPXXKkb9++br5jx47SbM+ePW7brVu3unnv3r3dvKmpqTRLjcN57LHH3PyJJ55w8/3797v56tWrS7MrrrjCbdvZcYQDIBsKDoBsKDgAsqHgAMiGggMgGwoOgGwoOACyYRwOSm3fvt3NU+NJUnm/fv1Ks9QYoFS+ePFiN/fG2qTmIEqtV2qMULdu/tuuS5fy44Bly5a5bTs7jnAAZEPBAZANBQdANhQcANlQcABkQ8EBkA0FB0A2jMPp5FJjQlK5N6YjNefM0qVL3byurs7NU/Pl7Nq1q91t+/Tp4+br169380GDBpVmqXE0O3fudPMBAwa4+YYNG9x84sSJpdmmTZvctsuXL3fzYcOGufmhxhEOgGwoOACyoeAAyIaCAyAbCg6AbCg4ALLhZ/FOLnWZmAMHDrR72U8//bSbp35i9X5altLTW3hTRGzZssVt6/2kLqV/VvcuUdOzZ0+3bWo4QWq9165d6+a33XZbaTZ//ny3bWrqjI7GEQ6AbCg4ALKh4ADIhoIDIBsKDoBsKDgAsqHgAMjGUtMbZNRpOtKZpMbZeNNPpDQ0NLj5ueee6+a9e/d281r6npoiIvXcJ510kpvv3r27XZkkNTU1uXnqEjYp9fX1pdmdd97ptr3gggtSi/cHdh1iHOEAyIaCAyAbCg6AbCg4ALKh4ADIhoIDIBsKDoBsjpj5cLzxRLVeaiWVe/O6pOazSallnE3KOeec4+Z9+/Z189SlWlJz1njbJjWOZt++fW6empMmNeeNp0ePHm7uvR6kdN/nzp1bmqX2SWfHEQ6AbCg4ALKh4ADIhoIDIBsKDoBsKDgAsqHgAMjmsBmHU8vcKrWOhelIS5cudfOHHnrIzZ966qnSzJt3RUpfdyo1zmbv3r1u3q1b+cuvX79+btvUWBbvulOStG3bttIsNfYpNf4oZefOne1e/o9+9CO37dlnn92uPuXCEQ6AbCg4ALKh4ADIhoIDIBsKDoBsKDgAsqHgAMiG61IpPS5iy5Ytbt7Y2Fiavfbaa27bBx980M3nz5/v5nV1dW6+f//+0iw1J4w3VkWSRowY4eap6zt543hS2y01J01qPpyLL764NEut94wZM9w8NR/OgAED3HzPnj2l2dChQ922CxYscHNxXSoARwsKDoBsKDgAsqHgAMiGggMgGwoOgGwOm5/Fly1b5ja+5ZZbSrOVK1e6bV9//XU37969u5t70zCceOKJbtvUz7upn+R79+7t5t60HqlLjpx11llufvfdd7v55MmT3Xzjxo2l2Zo1a9y2qWk7UkaPHl2abd682W3bv39/N09NrdHU1OTm3j5P9S01HED8LA7gaEHBAZANBQdANhQcANlQcABkQ8EBkA0FB0A2nWYczoEDB9yOXHjhhW77V155pTTzLkcipcfZpMZVeFKXSkmNhanVunXrSrPUWJfHH3/czWfOnOnmt99+u5sPGzasNEtNuzFu3Dg3P+2009z85ZdfLs1WrVrltk2NfUpdPscbfyT503qkpr7w3gcR43AAHB0oOACyoeAAyIaCAyAbCg6AbCg4ALKh4ADIptOMw3nyySfdjlxzzTVu+/Hjx5dmmzZtctum8tS4Co93yQ8pPSYjNZ7k9NNPd/Ply5eXZt4lZCRpxYoVbj5nzhw3T10mpqGhoTTbunWr23bu3LluPmvWLDf35gnq1auX2za13Wp5vUh+31LjuhYtWuTm/fr1YxwOgKMDBQdANhQcANlQcABkQ8EBkA0FB0A2FBwA2fgTxWR0/PHHu/moUaPcfP369aVZnz593LYDBw5081rG6Xj9ktLXrRozZoybp65b5c23U19f77ZNXTPr/PPPd/MJEya4+fPPP1+aefP4SFLPnj3d/Ljjjmt3+9T8SalxOqnxR6k5bbyxcalxXam5fGqZ2+lg4AgHQDYUHADZUHAAZEPBAZANBQdANhQcANkcNj+Lm/n/q37kyJGl2bZt29y2K1eudPMTTjjBzQcNGlSaDR061G2bmm4gNdVB6idYb903bNjgtvWmSZDSwwnmzZvn5t5whREjRtT03Dt27HBzb5+lLhtU62WHdu7c6ebelCKp6WQWLFjg5qlhFocaRzgAsqHgAMiGggMgGwoOgGwoOACyoeAAyIaCAyCbTjMOZ/DgwW4+ZcoUN7/rrrtKs9SlVMaOHevmqekIvLEuqXE027dvd/PUmI19+/a5eV1dXWmWGi+SGvuUmurg1FNPdXNvmobUWJfUNA2pcV3etB6p/T1gwICa8tS0H952W7x4sds29T7qaBzhAMiGggMgGwoOgGwoOACyoeAAyIaCAyAbCg6AbCw1v0ZGNXVk4cKFpdkdd9zhtm1oaHDzYcOGuXn//v1Ls9QlQfbv3+/mqfEmqXE43vJT+z41DifVt9RcPd4YpdT4pVpft1774cOH17Ts1Hp36eJ/zr/66qul2Xnnnee2nTZtmptL8nfqIcYRDoBsKDgAsqHgAMiGggMgGwoOgGwoOACyoeAAyKbTjMMJiY6kxoTUYsmSJW5+0003uXljY2NptnHjRrdt6tpPqXE6qetaeeOAUvt+yJAhbl7LtcIkv2/eNauk9HZJ8fqemieovr7ezVP79LLLLnNzb/6m1BxDVWAcDoCjAwUHQDYUHADZUHAAZEPBAZANBQdANhQcANl0mnE4qnE+nM5q3bp1br5582Y379u3r5uvXbvWzQcOHFiapa79dOyxx7o5DkuMwwFwdKDgAMiGggMgGwoOgGwoOACyoeAAyIafxYGjCz+LAzg6UHAAZEPBAZANBQdANhQcANlQcABkQ8EBkA0FB0A2FBwA2VBwAGRDwQGQDQUHQDYUHADZUHAAZEPBAZCNf52QvDp0ng4Ahx5HOACyoeAAyIaCAyAbCg6AbCg4ALKh4ADI5v8BwSQATioEge4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = ds[0]['image']\n",
    "label = ds[0]['label']\n",
    "\n",
    "figure, axs = plt.subplots()\n",
    "\n",
    "axs.imshow(ds[0]['image'], cmap='Greys')\n",
    "axs.set_title(f'label={label}, which translates to \"{ds.features[\"label\"].int2str(label)}\"');\n",
    "axs.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5f664a-7e41-41a5-bf47-6e97189034a1",
   "metadata": {},
   "source": [
    "Since we want to start simple, and only later get to Datsets and Dataloaders: let's pull out the data into a tensor so we can build simple linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced0c08f-7529-409f-9595-3ab96a701a35",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 60000, 784)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.transforms.functional as TF   # to transform from PIL to tensor\n",
    "import torch\n",
    "\n",
    "x_train = [TF.to_tensor(i).view(-1) for i in ds['image']]\n",
    "y_train = [torch.tensor(i) for i in ds['label']]\n",
    "\n",
    "len(x_train), len(y_train), len(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf3867f",
   "metadata": {},
   "source": [
    "So `x_train` and `y_train` are both lists of length 60000, and an element in `x_train` has length 784 (28x28 pixels). Now that we have the data, let's create our very first network operation: a linear layer which takes the 784 long flattened out image vector, and maps it to an output vector of length 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ccde6f-7b74-479b-b841-f8864033ab3f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def lin(x, a, b):\n",
    "    return x@a + b\n",
    "\n",
    "a = torch.randn(784, 10)\n",
    "b = torch.randn(10)\n",
    "\n",
    "out = lin(x_train[0], a, b)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae6f7f3",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "For details on matrix multiplications, check out this [post](https://lucasvw.github.io/posts/04_matmul/) I wrote earlier. \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18210f04-74f8-4e1d-9f0e-7d93c4e1e4fc",
   "metadata": {},
   "source": [
    "Let's do the same for all our training data at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf8ca7b-50e3-435d-bb92-894eeb46a573",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 10])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = torch.stack(x_train)\n",
    "out = lin(x_train, a,b)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9796dee-5fd2-4765-9fb9-e8ae0e66d7fd",
   "metadata": {},
   "source": [
    "Nice, that's basically a forward pass through our model on all our training data! \n",
    "\n",
    "Now if we want to increase the depth of our network by adding an additional layer, we need to add a non-linearity in the middle. Why? See for example the first paragraphs of this [answer](https://stats.stackexchange.com/a/335972). \n",
    "\n",
    "Let's add a ReLu nonlinearity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41af6235-55e6-4cc6-af39-e9457945a955",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return x.clamp_min(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52437d1e-4d94-4181-9512-db8a2ea75aeb",
   "metadata": {},
   "source": [
    "And let's combine these into our first \"model\", consisting of two linear layers and a relu nonlinearity in the middle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36844072-8ec7-4c06-9b1f-cc966d45027c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "n_in = 784 # number of input units (28x28)\n",
    "n_h = 50   # number of hidden units\n",
    "n_out = 10 # number of output units\n",
    "\n",
    "w1 = torch.randn(n_in, n_h)\n",
    "b1 = torch.zeros(n_h)\n",
    "w2 = torch.randn(n_h, n_out)\n",
    "b2 = torch.zeros(n_out)\n",
    "\n",
    "def model(x):\n",
    "    a1 = lin(x, w1, b1)\n",
    "    z1 = relu(a1)\n",
    "    return lin(z1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374c9742-defa-4544-a183-3d362616f1b9",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "out = model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae45976-f263-411b-baf9-13ad07d9f113",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 10])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615e4efe-fc6d-4ab9-b208-c2f6947e6a6d",
   "metadata": {},
   "source": [
    "Our \"model\" currently only does a forward pass through the network. And as a matter of fact, it's doing a forward pass with random weights. When training a neural network, we want to change these parameters in a way that the outputs of the network align with the outputs (`y_train`). I will not go into the details of this, but here is a great [video](https://youtu.be/VMj-3S1tku0) by Andrej Karpathy which in my opinion gives one of the best explanations into how this works.\n",
    "\n",
    "Before doing a backward pass, we first have to calculate the loss. Since the outputs represent any of the 10 classes the image corresponds with,  cross entropy is a straight forward loss function. Some details about cross entropy loss can be found in a [post](https://lucasvw.github.io/posts/05_crossentropy/) I wrote earlier. However, since we want to add the backpropagation ourselves and I don't know how to backpropagate through cross entropy (and I don't feel like spending a lot of time on it), let's use a much easier loss function for now: mean squared error (MSE). This obviously doesn't make any sense in the context of our data, but mathematically it's possible. We just have to end up with a single activation of our model instead of 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4499b9-baeb-4fd9-b99e-c97155ec0e4a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "n_out = 1  # number of output units changed to 1\n",
    "\n",
    "w2 = torch.randn(n_h, n_out)\n",
    "b2 = torch.zeros(n_out)\n",
    "\n",
    "def model(x):\n",
    "    a1 = lin(x, w1, b1)\n",
    "    z1 = relu(a1)\n",
    "    return lin(z1, w2, b2)\n",
    "\n",
    "out = model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58bb243-b528-4595-91c9-f1a6404112e6",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b55a9d-2006-46ae-8df6-2c939940d13d",
   "metadata": {},
   "source": [
    "From which we see that the outputs have an empty trailing dimension. `y_train` doesn't have this, so we have to squeeze out this empty dimension when computing the MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7542ce-91e4-44a5-95bd-934252611d86",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2376.7515)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mse(pred, targ): \n",
    "    return (pred.squeeze(-1)-targ).pow(2).mean() \n",
    "\n",
    "y_train = torch.stack(y_train)\n",
    "mse(out, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70062151-3121-4c1a-80b9-b88e071290ed",
   "metadata": {},
   "source": [
    "The next step will be to add the backward pass. But let's refactor our code to put things into classes, that way the backward pass can be added more easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256b9f6a-59c4-47f2-af46-dbabb2716f34",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, n_in, n_out):\n",
    "        self.w = torch.randn(n_in, n_out)\n",
    "        self.b = torch.zeros(n_out)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.inp = x                      # storing this for the backward pass\n",
    "        self.out = x@self.w + self.b      # storing this for the backward pass\n",
    "        return self.out\n",
    "    \n",
    "class Relu():\n",
    "    def __call__(self, x):\n",
    "        self.inp = x                      # storing this for the backward pass\n",
    "        self.out = x.clamp_min(0.)        # storing this for the backward pass\n",
    "        return self.out\n",
    "    \n",
    "class MSE():\n",
    "    def __call__(self, pred, targ):\n",
    "        self.pred = pred                   # storing this for the backward pass\n",
    "        self.targ = targ                   # storing this for the backward pass\n",
    "        self.out = (pred.squeeze(-1)-targ).pow(2).mean()\n",
    "        return self.out\n",
    "    \n",
    "class Model():\n",
    "    def __init__(self, n_in, n_h, n_out):\n",
    "        self.layers = [Linear(n_in, n_h), Relu(), Linear(n_h, n_out)]\n",
    "        self.loss = MSE()\n",
    "        \n",
    "    def __call__(self, x, y):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return self.loss(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96646d9f-cee9-44a5-81e5-4c37c3dd94f6",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 784])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6221762e-001e-4251-a10f-935aeb60959b",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(637.0565)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Model(n_in, n_h, n_out)\n",
    "l = m(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a441f0-fa7a-419c-ae27-4cc8e79780b4",
   "metadata": {},
   "source": [
    "To add in the functionality for the backward pass, redefining the whole class is a nuisance. So instead we'll `patch` the classes. We can do this very easily by using the `fastcore` library. Let's see a small example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60baf247-adb4-4af5-81d9-107c6fbdf2be",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello ðŸ˜Ž\n",
      "howdy ðŸ¤ \n"
     ]
    }
   ],
   "source": [
    "import fastcore.all as fc\n",
    "\n",
    "class A():\n",
    "    def hi(self): print('hello ðŸ˜Ž')\n",
    "    \n",
    "a = A()\n",
    "a.hi()\n",
    "\n",
    "@fc.patch\n",
    "def hi(self:A): print('howdy ðŸ¤ ')\n",
    "\n",
    "a.hi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f3eb88",
   "metadata": {},
   "source": [
    "So with `fc.patch` we can extend or change the behavior of Classes that have been defined elsewhere, even on instances of the objects that are already created. Nice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d160fba4-2957-4aef-a351-1b55a13bdfb1",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "@fc.patch\n",
    "def backward(self: Linear):\n",
    "    self.inp.g = self.out.g @ self.w.t()\n",
    "    self.w.g = self.inp.t() @ self.out.g\n",
    "    self.b.g = self.out.g.sum(0)\n",
    "    \n",
    "@fc.patch\n",
    "def backward(self: Relu):\n",
    "    self.inp.g = (self.inp>0).float() * self.out.g\n",
    "    \n",
    "@fc.patch\n",
    "def backward(self: MSE):\n",
    "    self.pred.g = 2. * (self.pred.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]\n",
    "    \n",
    "@fc.patch\n",
    "def backward(self: Model):\n",
    "    self.loss.backward()\n",
    "    for l in reversed(self.layers): l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95417fa-a50e-401e-b3b3-ffc20c48fb57",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2255.4106)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Model(n_in, n_h, n_out)\n",
    "l = m(x_train, y_train)\n",
    "m.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863ee419-38b3-45e3-815f-b7d2f6aea0fd",
   "metadata": {},
   "source": [
    "Now the actual operations in the backward methods you will just have to take for granted as I am not going to derive them. If you want, you can have some fun (?) to try and derive it yourself. What I think is most important about these formulas:\n",
    "\n",
    "1. Notice that each layer has a reference to it's inputs and it's outputs\n",
    "2. During the backward pass, each layer uses the gradient from the *outputs* and uses it to set the gradient on the *inputs*\n",
    "3. The inputs from layer $n$ are the outputs from layer $n-1$, so when the gradients are being set on the inputs from layer $n$, this means that layer $n-1$ it's outputs are being set at the same time\n",
    "4. This is the fundamental point about backpropagation of the gradient: in reverse order, layer by layer the gradients are being *propagated back* through the network using the chain rule\n",
    "5. Although we don't derive the operations, we can see that that there *exist* operations that do this. These operations are not magical, they are just the result of calculus: not very different from the fact that if $f(x) = x^2$ then $f'(x) = 2x$ and if $h(x) = f(g(x))$ then $h'(x) = f'(g(x)) * g'(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93c9c85-467d-4cbc-a137-f631b3b82b08",
   "metadata": {},
   "source": [
    "Now let's see how we can make this a little better. One thing that seems a bit silly is that in each of the `Linear`, `MSE` and `Relu` classes, we are storing explicitly the inputs and outputs when doing a forward call. As mentioned, we need this to backpropagate the gradients. However, we rather not store that explicitly all the time when creating a new layer. \n",
    "\n",
    "So let's create a base class that takes care of this:\n",
    "\n",
    "- Pack the forward functionality of each layer in a dedicated `forward` method\n",
    "- let the storing of inputs and ouputs be done in the `__call__` method of the baseclass, and call the `self.forward` method in between.\n",
    "\n",
    "This works, but there is one caveat: most layers just have one input when they are called (`x`), but the loss has 2 (`pred` and `targ`). To make this storing of the inputs generic we can store them as an array on the base class, and also pass them as positional arguments to `_backward`. This way, `forward` and `_backward` have the same arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b50394e-2d69-4c4e-9393-d1d5a41bfda8",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class Module():\n",
    "    def __call__(self, *args):\n",
    "        self.args = args\n",
    "        self.out = self.forward(*args)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self): self._backward(*self.args)\n",
    "\n",
    "    \n",
    "class Linear(Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        self.w = torch.randn(n_in, n_out)\n",
    "        self.b = torch.zeros(n_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x@self.w + self.b\n",
    "    \n",
    "    def _backward(self, inp):\n",
    "        inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = inp.t() @ self.out.g\n",
    "        self.b.g = self.out.g.sum(0)\n",
    "    \n",
    "    \n",
    "class Relu(Module):\n",
    "    def forward(self, x):\n",
    "        return x.clamp_min(0.)\n",
    "    \n",
    "    def _backward(self, inp):\n",
    "        inp.g = (inp>0).float() * self.out.g\n",
    "\n",
    "    \n",
    "class MSE(Module):\n",
    "    def forward(self, pred, targ):\n",
    "        return (pred.squeeze(-1)-targ).pow(2).mean()\n",
    "    \n",
    "    def _backward(self, pred, targ):\n",
    "        pred.g = 2. * (pred.squeeze() - targ).unsqueeze(-1) / targ.shape[0]\n",
    "    \n",
    "    \n",
    "class Model(Module):\n",
    "    def __init__(self, n_in, n_h, n_out):\n",
    "        self.layers = [Linear(n_in, n_h), Relu(), Linear(n_h, n_out)]\n",
    "        self.loss = MSE()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return self.loss(x, y)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers): l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04947b0b-5890-430f-8c28-df681531bc26",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1411.1520)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Model(n_in, n_h, n_out)\n",
    "l = m(x_train, y_train)\n",
    "m.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfe76a7-4ea6-4bbb-841a-f8fa9d354830",
   "metadata": {},
   "source": [
    "And this is pretty nice, since we have now kind of replicated `nn.Module` with our `Module` class. It works very similarly, so let's now instead use PyTorch's version. This has the advantage that we no longer need to write a backward function, since PyTorch knows how to backpropagate through all the functions and methods it has, it can propagate through anything we create with these functions using the chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadb6882-ac0c-46c1-8308-37ee7343d54b",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        self.w = torch.randn(n_in, n_out).requires_grad()\n",
    "        self.b = torch.zeros(n_out).requires_grad()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x@self.w + self.b\n",
    "    \n",
    "    \n",
    "class Relu(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.clamp_min(0.)\n",
    "    \n",
    "    \n",
    "class MSE(nn.Module):\n",
    "    def forward(self, pred, targ):\n",
    "        return (pred.squeeze(-1)-targ).pow(2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9c3953-f10f-4b2e-91e9-09fe8c510424",
   "metadata": {},
   "source": [
    "Furthermore we use the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be16646-4e74-4440-94de-8e260f8765da",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c365862f-ee7c-4873-8ef5-71c6fa611ba0",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0ac1c8-a84d-46a3-acd4-8b3c8941d5cf",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c6b279-7aa1-42b4-968c-e30e1082bc83",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
