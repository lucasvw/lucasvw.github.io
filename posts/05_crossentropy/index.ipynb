{
 "cells": [
  {
   "cell_type": "raw",
   "id": "99dad563-8ae4-45e1-9e2e-9b4c75bf51b9",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Cross entropy any which way\"\n",
    "author: \"Lucas van Walstijn\"\n",
    "date: \"2023-03-15\"\n",
    "categories: [loss functions, softmax, nll, cross entropy, code]\n",
    "image: \"image.png\"\n",
    "comments:\n",
    "  utterances:\n",
    "    repo: lucasvw/BlogComments\n",
    "format:\n",
    "  html:\n",
    "    code-overflow: wrap\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b36da86-35be-4fc4-a860-b67cc896a548",
   "metadata": {},
   "source": [
    "Cross entropy is one of the most commonly used loss functions. In this post, we will have a look at how it works, and compute it in a couple of different ways.\n",
    "\n",
    "Consider a network that is build for image classification. During the forward pass, images are passed into the network and the network processes the data layer by layer, until evenually some final activations are being returned by the model. These final activations are called \"logits\" and represent the unnormalized predictions of our model.\n",
    "\n",
    "Since we generally use mini-batches during training, these logits are of shape `[bs, num_classes]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4133831-48d7-4f4b-bea7-ff26515232e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269,  1.4873,  0.9007],\n",
       "        [-2.1055,  0.6784, -1.2345],\n",
       "        [-0.0431, -1.6047, -0.7521],\n",
       "        [ 1.6487, -0.3925, -1.4036]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "g = torch.manual_seed(42) # use a generator for reproducability\n",
    "\n",
    "bs = 32 # batch size of 32\n",
    "num_classes = 3 # image classification with 3 different classes\n",
    "\n",
    "logits = torch.randn(size=(bs, num_classes), generator=g) # size: [32,3]\n",
    "\n",
    "logits[0:4] # show the logits for the first couple of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebe8d21-139a-4cb6-bba4-72d876f74349",
   "metadata": {},
   "source": [
    "Each row of this tensor represents the unnormalized predictions for each of our samples in the batch. We can normalize these predictions by applying a softmax. The softmax function does two things:\n",
    "\n",
    "1. make all our logits positive, by applying the exponential function, [wolfram alpha reference](https://www.wolframalpha.com/input?i=exp%28x%29){target=\"_blank\"}\n",
    "2. divide each value of the exponentiated logits by the sum over all the classes\n",
    "\n",
    "This makes sure that we can treat the output of this as probabilities, because:\n",
    "\n",
    "1. all individual predictions will be between 0 and 1\n",
    "2. the predictions will sum to 1\n",
    "\n",
    "Specifically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c24afd-6829-4ee5-a3a0-4f25e49f6d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.9269, 1.4873, 0.9007])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unnormalized predictions for our first sample (3 classes)\n",
    "logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee237e9-3504-47e5-8362-43580d7662ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.8683, 4.4251, 2.4614])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exponentiated predictions, making them all positive\n",
    "exp_logits = logits[0].exp()\n",
    "exp_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd50daa-c979-42c3-a851-92cc3fa7d81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4993, 0.3217, 0.1789])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn these values into probabilities by dividing by the sum\n",
    "probs = exp_logits / exp_logits.sum()\n",
    "\n",
    "# verify that the sum of the probabilities sum to 1\n",
    "assert torch.allclose(probs.sum(), torch.tensor(1.))\n",
    "\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0aa3da-0dfe-4e31-98e3-108699b8b8eb",
   "metadata": {},
   "source": [
    "So, let's create a softmax function that does this for a whole batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7a74c8-f87f-45b3-a147-acbd912551e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4993, 0.3217, 0.1789],\n",
       "        [0.0511, 0.8268, 0.1221],\n",
       "        [0.5876, 0.1233, 0.2891],\n",
       "        [0.8495, 0.1103, 0.0401]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(logits):\n",
    "    exp_logits = logits.exp() # shape: [32, 3]\n",
    "    exp_logits_sum = exp_logits.sum(dim=1, keepdim=True) # shape: [32, 1]\n",
    "    \n",
    "    # Note: this get's correctly broadcasted, since the exp_logits_sum will \n",
    "    # expand to [32, 3], so each value in exp_logits gets divided by the sum over its row\n",
    "    probs = exp_logits / exp_logits_sum # shape: [32, 3]\n",
    "    \n",
    "    return probs \n",
    "\n",
    "probs = softmax(logits)\n",
    "probs[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3572ce-d7fc-42d2-8d60-8c928b5008ca",
   "metadata": {},
   "source": [
    "Next, we want to compute the loss for which also need our `labels`. These labels represent the ground truth class for each of our samples in the batch. Since we have 3 classes they will be between 0 and 3 (e.g. either 0, 1 or 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4756f11-6e21-435c-bc42-2070f8065f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 1, 1, 0, 2, 1, 2, 1, 2, 1, 1, 2, 0, 0, 1, 2, 1, 0, 1, 1, 2, 1, 2,\n",
       "        2, 1, 2, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.manual_seed(42) # use a generator for reproducability\n",
    "\n",
    "labels = torch.randint(low=0, high=3, size=(32,), generator=g)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e24f45-ccd6-4e9a-927b-5d111433d628",
   "metadata": {},
   "source": [
    "For classification we use the Negative Log Likelihood loss function, which is defined as such:\n",
    "\n",
    "$$\n",
    "\\textrm{NLL} = - \\sum_{i}{q_i * \\log(p_i)}\n",
    "$$\n",
    "\n",
    "with $i$ being the index that moves along the classes (3 in our example) and $q_i$ being the probability that the ground truth label is class $i$ (this is a somewhat strange formulation, since this probability is either 1 (for the correct class) or 0 (for all the non-correct classes)). Finally, $p_i$ is the probability that the model associated to class $i$.\n",
    "\n",
    "For the very first row of our `probs` (`[0.4993, 0.3217, 0.1789]`) and our first label (`0`) we thus get:\n",
    "\n",
    "\\begin{align}\n",
    "\\textrm{NLL} &= - ( (1 \\cdot \\log(0.4993)) + (0 \\cdot \\log(0.3217)) + (0 \\cdot \\log(0.1789)) ) \\\\\n",
    "\\textrm{NLL} &= - ( (1 \\cdot \\log(0.4993)) ) \\\\\n",
    "\\textrm{NLL} &= - \\log(0.4993)\n",
    "\\end{align}\n",
    "\n",
    "From which we see that it's just the negative log of the probability associated with the ground truth class.\n",
    "\n",
    "Since this computes only the NLL per sample, we also need a way to combine the NLL across the samples in our batch. We can do this either by summing or averaging, averaging has the advantage that the size of the loss remains the same when we change the batch-size, so let's use that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaf73cf-2480-4c82-acbc-528eba9337bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(probs, labels):\n",
    "    # probs: shape [32, 3]\n",
    "    # labels: shape [32]\n",
    "    \n",
    "    # this plucks out the probability of the ground truth label per sample, \n",
    "    # it uses \"numpy's integer array indexing\":\n",
    "    # https://numpy.org/doc/stable/user/basics.indexing.html#integer-array-indexing\n",
    "    probs_ground_truth_class = probs[range(len(labels)), labels] # shape: [32]\n",
    "    \n",
    "    nll = -torch.log(probs_ground_truth_class).mean() # shape: []\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb8fd22-c4ef-4018-b220-6aa159bd2d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3465)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll(probs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e2dfd8-9404-45b9-9a01-3049d07c695a",
   "metadata": {},
   "source": [
    "## Using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb30804d-362d-444f-96d2-df5a2d1b1e81",
   "metadata": {},
   "source": [
    "Instead of using our custom `softmax`, we can also use the build-in softmax function from PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1471b1-8f30-476b-827a-8d569401a59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3465)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = F.softmax(logits, dim=1) # dim=1 --> compute the sum across the columns\n",
    "nll(p, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c42b18c-6dc1-439b-bc24-465aa9555a2d",
   "metadata": {},
   "source": [
    "Instead of using our custom `nll` we can also use the build-in version from PyTorch. However, `nll_loss` expects the log of the softmax (for numerical stability) so instead of `softmax` we have to use `log_softmax`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0f66eb-af60-4f59-bdb9-1124a8208a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3465)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = F.log_softmax(logits, dim=1)\n",
    "\n",
    "# Assert that indeed the log_softmax is just the softmax followed by a log\n",
    "assert torch.allclose(p, F.softmax(logits, dim=1).log())\n",
    "\n",
    "torch.nn.functional.nll_loss(p, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7355bc-4a0e-4549-b895-677e5e305a2a",
   "metadata": {},
   "source": [
    "The combination of `softmax` and `nll` is called cross entropy, so we can also use PyTorch's build-in version of that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc6b9aa-f9dd-4df1-b8da-8517a030bdd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3465)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(logits, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6c6e81-fe31-4be0-bb93-82164f8218f4",
   "metadata": {},
   "source": [
    "Instead of the methods in `nn.functional`, we can also use classes. For that, we first create an instance of the object, and then \"call\" the instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d1c4be-995a-47c6-893e-2babbe593aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3465)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce = torch.nn.CrossEntropyLoss() # create a CrossEntropyLoss instance\n",
    "ce(logits, labels) # calling the instance with the arguments returns the cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa8ef20-1447-4c4f-be6d-20c6cdb56422",
   "metadata": {},
   "source": [
    "Similarly, we can use classes for the `log_softmax` and `nll_loss` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbc2e7c-0a85-4965-bfa9-e64dccc0a267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3465)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = torch.nn.LogSoftmax(dim=1)\n",
    "nll = torch.nn.NLLLoss()\n",
    "\n",
    "p = ls(logits)\n",
    "nll(p, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd626634-ed97-4a5a-84ed-bcc24186d861",
   "metadata": {},
   "source": [
    "This is practical, if we want specify custom behavior of the loss function ahead of time of calling the actual loss function. For example, let's say we want to compute the cross entropy loss based on 'sums' instead of 'averages'. Then when using the method in `F` we would do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a17b31-8a4b-4103-8987-67fa23ba1852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(43.0866)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(logits, labels, reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3954d1d-020c-4bd1-8076-09b8617435d5",
   "metadata": {},
   "source": [
    "So whenever we call the loss, we have to specify the additional `reduction` argument.\n",
    "\n",
    "Whereas when using the loss classes, we can instantiate the class with that `reduction` argument, and then call the instance as per usual without passing anything but the logits and the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19947fd9-788f-4fb6-8fad-039eab899b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(43.0866)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate \n",
    "ce = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "# at some other point in your code, compute the loss as per default\n",
    "ce(logits, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a83a67-f8df-4c29-8b72-5ab20e3b27f8",
   "metadata": {},
   "source": [
    "This is practical when the loss function is getting called by another object to which we don't have easy access. So that we can't easily change the arguments for that call. This is for example the case when using the FastAI `Learner` class, to which we pass the loss function which then get's called by the `Learner` object with the default arguments (`logits` and `labels`). By using the classes, we can specify the reduction argument ahead of time and pass that instance to the `Learner` class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
