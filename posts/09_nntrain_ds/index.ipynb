{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5a706372-b4f8-4d32-9470-24b837947fa5",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"`nntrain` (1/n): Datasets and Dataloaders\"\n",
    "author: \"Lucas van Walstijn\"\n",
    "date: \"2023-08-14\"\n",
    "categories: [dataloading, training, collation, sampler]\n",
    "image: \"image.png\"\n",
    "comments:\n",
    "  utterances:\n",
    "    repo: lucasvw/BlogComments\n",
    "format:\n",
    "  html:\n",
    "    code-overflow: scroll\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438a6911-402a-4234-9a58-7e1530bcb1bb",
   "metadata": {},
   "source": [
    "In this series, I want to discuss the creation of a small PyTorch based library for training neural networks: `nntrain`. It's based off the excellent [part 2](https://course.fast.ai/) of Practical Deep Learning for Coders by Jeremy Howard, in which from lessons 13 to 18 (roughly) the development of the `miniai` library is discussed.\n",
    "\n",
    "We'll try to build everything as much as possible from scratch to understand how things work. Once the main functionality of components is implemented and verified, we can switch over to PyTorch's version. This is similar to how things are done in the course. However, this is not just a \"copy / paste\" of the course: on many occasions I take a different route, and most of the code is my own. That is not to say that all of this is meant to be extremely innovative, instead I had the following goals:\n",
    "\n",
    "- Deeply understand the training of neural networks with a focus on PyTorch\n",
    "- Try to create an even better narrative then what's presented in FastAI üôâü§∑‚Äç‚ôÇÔ∏èüôà\n",
    "- Get hands-on experience with creating a library with [`nb_dev`](https://nbdev.fast.ai/)\n",
    "\n",
    "`nb_dev` is another great project from the fastai community, which allows python libraries to be written in jupyter notebooks. This may sound a bit weird and controversial, but it has the advantage that we can create the source code for our library in the very same environment in which we want to experiment and interact with our methods, objects and structure **while we are building the library**. For more details on why this is a good idea and other nice features of `nb_dev`, see [here](https://www.fast.ai/posts/2022-07-28-nbdev2.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc98b7c4-9002-407d-90f6-ea9d8c07b8ae",
   "metadata": {},
   "source": [
    "So without further ado, let's start with where we left off in the previous [post](https://lucasvw.github.io/posts/08_nntrain_setup/):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de55ab2-bd58-40fe-8d16-7e9507741c5e",
   "metadata": {},
   "source": [
    "## End of last post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d50bf1-5741-4404-8217-0524df11e380",
   "metadata": {},
   "source": [
    "```{.python code-line-numbers='true'}\n",
    "from datasets import load_dataset,load_dataset_builder\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "name = \"fashion_mnist\"\n",
    "ds_builder = load_dataset_builder(name)\n",
    "ds_hf = load_dataset(name, split='train')\n",
    "\n",
    "x_train = torch.stack([TF.to_tensor(i).view(-1) for i in ds_hf['image']])\n",
    "y_train = torch.stack([torch.tensor(i) for i in ds_hf['label']])\n",
    "\n",
    "def fit(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0,len(x_train), bs):\n",
    "            xb = x_train[i:i+bs]\n",
    "            yb = y_train[i:i+bs]\n",
    "\n",
    "            preds = model(xb)\n",
    "            acc = accuracy(preds, yb)\n",
    "            loss = loss_func(preds, yb)\n",
    "            loss.backward()\n",
    "\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        print(f'{epoch=} | {loss=:.3f} | {acc=:.3f}')\n",
    "\n",
    "def accuracy(preds, targs):\n",
    "    return (preds.argmax(dim=1) == targs).float().mean()        \n",
    "\n",
    "def get_model_opt():\n",
    "    layers = [nn.Linear(n_in, n_h), nn.ReLU(), nn.Linear(n_h, n_out)]\n",
    "    model = nn.Sequential(*layers)\n",
    "    \n",
    "    opt = torch.optim.SGD(model.parameters(), lr)\n",
    "    \n",
    "    return model, opt\n",
    "\n",
    "n_in  = 28*28\n",
    "n_h   = 50\n",
    "n_out = 10\n",
    "lr    = 0.01\n",
    "bs    = 1024\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "model, opt = get_model_opt()\n",
    "fit(5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36994e4-8b2a-41df-b785-aced0dc72a02",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7367832-db8b-41ce-8f9a-7dff24078a45",
   "metadata": {},
   "source": [
    "This post will be about improving the minibatch construct we currently have in the training loop on lines 16-18:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab697b3d-5ff6-4d70-bf05-bb266ad6b82c",
   "metadata": {},
   "source": [
    "```{.python code-line-numbers='true' startFrom=\"15\"}\n",
    "...\n",
    "for i in range(0,len(x_train), bs):\n",
    "    xb = x_train[i:i+bs]\n",
    "    yb = y_train[i:i+bs]\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36309999-ce35-4c7d-bd23-2711f5099b1f",
   "metadata": {},
   "source": [
    "As a first refactor, we will create a Dataset object, which allows us to simplify:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7595de2-6081-4121-baf5-8e237fac018d",
   "metadata": {},
   "source": [
    "```{.python code-line-numbers='true' startFrom=\"15\"}\n",
    "...\n",
    "for i in range(0,len(x_train), bs):\n",
    "    xb, yb = dataset[i:i+bs]\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff06715a-a1f3-4446-8712-ddf8a7286095",
   "metadata": {},
   "source": [
    "This is pretty straight-forward, a Dataset is something that holds our data and upon \"indexing into\" it returns a sample of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9473d297-9350-4d05-93d8-e50ec88e09c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    \n",
    "    def __init__(self, x_train, y_train):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.x_train[i], self.y_train[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed559b97-3076-409f-bc59-65278531dbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([784]), torch.Size([])]\n"
     ]
    }
   ],
   "source": [
    "ds = Dataset(x_train, y_train)\n",
    "print([i.shape for i in ds[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f7ee0c-c1d6-4da4-ade0-66506362d8e8",
   "metadata": {},
   "source": [
    "Next, we want to further improve the training loop and get to this behavior:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1b9b14-7115-4ae1-88ee-332425569603",
   "metadata": {},
   "source": [
    "```{.python code-line-numbers='true' startFrom=\"15\"}\n",
    "...\n",
    "for xb, yb in dataloader:\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142d179a-c3df-44d4-9152-30a4ee8c2229",
   "metadata": {},
   "source": [
    "So our dataloader needs to wrap the dataset, and provide some kind of an iterator returning batches of data, based on the specified batch size. Let's create one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55fc527-eb1c-4db3-bfb0-d96d123d2cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    \n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for i in range(len(self.dataset), self.batch_size):\n",
    "            yield self.dataset[i:i+self.batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bde37b-d6fb-42d8-8d88-42fb8af0e0fe",
   "metadata": {},
   "source": [
    "Now the training loop is simplified to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcb1da9-393c-46eb-83d1-a0bc794be0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for xb, yb in dl:\n",
    "            preds = model(xb)\n",
    "            acc = accuracy(preds, yb)\n",
    "            loss = loss_func(preds, yb)\n",
    "            loss.backward()\n",
    "\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        print(f'{epoch=} | {loss=:.3f} | {acc=:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aa86f7-7415-48d1-886f-68c1d1155ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 | loss=2.062 | acc=0.441\n",
      "epoch=1 | loss=1.785 | acc=0.597\n",
      "epoch=2 | loss=1.531 | acc=0.637\n",
      "epoch=3 | loss=1.334 | acc=0.645\n",
      "epoch=4 | loss=1.190 | acc=0.660\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds, bs)\n",
    "\n",
    "model, opt = get_model_opt()\n",
    "fit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a88be1a-d2e3-4984-a1bb-210139c30f5c",
   "metadata": {},
   "source": [
    "## Next up: shuffling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd88daf-5260-4c10-b40b-04d455197560",
   "metadata": {},
   "source": [
    "The next change will improve the training of the model. So far, we cycle each epoch through the data in the exact same order. This means that all training samples are always batched together with the exact same other samples. This is not good for training our model, instead we want to shuffle the data each epoch. So that each epoch, we have batches of data that have not yet been batched together. This additional variation helps the model to generalize as we will see.\n",
    "\n",
    "The simplest implementation would be to create a list of indices, which we put in between the dataset and the sampling of the mini-batches. This list will function as a map. In case we don't need to shuffle, this list will simply be `[0, 1, ... len(dataset)]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2076aeb-868a-4c91-9fc0-aa2a9128f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class DataLoader():\n",
    "    \n",
    "    def __init__(self, dataset, batch_size, shuffle):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.indices = list(range(len(self.dataset)))\n",
    "        if self.shuffle: \n",
    "            random.shuffle(self.indices)\n",
    "            \n",
    "        for i in range(0,len(self.dataset),self.batch_size):\n",
    "            yield self.dataset[self.indices[i:i+self.batch_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfe4320-5f5c-407d-be08-90fb5be3302d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 | loss=2.067 | acc=0.429\n",
      "epoch=1 | loss=1.800 | acc=0.515\n",
      "epoch=2 | loss=1.539 | acc=0.592\n",
      "epoch=3 | loss=1.358 | acc=0.618\n",
      "epoch=4 | loss=1.187 | acc=0.692\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model_opt()\n",
    "dl = DataLoader(ds, bs, shuffle=True)\n",
    "fit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e94cfd-24a2-4fc1-b0d2-818f2ebf08dd",
   "metadata": {},
   "source": [
    "This works just fine, but let's see if we can encapsulate this logic in a separate class. We start with a simple `Sampler` class that we can iterate through and either gives indices in order, or shuffled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebc5826-5927-47e1-9f74-8f5916fab1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler():\n",
    "    def __init__(self, ds, shuffle=False):\n",
    "        self.range = list(range(0, len(ds)))\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        if self.shuffle: random.shuffle(self.range)\n",
    "        for i in self.range:\n",
    "            yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb1fda-fdb9-4efc-a310-9b5cda02f617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 1, 2, 3, 4, 5, "
     ]
    }
   ],
   "source": [
    "s = Sampler(ds, False)           # shuffle = False\n",
    "for i, sample in enumerate(s): \n",
    "    print(sample, end=', ')\n",
    "    if i == 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2d0543-2d36-4816-8d55-78f20c39445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58844, 19394, 36509, 38262, 51037, 46835, "
     ]
    }
   ],
   "source": [
    "s = Sampler(ds, True)            # shuffle = TRUE\n",
    "for i, sample in enumerate(s): \n",
    "    print(sample, end=', ')\n",
    "    if i == 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136eb35a-633c-4929-b72f-51a18329ac4a",
   "metadata": {},
   "source": [
    "Next, let's create a BatchSampler that does the same, but returns the indexes in batches. For that we can use the `islice()` function from the `itertools` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbd3ef4-32ba-4357-82a4-fcee195f0c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n",
      "[5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def printlist(this): print(list(this))\n",
    "\n",
    "lst = list(range(0, 10))         # create a list of 10 numbers\n",
    "\n",
    "printlist(islice(lst, 0, 3))     # with islice we can get a slice out of the list\n",
    "printlist(islice(lst, 5, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc0e2d3-77e0-412c-9aa1-346a92363332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n",
      "[0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "printlist(islice(lst, 4))        # we can also get the \"next\" 4 elements\n",
    "printlist(islice(lst, 4))        # doing that twice gives the same first 4 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47374626-6268-473d-b039-c786f8b46e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n",
      "[4, 5, 6, 7]\n",
      "[8, 9]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "lst = iter(lst)                  # however if we put an iterator on the list:\n",
    "\n",
    "printlist(islice(lst, 4))        # first 4 elements\n",
    "printlist(islice(lst, 4))        # second 4 elements\n",
    "printlist(islice(lst, 4))        # remaining 2 elements\n",
    "printlist(islice(lst, 4))        # iterator has finished.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d61d2e-3df7-4b5b-ac17-5643176ca7b6",
   "metadata": {},
   "source": [
    "And thus we create our `BatchSampler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27912ceb-2693-444f-9eef-553e1edf4f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSampler():\n",
    "    def __init__(self, sampler, batch_size):\n",
    "        self.sampler = sampler\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __iter__(self):\n",
    "        it = iter(self.sampler)\n",
    "        while True:\n",
    "            res = list(islice(it, self.batch_size))\n",
    "            if len(res) == 0:    # return when the iterator has finished          \n",
    "                return           \n",
    "            yield res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d070a0",
   "metadata": {},
   "source": [
    "Let's see the BatchSamepler in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014976ed-908d-4413-9688-2e193f41780b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n",
      "[4, 5, 6, 7]\n",
      "[8, 9]\n"
     ]
    }
   ],
   "source": [
    "s = Sampler(list(range(0,10)), shuffle=False)\n",
    "batchs = BatchSampler(s, 4)\n",
    "for i in batchs:\n",
    "    printlist(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff27a07d-3e97-4e09-97ca-67ff668a647c",
   "metadata": {},
   "source": [
    "And let's incorporate it into the DataLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40033fb3-35c4-4fda-bb68-699b58948b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    \n",
    "    def __init__(self, dataset, batch_sampler):\n",
    "        self.dataset = dataset\n",
    "        self.batch_sampler = batch_sampler\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for batch in self.batch_sampler:\n",
    "            yield self.dataset[batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8b2cd6-8e60-4abe-aa66-10cfd94375b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 | loss=1.981 | acc=0.462\n",
      "epoch=1 | loss=1.698 | acc=0.567\n",
      "epoch=2 | loss=1.468 | acc=0.620\n",
      "epoch=3 | loss=1.346 | acc=0.613\n",
      "epoch=4 | loss=1.202 | acc=0.656\n"
     ]
    }
   ],
   "source": [
    "s = Sampler(ds, shuffle=True)\n",
    "dl = DataLoader(ds, BatchSampler(s, bs))\n",
    "\n",
    "model, opt = get_model_opt()\n",
    "fit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199be525",
   "metadata": {},
   "source": [
    "## Collation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707a3803-b07a-429b-9134-39b9947e62b5",
   "metadata": {},
   "source": [
    "With the Sampler, the Dataloader and the Dataset we have made some good progress. However, there is one other thing we need to fix. In the very beginning of this post we did:\n",
    "\n",
    "```{.python code-line-numbers='true' startFrom=\"11\"}\n",
    "x_train = torch.stack([TF.to_tensor(i).view(-1) for i in ds_hf['image']])\n",
    "y_train = torch.stack([torch.tensor(i) for i in ds_hf['label']])\n",
    "```\n",
    "\n",
    "And we ideally would like these transformations to be part of the Dataloaders / Dataset paradigm. So instead of first transforming the Huggingface Dataset into `x_train` and `y_train`, we want to directly use the dataset. We can do so by adding a **collate function**. This wraps around a list of individual samples into the datasets, and receives a list of individual x,y tuples (`[(x1,y1), (x2,y2), ..]`) as argument. In that function, we can determine how to treat these items and parse it in a way that is suitable to our needs. i.e.:\n",
    "\n",
    "- batch the `x` and `y`, so that we transform from `[(x1,y1), (x2,y2), ..]`  to `[(x1,x2, ..), (y1,y2, ..)]`\n",
    "- move individual items `x_i` and `y_i` to tensors\n",
    "- stack the `x` tensors and `y` tensors respectively into one big tensor\n",
    "\n",
    "So let's update our DataLoader with a `collate_func` that wraps around individual samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e8af86-5c24-4db8-ba5e-48384c25cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    \n",
    "    def __init__(self, dataset, batch_sampler, collate_func):\n",
    "        self.dataset = dataset\n",
    "        self.batch_sampler = batch_sampler\n",
    "        self.collate_func = collate_func\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for batch in self.batch_sampler:\n",
    "            yield self.collate_func(self.dataset[sample] for sample in batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91359b28-5dc4-45cb-bf1d-c8daffb8ba7c",
   "metadata": {},
   "source": [
    "And now let's create a custom collate function to deal with our data. Specifically, remember that a sample of our huggingface dataset is a dictionary (and not a tuple) with keys `image` and `label` holding a `PIL.Image.image` object and a number (representing any out of 10 classes) respectively.\n",
    "\n",
    "So our `collate_func` should:\n",
    "\n",
    "(1) transform the dictionary into a tuple\n",
    "(2) move everything to a tensor\n",
    "(3) zip the results so that `x` and `y` are batched\n",
    "(4) and combine the list of tensors for `x` and `y` respectively into one big tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabaedb9-77d8-4254-8d28-2c37483b38e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_func(data):\n",
    "    data = [(TF.to_tensor(sample['image']).view(-1), torch.tensor(sample['label'])) for sample in data]\n",
    "    x, y = zip(*data)\n",
    "    return torch.stack(x), torch.stack(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fafc79",
   "metadata": {},
   "source": [
    "And let's see it in action, now using the huggingface dataset `ds_hf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86776f18-2e11-499f-8903-5ac15fdbc050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 | loss=2.125 | acc=0.345\n",
      "epoch=1 | loss=1.899 | acc=0.497\n",
      "epoch=2 | loss=1.635 | acc=0.609\n",
      "epoch=3 | loss=1.389 | acc=0.640\n",
      "epoch=4 | loss=1.260 | acc=0.641\n"
     ]
    }
   ],
   "source": [
    "s = Sampler(ds_hf, shuffle=True)\n",
    "dl = DataLoader(ds_hf, BatchSampler(s, bs), collate_func)\n",
    "\n",
    "model, opt = get_model_opt()\n",
    "fit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad0132c-bbd5-4d44-80c3-ece023e5196c",
   "metadata": {},
   "source": [
    "Not bad, we have replicated the main logic of PyTorch's DataLoader. The version from PyTorch has a slightly different API as we don't have to specify the `BatchSampler`, instead we can just pass `shuffle=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74ba588-bb65-4969-827e-0e813d36a2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 | loss=2.107 | acc=0.434\n",
      "epoch=1 | loss=1.840 | acc=0.620\n",
      "epoch=2 | loss=1.605 | acc=0.641\n",
      "epoch=3 | loss=1.354 | acc=0.641\n",
      "epoch=4 | loss=1.258 | acc=0.618\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "s = Sampler(ds_hf, shuffle=True)\n",
    "dl = DataLoader(ds_hf, batch_size=bs, shuffle=True, collate_fn=collate_func)\n",
    "\n",
    "model, opt = get_model_opt()\n",
    "fit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da52d55-2c52-4aa2-bd94-be854b29b42e",
   "metadata": {},
   "source": [
    "## Validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2703d7-58b5-4ae8-b18c-e06b97c33a14",
   "metadata": {},
   "source": [
    "Let's add a validation set to make sure we validate on data we are not training on. For that we are going to pull the data from the datasets library without the `splits` argument, which will give us a dataset dictionary containing both a training and a test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a657046-2be7-4582-86c3-326714e26576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset fashion_mnist (/root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17183763453740089b95f79c7cbabd59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 60000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dd = load_dataset(name)\n",
    "hf_dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9381f25-8248-4fc0-9ca1-d109d625e442",
   "metadata": {},
   "source": [
    "And let's create two dataloaders, one for the train and one for the validation set. For the validation loader we can double the batch size since we won't be computing gradients for the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0570515a-278c-4b3e-9459-d895cec6f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(hf_dd['train'], batch_size=bs, shuffle=True, collate_fn=collate_func)\n",
    "valid_loader = DataLoader(hf_dd['test'], batch_size=2*bs, shuffle=False, collate_fn=collate_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf65c44-5ed7-46d2-ba7d-f168c2b202c4",
   "metadata": {},
   "source": [
    "We change the training loop in a couple of ways:\n",
    "\n",
    "- compute loss and metrics more correctly, by taking care of the batch-size and taking the average over all data\n",
    "- add a seperate forward pass for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d188a9-6bfc-4c2d-a00b-7a5ebbe3089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()                                       # put the model in \"train\" mode\n",
    "        n_t = train_loss_s = 0                              # initialize variables for computing averages\n",
    "        for xb, yb in train_loader:\n",
    "            preds = model(xb)\n",
    "            train_loss = loss_func(preds, yb)\n",
    "            train_loss.backward()\n",
    "            \n",
    "            n_t += len(xb)\n",
    "            train_loss_s += train_loss.item() * len(xb)\n",
    "            \n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        model.eval()                                        # put the model in \"eval\" mode\n",
    "        n_v = valid_loss_s = acc_s = 0                      # initialize variables for computing averages\n",
    "        for xb, yb in valid_loader:\n",
    "            with torch.no_grad():                           # no need to compute gradients on validation set\n",
    "                preds = model(xb)\n",
    "                valid_loss = loss_func(preds, yb)\n",
    "                \n",
    "                n_v += len(xb)\n",
    "                valid_loss_s += valid_loss.item() * len(xb)\n",
    "                acc_s += accuracy(preds, yb) * len(xb)\n",
    "        \n",
    "        train_loss = train_loss_s / n_t                     # compute averages of loss and metrics\n",
    "        valid_loss = valid_loss_s / n_v\n",
    "        acc = acc_s / n_v\n",
    "        print(f'{epoch=} | {train_loss=:.3f} | {valid_loss=:.3f} | {acc=:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c979153-56ee-451e-a53f-d2ffdc468645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 | train_loss=2.198 | valid_loss=2.095 | acc=0.276\n",
      "epoch=1 | train_loss=1.980 | valid_loss=1.852 | acc=0.539\n",
      "epoch=2 | train_loss=1.718 | valid_loss=1.591 | acc=0.617\n",
      "epoch=3 | train_loss=1.481 | valid_loss=1.387 | acc=0.624\n",
      "epoch=4 | train_loss=1.305 | valid_loss=1.241 | acc=0.637\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model_opt()\n",
    "\n",
    "fit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaa467e-1e91-4cfa-8cc4-dae44647af65",
   "metadata": {},
   "source": [
    "And that's it for this post (almost)! We have seen a lot of details on Datasets, Dataloaders and the transformation of data. We have used these concepts to improve our training loop: shuffling the training data on each epoch, and the computation of the metrics on the validation set. But before we close off, let's make our very first exports into the library, so that next time we can continue where we finished off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd6117-f9c9-40fd-902e-3bd4201b35f7",
   "metadata": {},
   "source": [
    "## First exports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18cfe11-c26d-4d2d-99de-0dc464c58895",
   "metadata": {},
   "source": [
    "When exporting code to a module with `nbdev` the first thing we need to do is declare the `default_exp` directive. This makes sure that when we run the export, the module will be exported to `dataloaders.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1cf4a2-c9ca-452a-b378-69441144ee85",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| default_exp dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4b19e3-fa3b-4549-ba3a-c159ff08e5d0",
   "metadata": {},
   "source": [
    "Next, we can export any code into the module by adding `#|export` on top of the cell we want to export. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995453be-ca15-4f1a-8452-910132e5c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "\n",
    "def print_hello():\n",
    "    print('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febf376b-09b9-48f8-8c72-dae27ef62978",
   "metadata": {},
   "source": [
    "To export, we simply execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4214d9-52e4-41d3-94bb-f96fffc09bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2c85ff-13d0-49b0-ab9a-94bcf3532ebd",
   "metadata": {},
   "source": [
    "This will create a file called `dataloaders.py` in the library folder (in my case `nntrain`) with the contents:\n",
    "\n",
    "```{.python}\n",
    "# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_dataloaders.ipynb.\n",
    "\n",
    "# %% auto 0\n",
    "__all__ = ['func']\n",
    "\n",
    "# %% ../nbs/01_dataloaders.ipynb 59\n",
    "def print_hello():\n",
    "    print('hello')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2c2ff0-5c48-4db1-9b95-e620845ef6af",
   "metadata": {},
   "source": [
    "So what do we want to export here? Let's see if we can create some generic code for loading data from the Huggingface datasets library into a PyTorch Dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adb3934-ae03-437a-93bb-89e33d4c2784",
   "metadata": {},
   "outputs": [],
   "source": [
    " #|export\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483cb732-4797-4119-ad97-9ec0812aa843",
   "metadata": {},
   "outputs": [],
   "source": [
    " #|export\n",
    "\n",
    "def hf_ds_collate_fn(data, flatten=True):\n",
    "    '''\n",
    "    Collation function for building a PyTorch DataLoader from a a huggingface dataset.\n",
    "    Tries to put all items from an entry into the dataset to tensor.\n",
    "    PIL images are converted to tensor, either flattened or not \n",
    "    '''\n",
    "\n",
    "    def to_tensor(i, flatten):\n",
    "        if isinstance(i, PIL.Image.Image):\n",
    "            if flatten:\n",
    "                return torch.flatten(TF.to_tensor(i))\n",
    "            return TF.to_tensor(i)\n",
    "        else:\n",
    "            return torch.tensor(i)\n",
    "    \n",
    "    to_tensor = partial(to_tensor, flatten=flatten)      # partially apply to_tensor() with flatten arg\n",
    "    data = [map(to_tensor, el.values()) for el in data]  # map each item from a dataset entry through to_tensor()\n",
    "    data = zip(*data)                                    # zip data of any length not just (x,y) but also (x,y,z)\n",
    "    return (torch.stack(i) for i in data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bbb8c8-7402-4c71-804d-ef4312336588",
   "metadata": {},
   "outputs": [],
   "source": [
    " #|export\n",
    "class DataLoaders:\n",
    "    def __init__(self, train, valid):\n",
    "        '''Class that exposes two PyTorch dataloaders as train and valid arguments'''\n",
    "        self.train = train\n",
    "        self.valid = valid\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_dls(cls, train_ds, valid_ds, bs, collate_fn, **kwargs):\n",
    "        '''Helper function returning 2 PyTorch Dataloaders as a tuple for 2 Datasets. **kwargs are passed to the DataLoader'''\n",
    "        return (DataLoader(train_ds, batch_size=bs, shuffle=True, collate_fn=collate_fn, **kwargs),\n",
    "                DataLoader(valid_ds, batch_size=bs*2, collate_fn=collate_fn, **kwargs))\n",
    "        \n",
    "    @classmethod\n",
    "    def from_hf_dd(cls, dd, batch_size, collate_fn=hf_ds_collate_fn, **kwargs):\n",
    "        '''Factory method to create a Dataloaders object for a Huggingface Dataset dict,\n",
    "        uses the `hf_ds_collate_func` collation function by default, **kwargs are passes to the DataLoaders'''\n",
    "        return cls(*cls._get_dls(*dd.values(), batch_size, collate_fn, **kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c727dfb-dfe2-4dc6-8f0f-90a413068729",
   "metadata": {},
   "source": [
    "With `show_doc()` we can include the documentations of class methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55426fec-9e43-4657-81c8-7c3c10477008",
   "metadata": {},
   "outputs": [],
   "source": [
    " #|hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8299bec3-67bb-46c9-9562-8a94a8761b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DataLoaders.from_hf_dd\n",
       "\n",
       ">      DataLoaders.from_hf_dd (dd, batch_size, collate_fn=<function\n",
       ">                              hf_ds_collate_fn>, **kwargs)\n",
       "\n",
       "Factory method to create a Dataloaders object for a Huggingface Dataset dict,\n",
       "uses the `hf_ds_collate_func` collation function by default, **kwargs are passes to the DataLoaders"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DataLoaders.from_hf_dd\n",
       "\n",
       ">      DataLoaders.from_hf_dd (dd, batch_size, collate_fn=<function\n",
       ">                              hf_ds_collate_fn>, **kwargs)\n",
       "\n",
       "Factory method to create a Dataloaders object for a Huggingface Dataset dict,\n",
       "uses the `hf_ds_collate_func` collation function by default, **kwargs are passes to the DataLoaders"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DataLoaders.from_hf_dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6704ce35",
   "metadata": {},
   "source": [
    "Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea51006a-0f60-4b78-af1c-0421483dbdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()                                       \n",
    "        n_t = train_loss_s = 0                              \n",
    "        for xb, yb in dls.train:\n",
    "            preds = model(xb)\n",
    "            train_loss = loss_func(preds, yb)\n",
    "            train_loss.backward()\n",
    "            \n",
    "            n_t += len(xb)\n",
    "            train_loss_s += train_loss.item() * len(xb)\n",
    "            \n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        model.eval()                                        \n",
    "        n_v = valid_loss_s = acc_s = 0                      \n",
    "        for xb, yb in dls.valid: \n",
    "            with torch.no_grad():                           \n",
    "                preds = model(xb)\n",
    "                valid_loss = loss_func(preds, yb)\n",
    "                \n",
    "                n_v += len(xb)\n",
    "                valid_loss_s += valid_loss.item() * len(xb)\n",
    "                acc_s += accuracy(preds, yb) * len(xb)\n",
    "        \n",
    "        train_loss = train_loss_s / n_t                     \n",
    "        valid_loss = valid_loss_s / n_v\n",
    "        acc = acc_s / n_v\n",
    "        print(f'{epoch=} | {train_loss=:.3f} | {valid_loss=:.3f} | {acc=:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c97699-a441-41ab-a146-8616ca2710ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset fashion_mnist (/root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "812c4c34962d4e1aba8baa1b1f2204d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_dd = load_dataset('fashion_mnist')\n",
    "bs    = 1024\n",
    "dls = DataLoaders.from_hf_dd(hf_dd, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2128fd28-f5a0-482c-9b61-f9a043535b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 | loss=2.094 | acc=0.431\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model_opt()\n",
    "\n",
    "fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e47c4-2441-4081-84bb-14e680565c24",
   "metadata": {},
   "outputs": [],
   "source": [
    " #|hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e294b5-e777-4005-b71f-1458a2da81c4",
   "metadata": {},
   "source": [
    "And that's it. We have created our first module of the `nntrain` libraryüï∫. Links:\n",
    "\n",
    "- [Dataloaders Notebook](https://github.com/lucasvw/nntrain/blob/main/nbs/01_dataloaders.ipynb): the \"source\" of the source code\n",
    "- [Dataloaders module](https://github.com/lucasvw/nntrain/blob/main/nntrain/dataloaders.py): the `.py` source code exported from the notebook\n",
    "- [Documentation](https://lucasvw.github.io/nntrain/dataloaders.html): automatically created from the notebook and hosted on Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f3eae6-65b8-4f29-857c-73f188692cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
