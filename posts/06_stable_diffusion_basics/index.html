<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Lucas van Walstijn">
<meta name="dcterms.date" content="2023-03-18">

<title>Lucas van Walstijn - Introduction to Stable Diffusion - Concepts</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../profile.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Lucas van Walstijn</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/lucasvw" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/lvWal" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/lucasvanwalstijn/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://lucasvw.github.io/index.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Introduction to Stable Diffusion - Concepts</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Generative</div>
                <div class="quarto-category">Stable Diffusion</div>
                <div class="quarto-category">Diffusers</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Lucas van Walstijn </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 18, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>Stable Diffusion, a generative deep learning algorithm developed in 2022, is capable of creating images from prompts. For example, when presented the prompt: <strong>A group of people having lunch on the moon</strong>, the algorithm creates the following image:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="image.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>And although this image isn’t perfect and can be improved by more detailed prompting, it’s pretty amazing that it took less then 30 seconds to create this image, and the depiction is a fairly accurate representation of the prompt: The algorithm “imagined” that people on the moon should be wearing space suits, a good idea indeed! All have gathered around a table an are sitting on some kind of chair, which is generally how people eat their lunch. Last but not least, the background looks pretty moonish. Not bad at all!</p>
<p>Now let’s look at the main components involved in creating this image, this follows largely the steps of <a href="https://course.fast.ai/Lessons/lesson9.html">Lesson 9 of Deep Learning for Coders</a></p>
<section id="intuition" class="level2">
<h2 class="anchored" data-anchor-id="intuition">Intuition</h2>
<p>Consider some kind of black box system that takes an input, does “something” and then gives an output. Let’s say it takes an image of a handwritten digit as input, and outputs the probability that the image is indeed a hand written digit. Visually something like this:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="intuition-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>In statistics, we would call this a probability density function. It’s a function that takes data as input, and gives the probability that</p>
<ul>
<li>if the data is coming indeed from the distribution,</li>
<li>then what’s the probability that we see this data</li>
</ul>
<p>Applied to our use-case: if the image is indeed from this distribution of images that represent images of hand written digits, what’s the probability that we observe the presented image?</p>
<p>As seen from the figure above: with such a system, we could start with an image of pure noise and iteratively do:</p>
<ol type="1">
<li>get the probability <span class="math inline">\(p_0\)</span> of the image being a handwritten digit from the black box system</li>
<li>change the value of one of the pixels at random</li>
<li>get the new probability <span class="math inline">\(p_1\)</span> whether the image is a handwritten digit from the black box system</li>
<li>when <span class="math inline">\(p_1 &gt; p_0\)</span> update the image with the changed pixel value</li>
</ol>
<p>If we follow this procedure long enough, we would gradually update all the pixel values and the image will start to resemble a handwritten digit.</p>
<p>In principle, this is the simple intuition behind stable diffusion.</p>
</section>
<section id="the-main-component-unet" class="level2">
<h2 class="anchored" data-anchor-id="the-main-component-unet">The main component: Unet</h2>
<p>So how are we going to create this magic system which will give the probability that an image is depicting a handwritten digit? Let’s start with the training data, we need lots of images that depict handwritten digits. Something like this:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mnist.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Since these images represent actual hand-written digits, the system will need to output a near 1.0 probability for these images. But how do we get images that “somewhat” represent handwritten digits and will result in lower probability values? We somehow have to “crappify” these existing images. We can do by using the images we already found and sprinkle them with different amounts of noise. The more noise we add, the less the image will resemble a handwritten digit. Visually:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="intuition-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Now we can train a network which we feed the <em>noisified images</em> as input and use the <em>noise image</em> as label. So instead of predicting the probability that an image depicts a handwritten digit, the model will predict the noise. By using a simple MSE loss on the actual noise (labels) and the predictions the model will learn how to predict the noise from looking at a noisified images.</p>
<p>The idea behind this model is that once this model is trained, we could run inference on some random noise. The model will give us a prediction of all the noise in the image, which when removed from the input, renders an image of a digit.</p>
<p>It turns out that this process works much better if, instead of removing all the noise that was predicted by the model at once, we just remove a little bit of the noise that was predicted. This way, we end up with an image which is just a bit less noisy then what we started with. We then feed this less noisy image again into our network, and thus iteratively remove more and more noise from the image, until after a certain amount of steps (50 for example) we end-up with an image that is free of noise.</p>
<p>One model architecture that is takes images as input and also outputs images is called a Unet and forms the first component of our Stable Diffusion system:</p>
<table class="table">
<colgroup>
<col style="width: 10%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th style="text-align: left;">Inputs</th>
<th style="text-align: left;">Outputs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Unet</td>
<td style="text-align: left;">Noisy images</td>
<td style="text-align: left;">Noise</td>
</tr>
</tbody>
</table>
</section>
<section id="compression-variational-autoencoder" class="level2">
<h2 class="anchored" data-anchor-id="compression-variational-autoencoder">Compression: Variational Autoencoder</h2>
<p>When working with images in neural networks we often reduce the resolution of images or use smaller patches of the original image to make sure everything fits on the GPU. With stable diffusion, we naturally want to output images of high resolution, so we need either very large GPUs, or we can use a compression trick by making use of a Variational Autoencoder (VAE).</p>
<p>A VAE is a network architecture having an encoder and a decoder. In the encoder the image input is being transformed through a series of convolutional layers into a compressed representation, the latent. In the decoder this compressed latent is passed through a series of layers that are trying to reconstruct the original image. Visually:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="intuition-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>This might look like a boring network architecture at first. But it’s actually a very neat way to compress things: We can feed this model all the different noisified images mentioned earlier, and use an MSE loss on the inputs and outputs. This will train the model to create compressed representations of our images (the latents) that can be used by the decoder to recreate the original image. This means that the latent representation carries close to the same amount of “information” as our full-size images.</p>
<p>With this, we can now train the previously discussed Unet on all the latents instead of the full size images!</p>
<p>During inference the combined architecture looks like this: we run any input first through the encoder returning a highly compressed version of our input (i.e.&nbsp;the latents). We then run it through the Unet, which will output a latent representation of the noise. If we (partly) subtract the noise latent from the noisy image latent, we end up with a latent representation of our image which is a bit less noisy then what we started with. Finally, to move from latent representation to full-size images, we can use the decoder of the VAE. Visually:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="intuition-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>To summarize:</p>
<table class="table">
<colgroup>
<col style="width: 10%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th style="text-align: left;">Inputs</th>
<th style="text-align: left;">Outputs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>VAE encoder</td>
<td style="text-align: left;">Noisy image</td>
<td style="text-align: left;">Noisy image latents</td>
</tr>
<tr class="even">
<td>Unet</td>
<td style="text-align: left;">Noisy image latents</td>
<td style="text-align: left;">Noise latents</td>
</tr>
<tr class="odd">
<td>VAE decoder</td>
<td style="text-align: left;">Noise latents</td>
<td style="text-align: left;">Noise</td>
</tr>
</tbody>
</table>
</section>
<section id="prompting-clip" class="level2">
<h2 class="anchored" data-anchor-id="prompting-clip">Prompting: CLIP</h2>
<p>So how can we create prompting? Let’s start simple and imagine we just want to specify which handwritten digit we would like to generate, so any number between 0 and 9. We could do this by training the Unet not only on the noisy image (input) and noise (output), but instead also give it a representation of the digit we sprinkled the noise on as input. The most generic way to do this, would be to create a one-hot encoded representation of the digit, visually:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="intuition-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>To create an image depicting the digit “three” from pure noise, we would then start with a random noise latent and feed it together with the one-hot encoded representation of the digit into the Unet. This way, the Unet is “guided” to create an image of digit “three” and not just any image, visually:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="intuition-5-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>To continue, how are we going to scale this for <em>any</em> text prompt besides our 10 digits? We can’t possibly create a one-hot encoding of any possible prompt, that would make our vector infinitely large. Instead, we want to compress the encoding in some finite, high dimensional space, e.g.&nbsp;we want to create an embedding encoding of our prompt.</p>
<p>To create these embeddings, we first of all need again lots of data. For example by capturing a lot of images from the internet, these image generally have a textual description in the HTML tag.</p>
<p>We can feed the text and images into two separate encoders. These encoders take the text and image respectively and output a vector. Next, we can align the vector representations in a matrix and take the dot-product between them. We want the text and image vectors of the same “object” to align, this means their dot-product should be large. Also, we want the vectors of different objects to not align, so their dot-product should be small. Visually:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="intuition-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>A loss function that does exactly this, is called the Contrastive Loss. And the model described here is called Contrastive Language Image Pre-training (CLIP).</p>
<p>During inference, we can use the trained text-encoder and apply it to the prompt. The outputted embedding can then be used as the encoding we feed into our Unet in combination with the noisy image latent.</p>
<p>To summarize:</p>
<table class="table">
<colgroup>
<col style="width: 10%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th style="text-align: left;">Inputs</th>
<th style="text-align: left;">Outputs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>CLIP text encoder</td>
<td style="text-align: left;">Prompt</td>
<td style="text-align: left;">Embedding</td>
</tr>
<tr class="even">
<td>VAE encoder</td>
<td style="text-align: left;">Noisy image</td>
<td style="text-align: left;">Noisy image latents</td>
</tr>
<tr class="odd">
<td>Unet</td>
<td style="text-align: left;">Noisy image latents + Prompt embedding</td>
<td style="text-align: left;">Noise latents</td>
</tr>
<tr class="even">
<td>VAE decoder</td>
<td style="text-align: left;">Noise latents</td>
<td style="text-align: left;">Noise</td>
</tr>
</tbody>
</table>
</section>
<section id="noise-scheduler" class="level2">
<h2 class="anchored" data-anchor-id="noise-scheduler">Noise scheduler</h2>
<p>Above it was stated, that “different” amounts of noise are sprinkled on our images during training, and during inference “some” amount of noise is being subtracted from the image. In the next post, which will be a “code” version of this post, we will see more how this exactly works, but let’s introduce one more concept here:</p>
<p>To formalize the amounts of noise we will use something called a noise schedule, which maps an integer value (called the timestep <span class="math inline">\(t\)</span>) to an amount of noise we will add to our image. This noise schedule is a monotonically decreasing function of <span class="math inline">\(t\)</span>, so large values of <span class="math inline">\(t\)</span> will add a small amount of noise and small values of <span class="math inline">\(t\)</span> add a large amount of noise. A typical noise schedule looks something like this:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="noise-schedule.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>With this noise schedule, we can pick different amounts of noise during training and add it to the images in the batch. Additionally, we will feed the noise parameter to the Unet, so that it knows how much noise was added to the image. This sould make it easier for the model to reconstruct the noise.</p>
<p>To summarize:</p>
<table class="table">
<colgroup>
<col style="width: 10%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th style="text-align: left;">Inputs</th>
<th style="text-align: left;">Outputs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>CLIP text encoder</td>
<td style="text-align: left;">Prompt</td>
<td style="text-align: left;">Embedding</td>
</tr>
<tr class="even">
<td>VAE encoder</td>
<td style="text-align: left;">Noisy image</td>
<td style="text-align: left;">Noisy image latents</td>
</tr>
<tr class="odd">
<td>Unet</td>
<td style="text-align: left;">Noisy image latents + Prompt embedding + Noise level</td>
<td style="text-align: left;">Noise latents</td>
</tr>
<tr class="even">
<td>VAE decoder</td>
<td style="text-align: left;">Noise latents</td>
<td style="text-align: left;">Noise</td>
</tr>
</tbody>
</table>
<p>That’s it for now! If you came this far, I hope you enjoyed it. For me, it helped a lot in my understanding by writing all this down. In the next blog post, we will have a look at how these concepts translate into code by making use of HuggingFace libraries such as <code>diffusers</code> and <code>transformers</code></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="lucasvw/BlogComments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>