<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Lucas van Walstijn">
<meta name="dcterms.date" content="2023-08-28">

<title>Lucas van Walstijn - nntrain (4/n): Accelerated optimization</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../profile.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="../../site_libs/quarto-contrib/iconify-1.0.0-beta.2/iconify-icon.min.js"></script>
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Lucas van Walstijn</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/lucasvw" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/lvWal" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/lucasvanwalstijn/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://www.kaggle.com/lucasvw" rel="" target="">
 <span class="menu-text"><iconify-icon inline="" icon="fa6-brands:kaggle"></iconify-icon></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://lucasvw.github.io/index.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#end-of-last-post" id="toc-end-of-last-post" class="nav-link active" data-scroll-target="#end-of-last-post">End of last post:</a></li>
  <li><a href="#weight-decay" id="toc-weight-decay" class="nav-link" data-scroll-target="#weight-decay">Weight decay</a></li>
  <li><a href="#momentum" id="toc-momentum" class="nav-link" data-scroll-target="#momentum">Momentum</a></li>
  <li><a href="#rmsprop" id="toc-rmsprop" class="nav-link" data-scroll-target="#rmsprop">RMSProp</a></li>
  <li><a href="#adam" id="toc-adam" class="nav-link" data-scroll-target="#adam">Adam</a></li>
  <li><a href="#learning-rate-schedulers" id="toc-learning-rate-schedulers" class="nav-link" data-scroll-target="#learning-rate-schedulers">Learning Rate Schedulers</a></li>
  <li><a href="#resnet" id="toc-resnet" class="nav-link" data-scroll-target="#resnet">ResNet</a></li>
  <li><a href="#parameters-and-macs" id="toc-parameters-and-macs" class="nav-link" data-scroll-target="#parameters-and-macs">Parameters and MACs</a></li>
  <li><a href="#augmentation" id="toc-augmentation" class="nav-link" data-scroll-target="#augmentation">Augmentation</a></li>
  <li><a href="#final-remarks" id="toc-final-remarks" class="nav-link" data-scroll-target="#final-remarks">Final Remarks</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><code>nntrain</code> (4/n): Accelerated optimization</h1>
  <div class="quarto-categories">
    <div class="quarto-category">code</div>
    <div class="quarto-category">neural network</div>
    <div class="quarto-category">deep learning</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Lucas van Walstijn </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">August 28, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>In this series, I want to discuss the creation of a small library for training neural networks: <code>nntrain</code>. It‚Äôs based off the excellent <a href="https://course.fast.ai/">part 2</a> of Practical Deep Learning for Coders by Jeremy Howard, in which from lessons 13 to 18 (roughly) the development of the <code>miniai</code> library is discussed.</p>
<p>The library will build upon PyTorch. We‚Äôll try as much as possible to build from scratch to understand how it all works. Once the main functionality of components are implemented and verified, we can switch over to PyTorch‚Äôs version. This is similar to how things are done in the course. However, this is not just a ‚Äúcopy / paste‚Äù of the course: on many occasions I take a different route, and most of the code is my own. That is not to say that all of this is meant to be extremely innovative, instead I had the following goals:</p>
<ul>
<li>Deeply understand the training of neural networks with a focus on PyTorch</li>
<li>Try to create an even better narrative then what‚Äôs presented in FastAI üôâü§∑‚Äç‚ôÇÔ∏èüôà</li>
<li>Get hands-on experience with creating a library with <a href="https://nbdev.fast.ai/"><code>nb_dev</code></a></li>
</ul>
<p><code>nb_dev</code> is another great project from the fastai community, which allows python libraries to be written in jupyter notebooks. This may sound a bit weird since the mainstream paradigm is to only do experimental work in notebooks. It has the advantage though that we can create the source code for our library in the very same environment in which we want to experiment and interact with our methods, objects and structure <strong>while we are building the library</strong>. For more details on why this is a good idea and other nice features of <code>nb_dev</code>, see <a href="https://www.fast.ai/posts/2022-07-28-nbdev2.html">here</a>.</p>
<p>So without further ado, let‚Äôs start with where we left off in the previous <a href="https://lucasvw.github.io/posts/11_nntrain_activations/">post</a>:</p>
<section id="end-of-last-post" class="level2">
<h2 class="anchored" data-anchor-id="end-of-last-post">End of last post:</h2>
<p>In the last <a href="https://lucasvw.github.io/posts/11_nntrain_activations/">post</a> we build a convolutional neural network, loaded the Fashion-Mnist data using our <a href="https://lucasvw.github.io/nntrain/dataloaders.html">Dataloaders</a> module, and trained it using the <a href="https://lucasvw.github.io/nntrain/learner.html">Learner</a> module. We created a <a href="https://lucasvw.github.io/nntrain/activations.html">module</a> which helped to understand the fine-print of training a model, more specifically by understanding the activations throughout the network during training.</p>
<p>We finally reached a performance of around 89% (evaluation) accuracy by training 5 epochs. In this post, we are going to look closely into how a model is learning: the step in which we update the weights after the backward pass in the training loop.</p>
<p>Updating the weights takes the form of:</p>
<p><span class="math display">\[
w_t = w_{t-1} - lr \cdot grad^{w}_{t-1}
\]</span> or in more pseudocode style syntax: <span class="math display">\[
w \leftarrow w - lr \cdot grad^{w}
\]</span></p>
<p>Before we dive into different versions of this update step and it‚Äôs implications, I would like to have a good look at this remarkebly simple formula. Specifically, it‚Äôs important that the change to the weights from one step to another is composed of the product of the learning rate <span class="math inline">\(lr\)</span> and the gradient of the loss with respect to the weight we are updating: <span class="math inline">\(grad^w\)</span>. And since it‚Äôs a product, doubling one and reducing the other by a factor of 2, cancel each other out.</p>
<p>For example, consider some training setup with some loss function <span class="math inline">\(loss\)</span> and some learning rate <span class="math inline">\(lr\)</span>. If we would change our loss function by adding a factor of 2: <span class="math inline">\(loss_1 = 2 \cdot loss\)</span> this would have the effect that the gradients would also double. Very similiar to how if</p>
<span class="math display">\[\begin{aligned}
f(x) &amp;= x^2 &amp;&amp;\rightarrow f'(x) = 2x \\
g(x) &amp;= 2f(x) = 2x^2  &amp;&amp;\rightarrow g'(x) = 4x = 2f'(x)
\end{aligned}\]</span>
<p>And since learning rate and gradients are multiplied in the weight update, we could offset the doubling of the loss by reducing the learning rate by 2. It‚Äôs thus important to realize how these components interact.</p>
</section>
<section id="weight-decay" class="level2">
<h2 class="anchored" data-anchor-id="weight-decay">Weight decay</h2>
<p>Weight decay is a very common regularization technique. Regularization is meant to reduce overfitting. When a model is overfitting, it basically has learned the structure of the training set to such a degree (e.g.&nbsp;memorized) that it performs very well on the training set, but performance is degraded on the validation set. This means the model is no longer generalizing well to data it was not trained on. And since we are always interested in using a model for making predictions on data it hasn‚Äôt seen (e.g.&nbsp;inference), this is an extremely important thing to be aware of.</p>
<p>Weight decay takes the simple form of adding a component to the loss which penalizes large weights:</p>
<p><span class="math display">\[
loss_{wd} = loss + c\sum{w_i^2}
\]</span></p>
<p>And since this simple sum we are adding has no interaction terms between weights, we simply get:</p>
<span class="math display">\[\begin{aligned}
grad_{wd} &amp;= grad + 2c\cdot w \\
&amp;= grad + d \cdot w
\end{aligned}\]</span>
<p>And then going back to updating the weights:</p>
<span class="math display">\[\begin{aligned}
w_{wd} &amp;\leftarrow w_{wd} - lr \cdot grad_{wd}  \\
&amp;= w_{wd} - lr \cdot (grad + d \cdot w_{wd}) \\
&amp;= w_{wd} - lr \cdot grad - lr \cdot d \cdot w_{wd} \\
&amp;= w_{wd}(1 - lr \cdot d) - lr \cdot grad
\end{aligned}\]</span>
<p>If we compare this expression to the regular update step: <span class="math inline">\(w \leftarrow w - lr \cdot grad^{w}\)</span>, we find that we can simply multiply the weights by <span class="math inline">\((1 - lr \cdot d)\)</span> before we do the step:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a> <span class="co">#| export</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SGD:</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, params, lr, wd<span class="op">=</span><span class="fl">0.</span>):</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.params <span class="op">=</span> <span class="bu">list</span>(params)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lr <span class="op">=</span> lr</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.wd <span class="op">=</span> wd</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>):                    <span class="co"># this is the method that get's called by the Learner</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params:</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.reg_step(p)       <span class="co"># first add regularization</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.opt_step(p)       <span class="co"># then do the actual step</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.i <span class="op">+=</span><span class="dv">1</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> opt_step(<span class="va">self</span>, p):</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        p <span class="op">-=</span> p.grad <span class="op">*</span> <span class="va">self</span>.lr          <span class="co"># regular step</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reg_step(<span class="va">self</span>, p):</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.wd <span class="op">!=</span> <span class="dv">0</span>:               <span class="co"># only regularize when the weight decay parameter is set</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>            p <span class="op">*=</span> <span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.lr<span class="op">*</span><span class="va">self</span>.wd   <span class="co"># update the weights as described above</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> zero_grad(<span class="va">self</span>):</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params:</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>            p.grad.data.zero_()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let‚Äôs try it out:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset,load_dataset_builder</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nntrain.dataloaders <span class="im">import</span> DataLoaders, hf_ds_collate_fn</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nntrain.learner <span class="im">import</span> <span class="op">*</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nntrain.activations <span class="im">import</span> <span class="op">*</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms.functional <span class="im">as</span> TF</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> operator <span class="im">import</span> attrgetter</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> fastcore.<span class="bu">all</span> <span class="im">as</span> fc</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torcheval.metrics <span class="im">as</span> tem</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>name <span class="op">=</span> <span class="st">"fashion_mnist"</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>ds_builder <span class="op">=</span> load_dataset_builder(name)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>hf_dd <span class="op">=</span> load_dataset(name)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>bs <span class="op">=</span> <span class="dv">1024</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>collate <span class="op">=</span> partial(hf_ds_collate_fn, flatten<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> DataLoaders.from_hf_dd(hf_dd, batch_size<span class="op">=</span>bs, collate_fn<span class="op">=</span>collate)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>set_seed(<span class="dv">1</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co"># model and initializaton</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> cnn_layers().<span class="bu">apply</span>(init_weights)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co"># normalization</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>xb, yb <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(dls.train))</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>norm <span class="op">=</span> NormalizationS(xb.mean(), xb.std())</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co"># subscribers</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>subs <span class="op">=</span> [norm, </span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        ProgressS(<span class="va">True</span>),</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        MetricsS(accuracy<span class="op">=</span>tem.MulticlassAccuracy()),</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        DeviceS(device)]</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Remember that the optim_func get's initialized during fit()</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="co"># https://github.com/lucasvw/nntrain/blob/main/nntrain/learners.py#L53C65-L53C65</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>optim_func <span class="op">=</span> partial(SGD, wd<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.4</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> MomentumLearner(model, dls, F.cross_entropy, optim_func, lr, subs)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>l.fit(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"31ebdffcb5af405d8ec502ab213a6844","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"3824d457a7334ddebf622b8745e17e4f","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Downloading and preparing dataset fashion_mnist/fashion_mnist (download: 29.45 MiB, generated: 34.84 MiB, post-processed: Unknown size, total: 64.29 MiB) to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1...
Dataset fashion_mnist downloaded and prepared to /root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1. Subsequent calls will reuse this data.</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ad0c916106cc4a0babf0402321a0e48d","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ba611c1bfcf84e9daada7880c6912a60","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"3945309e887b48eaa12a0bd50ba31238","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"5aa81ca5a3c545beaf85c412628cd03f","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"08ee275194344713b9ec63d722a10c11","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"66c380d706e5459b96f2499be1d89686","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"aa6489623b734880857fc76fd990bc0a","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">mode</th>
<th data-quarto-table-cell-role="th">loss</th>
<th data-quarto-table-cell-role="th">accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>train</td>
<td>0.590</td>
<td>0.788</td>
</tr>
<tr class="even">
<td>0</td>
<td>eval</td>
<td>0.406</td>
<td>0.846</td>
</tr>
<tr class="odd">
<td>1</td>
<td>train</td>
<td>0.350</td>
<td>0.872</td>
</tr>
<tr class="even">
<td>1</td>
<td>eval</td>
<td>0.365</td>
<td>0.862</td>
</tr>
<tr class="odd">
<td>2</td>
<td>train</td>
<td>0.305</td>
<td>0.888</td>
</tr>
<tr class="even">
<td>2</td>
<td>eval</td>
<td>0.349</td>
<td>0.871</td>
</tr>
<tr class="odd">
<td>3</td>
<td>train</td>
<td>0.280</td>
<td>0.896</td>
</tr>
<tr class="even">
<td>3</td>
<td>eval</td>
<td>0.342</td>
<td>0.876</td>
</tr>
<tr class="odd">
<td>4</td>
<td>train</td>
<td>0.261</td>
<td>0.903</td>
</tr>
<tr class="even">
<td>4</td>
<td>eval</td>
<td>0.308</td>
<td>0.889</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-5-output-15.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="momentum" class="level2">
<h2 class="anchored" data-anchor-id="momentum">Momentum</h2>
<p>Momentum has to do with smartly navigating the loss surface. The loss surface is a high dimensional plain (manifold) where each dimension is a single parameter in the model. Since humans are not made for thinking beyond 3 dimensions, let‚Äôs thus quickly replace this by a mental image with just 3 dimensions. Let the loss surface have just 2 parameters that are put on the x and y dimensions, and let z represent the value of our loss function.</p>
<p>Training a neural network is all about finding a low (the lowest) spot on this surface, e.g.&nbsp;a point in this space in which the parameters (x and y) take on a value so that the loss (z direction) is low.</p>
<p>To find such a spot, we have to navigate this surface. Sometimes it might be difficult to find this spot from the point where we are, since the lowest value might lay behind a hill. Realize that we also never ‚Äúsee‚Äù the complete surface, we can only navigate it by computing gradients from the point where we currently are. We can draw an analogy of a astronaut being dropped in a completely alien world, having a need to find water (the lowest point on the surface) as quickly as possible (because of limitted oxygen supply) with a visibility of only 1 meter.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="loss_surface2.png" class="img-fluid figure-img" width="400"></p>
<figcaption class="figure-caption">2D Loss surface: difficult to find the lowest spot from a random position with super short visibility!</figcaption>
</figure>
</div>
<p>So what‚Äôs our strategy to find water asap? With SGD we compute the gradient based on the contributions of each sample in the minibatch. Then we take a small step in this direction (of size: learning rate x gradient). And we keep repeating this. But imagine that each time we compute this gradient, we find that it‚Äôs more or less in the same direction. Would it then not make sense to maybe take a bit of a larger step? This is the intuition behind momentum, don‚Äôt just consider the gradient at the current position, but get some sense of the surrounding surface by inferring from the past couple of computed gradients. The following image shows this also nicely:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="momentum.png" class="img-fluid figure-img" width="600"></p>
<figcaption class="figure-caption">On the left SGD without momentum: just compute the gradient each batch and follow that. On the right, use momentum to find an average gradient across multiple batches. Note that this cancels the vertical component of the gradient somewhat out, and reinforces the horizontal component of the gradient, increasing the speed in the right direction.</figcaption>
</figure>
</div>
<p>To implement, we simply do:</p>
<span class="math display">\[\begin{aligned}
w &amp;\leftarrow w - lr \cdot grad\_avg
\end{aligned}\]</span>
<p>The definition of <span class="math inline">\(grad\_avg\)</span> takes different forms. In PyTorch SGD it‚Äôs simply:</p>
<p><span class="math display">\[
grad\_avg \leftarrow grad + \mu \cdot grad\_avg
\]</span></p>
<p>And this is also exactly the way we previously implemented in the <a href="https://lucasvw.github.io/posts/10_nntrain_learner/#momentumlearner"><code>MomentumLearner</code></a>. Additionally PyTorch defines a dampening parameter <span class="math inline">\(\tau\)</span> (defaulting to zero):</p>
<p><span class="math display">\[
grad\_avg \leftarrow (1 - \tau) grad + \mu \cdot grad\_avg
\]</span></p>
<p>Jeremy Howard from Fastai suggests we take a weighted average between the grad and the running average. This is equivalent to setting <span class="math inline">\(\tau = \mu\)</span> (dampening = momentum) in PyTorch:</p>
<p><span class="math display">\[
grad\_avg \leftarrow (1-c) grad + c \cdot grad\_avg
\]</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a> <span class="co">#| export</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Momentum(SGD):</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, params, lr, wd<span class="op">=</span><span class="fl">0.</span>, mom<span class="op">=</span><span class="fl">0.9</span>):</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(params, lr<span class="op">=</span>lr, wd<span class="op">=</span>wd)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mom<span class="op">=</span>mom</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> opt_step(<span class="va">self</span>, p):</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="bu">hasattr</span>(p, <span class="st">'grad_avg'</span>): p.grad_avg <span class="op">=</span> torch.zeros_like(p.grad)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        p.grad_avg <span class="op">=</span> p.grad_avg<span class="op">*</span><span class="va">self</span>.mom <span class="op">+</span> p.grad<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span><span class="va">self</span>.mom)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        p <span class="op">-=</span> <span class="va">self</span>.lr <span class="op">*</span> p.grad_avg</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>set_seed(<span class="dv">1</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> cnn_layers().<span class="bu">apply</span>(init_weights)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>optim_func <span class="op">=</span> Momentum</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">1.5</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> Learner(model, dls, F.cross_entropy, optim_func, lr, subs)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>l.fit(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">mode</th>
<th data-quarto-table-cell-role="th">loss</th>
<th data-quarto-table-cell-role="th">accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>train</td>
<td>0.606</td>
<td>0.784</td>
</tr>
<tr class="even">
<td>0</td>
<td>eval</td>
<td>0.439</td>
<td>0.838</td>
</tr>
<tr class="odd">
<td>1</td>
<td>train</td>
<td>0.359</td>
<td>0.869</td>
</tr>
<tr class="even">
<td>1</td>
<td>eval</td>
<td>0.372</td>
<td>0.861</td>
</tr>
<tr class="odd">
<td>2</td>
<td>train</td>
<td>0.322</td>
<td>0.881</td>
</tr>
<tr class="even">
<td>2</td>
<td>eval</td>
<td>0.372</td>
<td>0.864</td>
</tr>
<tr class="odd">
<td>3</td>
<td>train</td>
<td>0.293</td>
<td>0.892</td>
</tr>
<tr class="even">
<td>3</td>
<td>eval</td>
<td>0.329</td>
<td>0.878</td>
</tr>
<tr class="odd">
<td>4</td>
<td>train</td>
<td>0.271</td>
<td>0.900</td>
</tr>
<tr class="even">
<td>4</td>
<td>eval</td>
<td>0.314</td>
<td>0.886</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-7-output-3.png" class="img-fluid"></p>
</div>
</div>
<p>Geoffrey Hinton in this <a href="https://youtu.be/z2rTn8Evav8?si=Sd6surVPoo1c7pr8">lecture</a> on Momentum mentions that one problem with using plain SGD with a large learning rate is that you can get divergent oscillations if you try to travel down a long and narrow canyon. This would be similar to what happens on the left hand side of the figure above, but now with a learning rate so big that we actually diverge from the goal. By adding momentum he argues, it‚Äôs easier to find the ‚Äúdominant‚Äù (horizontal) direction and we can thus increase the learning rate without causing divergent oscillations.</p>
<p>One issue he describes, is that in the beginning of training when the gradients are still large, the momentum should be kept much smaller since otherwise the updates become just too big. One way to deal with these would be to change the momentum during training (but more about that later when we start looking into schedulers) or by using something like RMSProp:</p>
</section>
<section id="rmsprop" class="level2">
<h2 class="anchored" data-anchor-id="rmsprop">RMSProp</h2>
<p>RMSProp is a technique (first?) described by Geoffrey Hinton in an online <a href="https://www.youtube.com/watch?v=XhZahXzEuNo">MOOC</a>. Jeremy Howard describes RMSProp as ‚ÄúMomentum, but with the squares of the gradient‚Äù. I personally think this is a rather bad explanation, since this is <strong>not at all</strong> what‚Äôs going on. Instead it‚Äôs a technique that is concerned with the differences in magnitude of the gradients of different weights. This leads to weights getting updated at very different ‚Äúspeeds‚Äù. E.g. if you have one weight which gradient is 10 times the gradient of another, that weight get‚Äôs an update that‚Äôs 10 times as large as the other one.</p>
<p>This can exist between weights at one single step, but also between gradients of a single weight during different times in training.</p>
<p>The reason for this is of course that the learning rate is equal <strong>at all times for all parameters</strong>. RMSProp is a technique which scales the gradients inversely to the (averaged out) magnitude of the gradient:</p>
<p><span class="math display">\[
w \leftarrow w - lr \cdot \frac{grad}{\sqrt{acc\_sqrd\_grad}}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
acc\_sqrd\_grad = c \cdot acc\_sqrd\_grad + (1-c) grad^2
\]</span></p>
<p>From which it should be clear that using RMSprop has a large impact on the weight updates, you will thus have to recalibrate your learning rate.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a> <span class="co">#| export</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RMSProp(SGD):</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, params, lr, wd<span class="op">=</span><span class="fl">0.</span>, sqr_mom<span class="op">=</span><span class="fl">0.99</span>, eps<span class="op">=</span><span class="fl">1e-5</span>):</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(params, lr<span class="op">=</span>lr, wd<span class="op">=</span>wd)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sqr_mom <span class="op">=</span> sqr_mom</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> opt_step(<span class="va">self</span>, p):</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="bu">hasattr</span>(p, <span class="st">'sqr_avg'</span>): </span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>            p.sqr_avg <span class="op">=</span> p.grad<span class="op">**</span><span class="dv">2</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        p.sqr_avg <span class="op">=</span> p.sqr_avg<span class="op">*</span><span class="va">self</span>.sqr_mom <span class="op">+</span> (<span class="dv">1</span><span class="op">-</span><span class="va">self</span>.sqr_mom)<span class="op">*</span>p.grad<span class="op">**</span><span class="dv">2</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        p <span class="op">-=</span> <span class="va">self</span>.lr <span class="op">*</span> p.grad<span class="op">/</span>(p.sqr_avg.sqrt() <span class="op">+</span> <span class="va">self</span>.eps)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>set_seed(<span class="dv">1</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> cnn_layers().<span class="bu">apply</span>(init_weights)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>optim_func <span class="op">=</span> RMSProp</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">3e-3</span>                        <span class="co"># much smaller learning rate!</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> Learner(model, dls, F.cross_entropy, optim_func, lr, subs)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>l.fit(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">mode</th>
<th data-quarto-table-cell-role="th">loss</th>
<th data-quarto-table-cell-role="th">accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>train</td>
<td>0.699</td>
<td>0.754</td>
</tr>
<tr class="even">
<td>0</td>
<td>eval</td>
<td>0.472</td>
<td>0.831</td>
</tr>
<tr class="odd">
<td>1</td>
<td>train</td>
<td>0.421</td>
<td>0.847</td>
</tr>
<tr class="even">
<td>1</td>
<td>eval</td>
<td>0.452</td>
<td>0.837</td>
</tr>
<tr class="odd">
<td>2</td>
<td>train</td>
<td>0.373</td>
<td>0.862</td>
</tr>
<tr class="even">
<td>2</td>
<td>eval</td>
<td>0.387</td>
<td>0.858</td>
</tr>
<tr class="odd">
<td>3</td>
<td>train</td>
<td>0.344</td>
<td>0.874</td>
</tr>
<tr class="even">
<td>3</td>
<td>eval</td>
<td>0.378</td>
<td>0.860</td>
</tr>
<tr class="odd">
<td>4</td>
<td>train</td>
<td>0.322</td>
<td>0.881</td>
</tr>
<tr class="even">
<td>4</td>
<td>eval</td>
<td>0.352</td>
<td>0.875</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-9-output-3.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="adam" class="level2">
<h2 class="anchored" data-anchor-id="adam">Adam</h2>
<p>Adam is probably the most common optimizer used in practice, and is nothing more then the combination of RMSProp and Momentum:</p>
<p><span class="math display">\[
w \leftarrow w - lr \cdot \frac{avg\_grad}{\sqrt{acc\_sqrd\_grad}}
\]</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a> <span class="co">#| export</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Adam(SGD):</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, params, lr, wd<span class="op">=</span><span class="fl">0.</span>, beta1<span class="op">=</span><span class="fl">0.9</span>, beta2<span class="op">=</span><span class="fl">0.99</span>, eps<span class="op">=</span><span class="fl">1e-5</span>):</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(params, lr<span class="op">=</span>lr, wd<span class="op">=</span>wd)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.beta1,<span class="va">self</span>.beta2,<span class="va">self</span>.eps <span class="op">=</span> beta1,beta2,eps</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> opt_step(<span class="va">self</span>, p):</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="bu">hasattr</span>(p, <span class="st">'avg'</span>): </span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>            p.avg <span class="op">=</span> torch.zeros_like(p.grad.data)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>            p.sqr_avg <span class="op">=</span> torch.zeros_like(p.grad.data)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>        p.avg <span class="op">=</span> <span class="va">self</span>.beta1<span class="op">*</span>p.avg <span class="op">+</span> (<span class="dv">1</span><span class="op">-</span><span class="va">self</span>.beta1)<span class="op">*</span>p.grad</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>        unbias_avg <span class="op">=</span> p.avg <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> (<span class="va">self</span>.beta1<span class="op">**</span>(<span class="va">self</span>.i<span class="op">+</span><span class="dv">1</span>)))</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>        p.sqr_avg <span class="op">=</span> <span class="va">self</span>.beta2<span class="op">*</span>p.sqr_avg <span class="op">+</span> (<span class="dv">1</span><span class="op">-</span><span class="va">self</span>.beta2)<span class="op">*</span>(p.grad<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>        unbias_sqr_avg <span class="op">=</span> p.sqr_avg <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> (<span class="va">self</span>.beta2<span class="op">**</span>(<span class="va">self</span>.i<span class="op">+</span><span class="dv">1</span>)))</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>        p <span class="op">-=</span> <span class="va">self</span>.lr <span class="op">*</span> unbias_avg <span class="op">/</span> (unbias_sqr_avg <span class="op">+</span> <span class="va">self</span>.eps).sqrt()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>set_seed(<span class="dv">1</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> cnn_layers().<span class="bu">apply</span>(init_weights)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>optim_func <span class="op">=</span> Adam</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">6e-3</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> Learner(model, dls, F.cross_entropy, optim_func, lr, subs)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>l.fit(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">mode</th>
<th data-quarto-table-cell-role="th">loss</th>
<th data-quarto-table-cell-role="th">accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>train</td>
<td>0.609</td>
<td>0.783</td>
</tr>
<tr class="even">
<td>0</td>
<td>eval</td>
<td>0.449</td>
<td>0.838</td>
</tr>
<tr class="odd">
<td>1</td>
<td>train</td>
<td>0.366</td>
<td>0.865</td>
</tr>
<tr class="even">
<td>1</td>
<td>eval</td>
<td>0.373</td>
<td>0.866</td>
</tr>
<tr class="odd">
<td>2</td>
<td>train</td>
<td>0.320</td>
<td>0.882</td>
</tr>
<tr class="even">
<td>2</td>
<td>eval</td>
<td>0.365</td>
<td>0.863</td>
</tr>
<tr class="odd">
<td>3</td>
<td>train</td>
<td>0.290</td>
<td>0.893</td>
</tr>
<tr class="even">
<td>3</td>
<td>eval</td>
<td>0.325</td>
<td>0.881</td>
</tr>
<tr class="odd">
<td>4</td>
<td>train</td>
<td>0.269</td>
<td>0.902</td>
</tr>
<tr class="even">
<td>4</td>
<td>eval</td>
<td>0.323</td>
<td>0.882</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-11-output-3.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="learning-rate-schedulers" class="level2">
<h2 class="anchored" data-anchor-id="learning-rate-schedulers">Learning Rate Schedulers</h2>
<p>Now let‚Äôs look at learning rate schedulers: these are objects that change important parameters such as the learning rate and momentum <strong>during training</strong>. The <code>OneCycleLR</code> scheduler for example starts with a low learning rate, then increases it to a maximum value and then anneales it back to the lower value.</p>
<p>The idea being that initially, when our weights are random, we don‚Äôt want to take too large steps. As the weights are getting somewhat reasonable, we speed up the learning and once we arrive at a better solution because of this, we decrease the learning rate again to make sure we can squeeze out the last bit of performance gain:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">3e-3</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Every scheduler has the concept of steps. </span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Either specified as done here or as "epochs" and "steps_per_epoch" (=# of batches) as two separate args</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>steps <span class="op">=</span> <span class="dv">100</span>   </span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>sch <span class="op">=</span> torch.optim.lr_scheduler.OneCycleLR(opt, max_lr<span class="op">=</span><span class="fl">6e-2</span>, total_steps<span class="op">=</span>steps)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co"># The scheduler has a convenience method for getting the current learning rate</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>lrs <span class="op">=</span> [sch.get_last_lr()]</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="co"># For other parameters such as momentum, we have to look into the optimizer itself</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>moms <span class="op">=</span> [opt.param_groups[<span class="dv">0</span>][<span class="st">'momentum'</span>]]</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    opt.step()                 </span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    sch.step()                       <span class="co"># calling step() on the scheduler updates the parameters of the optimizer</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    lrs.append(sch.get_last_lr())</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    moms.append(opt.param_groups[<span class="dv">0</span>][<span class="st">'momentum'</span>])</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">5</span>))</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].plot(lrs)<span class="op">;</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].set_title(<span class="st">'learning rate'</span>)<span class="op">;</span></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].plot(moms)<span class="op">;</span></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].set_title(<span class="st">'momentum'</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-12-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Besides the cycling of the learning rate, the momentum is cycled inversely with the <code>OneCycleLr</code> scheduler</p>
<p>Let‚Äôs see if we can create a Subscriber that enables the use of Schedulers in our framework:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a> <span class="co">#| export</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SchedulerS(Subscriber):</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, scheduler_class):</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scheduler_class <span class="op">=</span> scheduler_class</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># intialize the scheduler instance after the optimizer has been intialized</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> before_fit(<span class="va">self</span>, learn):</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scheduler <span class="op">=</span> <span class="va">self</span>.scheduler_class(learn.opt) </span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># step the scheduler after the optimizer has stepped</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> after_step(<span class="va">self</span>, learn):</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scheduler.step()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>set_seed(<span class="dv">1</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="bu">len</span>(dls.train)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> cnn_layers().<span class="bu">apply</span>(init_weights)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">6e-2</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>sch <span class="op">=</span> partial(torch.optim.lr_scheduler.OneCycleLR, max_lr<span class="op">=</span>lr, epochs<span class="op">=</span>epochs, steps_per_epoch<span class="op">=</span>N)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>subs_sch <span class="op">=</span> subs <span class="op">+</span> [SchedulerS(sch)]</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> Learner(model, dls, F.cross_entropy, torch.optim.Adam, lr, subs_sch)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>l.fit(epochs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">mode</th>
<th data-quarto-table-cell-role="th">loss</th>
<th data-quarto-table-cell-role="th">accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>train</td>
<td>0.679</td>
<td>0.762</td>
</tr>
<tr class="even">
<td>0</td>
<td>eval</td>
<td>0.818</td>
<td>0.763</td>
</tr>
<tr class="odd">
<td>1</td>
<td>train</td>
<td>0.386</td>
<td>0.859</td>
</tr>
<tr class="even">
<td>1</td>
<td>eval</td>
<td>0.372</td>
<td>0.860</td>
</tr>
<tr class="odd">
<td>2</td>
<td>train</td>
<td>0.298</td>
<td>0.889</td>
</tr>
<tr class="even">
<td>2</td>
<td>eval</td>
<td>0.321</td>
<td>0.880</td>
</tr>
<tr class="odd">
<td>3</td>
<td>train</td>
<td>0.244</td>
<td>0.910</td>
</tr>
<tr class="even">
<td>3</td>
<td>eval</td>
<td>0.278</td>
<td>0.894</td>
</tr>
<tr class="odd">
<td>4</td>
<td>train</td>
<td>0.208</td>
<td>0.923</td>
</tr>
<tr class="even">
<td>4</td>
<td>eval</td>
<td>0.265</td>
<td>0.902</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-14-output-3.png" class="img-fluid"></p>
</div>
</div>
<p>And that helps us to reach 90% accuracy! Not bad for a simple convolutional neural network</p>
</section>
<section id="resnet" class="level2">
<h2 class="anchored" data-anchor-id="resnet">ResNet</h2>
<p>Next, let‚Äôs see if we can increase the depth of the model even more: we can use a reduced stride (=1) in the first convolution so that we don‚Äôt reduce the pixel-grid. We can thus add one more convolution at the end, to end up with a 1x1 pixel grid:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cnn_layers_more_depth():</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nn.Sequential(                               <span class="co"># 28x28  </span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>        conv_block(<span class="dv">1</span> , <span class="dv">8</span>, stride<span class="op">=</span><span class="dv">1</span>),                    <span class="co"># 28x28</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>        conv_block(<span class="dv">8</span> ,<span class="dv">16</span>),                              <span class="co"># 14x14</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>        conv_block(<span class="dv">16</span>,<span class="dv">32</span>),                              <span class="co"># 7x7</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>        conv_block(<span class="dv">32</span>,<span class="dv">64</span>),                              <span class="co"># 4x4</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>        conv_block(<span class="dv">64</span>,<span class="dv">128</span>),                             <span class="co"># 2x2</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>        conv_block(<span class="dv">128</span>,<span class="dv">10</span>, norm<span class="op">=</span><span class="va">False</span>, act<span class="op">=</span><span class="va">False</span>),      <span class="co"># 1x1</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>        nn.Flatten())</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>set_seed(<span class="dv">1</span>)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> cnn_layers_more_depth().<span class="bu">apply</span>(init_weights)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> Learner(model, dls, F.cross_entropy, torch.optim.Adam, lr, subs_sch)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>l.fit(epochs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">mode</th>
<th data-quarto-table-cell-role="th">loss</th>
<th data-quarto-table-cell-role="th">accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>train</td>
<td>0.571</td>
<td>0.801</td>
</tr>
<tr class="even">
<td>0</td>
<td>eval</td>
<td>0.600</td>
<td>0.821</td>
</tr>
<tr class="odd">
<td>1</td>
<td>train</td>
<td>0.325</td>
<td>0.880</td>
</tr>
<tr class="even">
<td>1</td>
<td>eval</td>
<td>0.399</td>
<td>0.864</td>
</tr>
<tr class="odd">
<td>2</td>
<td>train</td>
<td>0.248</td>
<td>0.909</td>
</tr>
<tr class="even">
<td>2</td>
<td>eval</td>
<td>0.302</td>
<td>0.894</td>
</tr>
<tr class="odd">
<td>3</td>
<td>train</td>
<td>0.192</td>
<td>0.929</td>
</tr>
<tr class="even">
<td>3</td>
<td>eval</td>
<td>0.234</td>
<td>0.914</td>
</tr>
<tr class="odd">
<td>4</td>
<td>train</td>
<td>0.143</td>
<td>0.948</td>
</tr>
<tr class="even">
<td>4</td>
<td>eval</td>
<td>0.228</td>
<td>0.919</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-15-output-3.png" class="img-fluid"></p>
</div>
</div>
<p>The question arises whether we can keep increasing the depth of our model to boost our performance. Since we already exhausted the pixel grid, the only way would be to add more stride 1 convolutions.</p>
<p>However, this doesn‚Äôt improve performance. Kaiming et al found in ‚ÄúDeep Residual Learning for Image Recognition‚Äù that deeper CNN models often perform worse then shallow CNN models. This is surprising since a deeper model can be understood as a superset of a more shallow model. It‚Äôs easy to see this if we consider that a deeper model can just duplicate the shallow model in it‚Äôs initial layers matching the layers of the shallow model, and keep it‚Äôs excess layers as an identity, passing through the data <em>as is</em>. But the observation they made, is that during training deeper model are performing worse and thus this ‚Äúoptimum‚Äù is apparently not found while training the deeper model.</p>
<p>To help the training process of deeper models, they came up with an architecture which should be able to <em>more easily</em> replicate the shallow network. And the way they did this, was by adding skip connections:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="skip_connection.png" class="img-fluid figure-img" width="600"></p>
<figcaption class="figure-caption">ResBlock consisting of two paths: a convolutional path, and a path in which the signal is propagated directly</figcaption>
</figure>
</div>
<p>So instead of having a fully sequential model, they added these identity skip connections which connects both the input to and the output of the convolutional path. They argued that by doing so, it would be trivial for the model to forward the data without modifying it. This way, the model could use the convolutional path for the depth of ‚Äúshallow‚Äù model, and use the skip connections for the remaining layers that only the ‚Äúdeep‚Äù model has.</p>
<p>Why a double convolutional layer? For that we come back to the idea of adding stride 1 convolution to increase the depth of our model. In the architecture described above, the first convolutional layer is a stride 1 convolution (keeping the pixel grid constant) and increases the number of filters. The second convolution is a stride 2 convolution which reduces the pixel grid by a factor of 2, but keeps the number of filters constant. <strong>The first convolution is thus expanding the data, and the second convolution compresses it, this is different from the standard ResBlock <a href="https://github.com/pytorch/vision/blob/1aef87d01eec2c0989458387fa04baebcc86ea7b/torchvision/models/resnet.py#L48-L51">implementation</a> in Pytorch and works significantly better in my experiments</strong>.</p>
<p>The name for the block shown above is a ResBlock, and a model that is using them is called a ResNet. Res is short for residual, and that name becomes apparent when we look at this block in the following way:</p>
<span class="math display">\[\begin{aligned}
&amp; y &amp;&amp;= x + conv_2(conv_1(x)) \\
&amp; y - x &amp;&amp;= conv_2(conv_1(x)) \\
&amp; residual &amp;&amp;= conv_2(conv_1(x))
\end{aligned}\]</span>
<p>The term <span class="math inline">\(y-x\)</span> is often called ‚Äúthe residual‚Äù in statistics. Now let‚Äôs see how we can create such a ResBlock, we start with the convolutional path:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a> <span class="co">#| export</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conv_conn(in_c, out_c, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nn.Sequential(</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>        conv_block(in_c, out_c, kernel_size<span class="op">=</span>kernel_size, stride<span class="op">=</span><span class="dv">1</span>, act<span class="op">=</span><span class="va">True</span>, norm<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>        conv_block(out_c, out_c, kernel_size<span class="op">=</span>kernel_size, stride<span class="op">=</span>stride, act<span class="op">=</span><span class="va">False</span>, norm<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>And let‚Äôs create the ResBlock with a couple of notes upfront:</p>
<ul>
<li>We can only use a ‚Äútrue‚Äù identity connection (<code>y_id = x</code>) under the condition that the convolutions don‚Äôt change the number of filters (<code>in_c = out_c</code>) nor the pixel grid (<code>stride=1</code>). In other words, we can only use a true identity connection if the trailing 3 dimensions in our batch NCHW stay constant. If this is not the case, we have to ‚Äúmatch up‚Äù the identity connection as <em>simple as possible</em> to these new dimensions:
<ul>
<li>If the number of filters has changed we thus add a convolution with stride and kernel size equal to 1, only changing the number of channels.</li>
<li>If the pixel grid has changed (halved in size due to a stride 2 convolution in the convolutional connection), we reduce the size of pixel grid in the same way, making use of a <code>AvgPool2d</code> layer, effectively averaging with a 2x2 kernel size.</li>
</ul></li>
<li>Note also that the second convolution doesn‚Äôt have an activation, this is because we want to apply the activation after summing both the contributions from the identity path and the convolutional path</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a> <span class="co">#| export</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ResBlock(nn.Module):</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_c, out_c, stride<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.in_c <span class="op">=</span> in_c</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out_c <span class="op">=</span> out_c</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.stride <span class="op">=</span> stride</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv_conn <span class="op">=</span> conv_conn(in_c, out_c, stride<span class="op">=</span>stride)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.identity_conn <span class="op">=</span> conv_block(in_c, out_c, kernel_size<span class="op">=</span><span class="dv">1</span>, stride<span class="op">=</span><span class="dv">1</span>, act<span class="op">=</span><span class="va">False</span>, norm<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pooling <span class="op">=</span> torch.nn.AvgPool2d(<span class="dv">2</span>, ceil_mode<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu <span class="op">=</span> nn.ReLU()</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>        y_conv <span class="op">=</span> <span class="va">self</span>.conv_conn(x)</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.in_c <span class="op">==</span> <span class="va">self</span>.out_c: y_id <span class="op">=</span> x</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.stride <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>            y_id <span class="op">=</span> <span class="va">self</span>.identity_conn(x)</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>            y_id <span class="op">=</span> <span class="va">self</span>.pooling(<span class="va">self</span>.identity_conn(x))</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.relu(y_conv <span class="op">+</span> y_id)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Creating the ResNet is very similar to what we have done before, we basically replace the <code>conv_block</code> with a <code>ResBlock</code>. Additionally we change the head of the model: First we flatten, then we use a linear layer to map to the required dimensions (10 output categories) and we conclude with a final BatchNorm layer.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Although the final BatchNorm layer has a positive impact on performance, I don‚Äôt have a good intuition as to the reasons why. I would think that the distributions of logits would be not very gaussian in an ideal setting: the correct categories should get much higher outputs then the other ones. Perhaps this works in some second order effect as regularization, or perhaps it has to do with the scaling that the BatchNorm layer does after the normalization. If anybody has a good intuition for this, please let me knowüôè</p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a> <span class="co">#| export</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> resnet():</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nn.Sequential(                             <span class="co"># pixel grid input: 28x28  </span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>        ResBlock(<span class="dv">1</span> , <span class="dv">8</span>, stride<span class="op">=</span><span class="dv">1</span>),                    <span class="co"># 28x28</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>        ResBlock(<span class="dv">8</span> ,<span class="dv">16</span>),                              <span class="co"># 14x14</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>        ResBlock(<span class="dv">16</span>,<span class="dv">32</span>),                              <span class="co"># 7x7</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>        ResBlock(<span class="dv">32</span>,<span class="dv">64</span>),                              <span class="co"># 4x4</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>        ResBlock(<span class="dv">64</span>,<span class="dv">128</span>),                             <span class="co"># 2x2</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>        ResBlock(<span class="dv">128</span>,<span class="dv">256</span>),                            <span class="co"># 1x1</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>        nn.Flatten(),                                 <span class="co"># flatten to 256 features</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>        nn.Linear(<span class="dv">256</span>, <span class="dv">10</span>, bias<span class="op">=</span><span class="va">False</span>),               <span class="co"># linear layer to map to 10 output features</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>        nn.BatchNorm1d(<span class="dv">10</span>)                            <span class="co"># final batchnorm layer</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>set_seed(<span class="dv">1</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> resnet().<span class="bu">apply</span>(init_weights)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>lrfind <span class="op">=</span> LRFindS()</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>lrfind_subs <span class="op">=</span> [norm,</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>               DeviceS(device),</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>               lrfind]</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>start_lr <span class="op">=</span> <span class="fl">1e-5</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> Learner(model, dls, F.cross_entropy, torch.optim.Adam, start_lr, lrfind_subs)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>l.fit(<span class="dv">1</span>)</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>lrfind.plot()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-19-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>set_seed(<span class="dv">1</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> resnet().<span class="bu">apply</span>(init_weights)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">1e-2</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>sch <span class="op">=</span> partial(torch.optim.lr_scheduler.OneCycleLR, max_lr<span class="op">=</span>lr, epochs<span class="op">=</span>epochs, steps_per_epoch<span class="op">=</span>N)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>subs_sch <span class="op">=</span> subs <span class="op">+</span> [SchedulerS(sch)]</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> Learner(model, dls, F.cross_entropy, torch.optim.Adam, lr, subs_sch)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>l.fit(epochs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">mode</th>
<th data-quarto-table-cell-role="th">loss</th>
<th data-quarto-table-cell-role="th">accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>train</td>
<td>0.815</td>
<td>0.783</td>
</tr>
<tr class="even">
<td>0</td>
<td>eval</td>
<td>0.578</td>
<td>0.846</td>
</tr>
<tr class="odd">
<td>1</td>
<td>train</td>
<td>0.399</td>
<td>0.889</td>
</tr>
<tr class="even">
<td>1</td>
<td>eval</td>
<td>0.372</td>
<td>0.885</td>
</tr>
<tr class="odd">
<td>2</td>
<td>train</td>
<td>0.282</td>
<td>0.914</td>
</tr>
<tr class="even">
<td>2</td>
<td>eval</td>
<td>0.292</td>
<td>0.906</td>
</tr>
<tr class="odd">
<td>3</td>
<td>train</td>
<td>0.212</td>
<td>0.934</td>
</tr>
<tr class="even">
<td>3</td>
<td>eval</td>
<td>0.257</td>
<td>0.918</td>
</tr>
<tr class="odd">
<td>4</td>
<td>train</td>
<td>0.159</td>
<td>0.953</td>
</tr>
<tr class="even">
<td>4</td>
<td>eval</td>
<td>0.237</td>
<td>0.927</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-20-output-3.png" class="img-fluid"></p>
</div>
</div>
<p>Which reaches a score of almost 93% within 5 epochs!</p>
</section>
<section id="parameters-and-macs" class="level2">
<h2 class="anchored" data-anchor-id="parameters-and-macs">Parameters and MACs</h2>
<p>Now that we are able to create fairly deep models, it‚Äôs a good idea to be able to track the memory footprint and the amount of compute that is involved in doing a single forward pass. To track the memory, we can look at the amount of parameters of each layer, and for the compute we can try to estimate something equivalent to Multiply‚Äìaccumulate operations (MACs). Let‚Äôs create a small Subscriber that tracks thes stats by running one single batch and then cancels the fit:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a> <span class="co">#| export</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ModelMonitorS(Subscriber):</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, modules): <span class="va">self</span>.modules <span class="op">=</span> modules</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> before_fit(<span class="va">self</span>, learn):</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hooks <span class="op">=</span> [Hook(i, module, partial(<span class="va">self</span>.record_stats, learn)) <span class="cf">for</span> i, module <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.modules)]</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> record_stats(<span class="va">self</span>, learn, hook, layer, inp, outp):</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> learn.model.training:</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>            hook.nparams <span class="op">=</span> <span class="bu">sum</span>(submodule.numel() <span class="cf">for</span> submodule <span class="kw">in</span> layer.parameters())</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">isinstance</span>(layer, ResBlock):</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>                <span class="co"># K √ó K √ó Cin √ó Hout √ó Wout √ó Cout source=https://machinethink.net/blog/how-fast-is-my-model/</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>                mac_conv1 <span class="op">=</span> <span class="dv">9</span> <span class="op">*</span> layer.in_c <span class="op">*</span> inp[<span class="dv">0</span>].shape[<span class="dv">2</span>] <span class="op">*</span> inp[<span class="dv">0</span>].shape[<span class="dv">3</span>] <span class="op">*</span> layer.out_c</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>                mac_conv2 <span class="op">=</span> <span class="dv">9</span> <span class="op">*</span> layer.out_c <span class="op">*</span> outp.shape[<span class="dv">2</span>] <span class="op">*</span> outp.shape[<span class="dv">3</span>] <span class="op">*</span> layer.out_c    </span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>                hook.mac <span class="op">=</span> (mac_conv1 <span class="op">+</span> mac_conv2) <span class="op">/</span> <span class="fl">1e6</span></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> layer.stride <span class="op">!=</span> <span class="dv">1</span>:</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># Add identity conv</span></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>                    hook.mac <span class="op">+=</span> (layer.in_c <span class="op">*</span> outp.shape[<span class="dv">2</span>] <span class="op">*</span> outp.shape[<span class="dv">3</span>] <span class="op">*</span> layer.out_c <span class="op">/</span> <span class="fl">1e6</span>)</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>                hook.mac <span class="op">=</span> hook.nparams <span class="op">/</span> <span class="fl">1e6</span></span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>            hook.batch_size <span class="op">=</span> inp[<span class="dv">0</span>].shape[<span class="dv">0</span>]</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>            hook.in_shape <span class="op">=</span> <span class="bu">list</span>(inp[<span class="dv">0</span>].shape[<span class="dv">1</span>:])</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>            hook.out_shape <span class="op">=</span> <span class="bu">list</span>(outp.shape[<span class="dv">1</span>:])</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> after_batch(<span class="va">self</span>, learn):</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> h <span class="kw">in</span> <span class="va">self</span>.hooks: h.remove()</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> CancelFitException                   <span class="co"># Only run this for a single batch, then cancel</span></span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__repr__</span>(<span class="va">self</span>):</span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span><span class="st">"layer"</span><span class="sc">:&lt;20}</span><span class="ss"> : </span><span class="sc">{</span><span class="st">"input"</span><span class="sc">:&lt;20}</span><span class="ss"> : </span><span class="sc">{</span><span class="st">"output"</span><span class="sc">:&lt;20}</span><span class="ss"> : </span><span class="sc">{</span><span class="st">"# params"</span><span class="sc">:&gt;10}</span><span class="ss"> : </span><span class="sc">{</span><span class="st">"# MACs"</span><span class="sc">:&gt;10}</span><span class="ch">\n</span><span class="ss">'</span></span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a>        total_params <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a>        total_mac <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> h <span class="kw">in</span> <span class="va">self</span>.hooks:</span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a>            out <span class="op">+=</span> <span class="ss">f'</span><span class="sc">{</span>h<span class="sc">.</span>layer_name<span class="sc">:&lt;20}</span><span class="ss"> : </span><span class="sc">{</span><span class="bu">str</span>(h.in_shape)<span class="sc">:&lt;20}</span><span class="ss"> : </span><span class="sc">{</span><span class="bu">str</span>(h.out_shape)<span class="sc">:&lt;20}</span><span class="ss"> : </span><span class="sc">{</span>h<span class="sc">.</span>nparams<span class="sc">:&gt;10d}</span><span class="ss"> : </span><span class="sc">{</span>h<span class="sc">.</span>mac<span class="sc">: 10.1f}</span><span class="ch">\n</span><span class="ss">'</span></span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a>            total_params <span class="op">+=</span> h.nparams</span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a>            total_mac <span class="op">+=</span> h.mac</span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="ss">f'</span><span class="sc">{</span><span class="st">"Total parameters:"</span><span class="sc">:&lt;20}{</span>total_params<span class="sc">:&gt;10d}</span><span class="ss"> </span><span class="ch">\n</span><span class="sc">{</span><span class="st">"Total MACs:"</span><span class="sc">:&lt;20}{</span>total_mac<span class="sc">:10.1f}</span><span class="ss"> </span><span class="ch">\n\n</span><span class="ss">'</span> <span class="op">+</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> resnet().<span class="bu">apply</span>(init_weights)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>modules <span class="op">=</span> [module <span class="cf">for</span> module <span class="kw">in</span> model.modules() <span class="cf">if</span> <span class="bu">isinstance</span>(module, (ResBlock, nn.Linear, torch.nn.BatchNorm1d))]</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>monitor <span class="op">=</span> ModelMonitorS(modules)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> Learner(model, dls, F.cross_entropy, torch.optim.Adam, lr, [monitor])</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>l.fit(<span class="dv">1</span>)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>monitor</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Total parameters:      1227900 
Total MACs:                8.4 

layer                : input                : output               :   # params :     # MACs
0_ResBlock           : [1, 28, 28]          : [8, 28, 28]          :        696 :        0.5
1_ResBlock           : [8, 28, 28]          : [16, 14, 14]         :       3664 :        1.4
2_ResBlock           : [16, 14, 14]         : [32, 7, 7]           :      14496 :        1.4
3_ResBlock           : [32, 7, 7]           : [64, 4, 4]           :      57664 :        1.5
4_ResBlock           : [64, 4, 4]           : [128, 2, 2]          :     230016 :        1.8
5_ResBlock           : [128, 2, 2]          : [256, 1, 1]          :     918784 :        1.8
6_Linear             : [256]                : [10]                 :       2560 :        0.0
7_BatchNorm1d        : [10]                 : [10]                 :         20 :        0.0</code></pre>
</div>
</div>
<p>From which we see how data flows through the model, where most parameters are and where most compute is being spend. Although the MACs computation isn‚Äôt perfectly valid, it gives a good indication. It especially shows:</p>
<ul>
<li>most parameters are in later layers, those are the layers that cost most memory. In fact the final ResBlock uses more memory as all earlier ResBlocks combined!</li>
<li>this is also where most compute is being used, although the differences between earlier layers is much less pronounced then for the memory footprint, the reason for this is that the pixel grid is reduced a lot in those later layers</li>
</ul>
</section>
<section id="augmentation" class="level2">
<h2 class="anchored" data-anchor-id="augmentation">Augmentation</h2>
<p>We have finally come to the point of data augmentation. Once you have a good model, you would ideally train for much longer times. When doing so, the problem that arises is that of overfitting. Which we described before as the tendency of the model to start to memorize our training data and no longer generalize well to the validation set.</p>
<p>To overcome this, data augmentation is used to increase the variety in the training data. For example we can rotate the images, or (horizontally or vertically) flip the image. For such augmentations the label stays the same, since a rotated image of a shoe is still a shoe, but there exist also augmentations that for example mix images, and thus also the labels are altereed. There are great libraries out there such as <a href="https://demo.albumentations.ai/">Albumentations</a> with a huge variety of transformations that can be applied to images, so we won‚Äôt be going into the full details. But let‚Äôs at least figure out how we can build this into our framework.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a> <span class="co">#| export</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AugmentS(Subscriber):</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, transform):</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transform <span class="op">=</span> transform</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> before_batch(<span class="va">self</span>, learn):</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> learn.model.training:                    <span class="co"># augmentations are only applied to the training data</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>            learn.batch[<span class="dv">0</span>] <span class="op">=</span> <span class="va">self</span>.transform(learn.batch[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>augs <span class="op">=</span> nn.Sequential(transforms.RandomHorizontalFlip(), </span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>                     transforms.RandomCrop(size<span class="op">=</span><span class="dv">28</span>, padding<span class="op">=</span><span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>set_seed(<span class="dv">1</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> resnet().<span class="bu">apply</span>(init_weights)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>epochs<span class="op">=</span><span class="dv">20</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">1e-2</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>sch <span class="op">=</span> partial(torch.optim.lr_scheduler.OneCycleLR, max_lr<span class="op">=</span>lr, epochs<span class="op">=</span>epochs, steps_per_epoch<span class="op">=</span>N)</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>subs_sch <span class="op">=</span> subs <span class="op">+</span> [SchedulerS(sch), AugmentS(augs)]</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> Learner(model, dls, F.cross_entropy, torch.optim.Adam, lr, subs_sch)</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>l.fit(epochs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">mode</th>
<th data-quarto-table-cell-role="th">loss</th>
<th data-quarto-table-cell-role="th">accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>train</td>
<td>1.052</td>
<td>0.694</td>
</tr>
<tr class="even">
<td>0</td>
<td>eval</td>
<td>0.757</td>
<td>0.778</td>
</tr>
<tr class="odd">
<td>1</td>
<td>train</td>
<td>0.641</td>
<td>0.837</td>
</tr>
<tr class="even">
<td>1</td>
<td>eval</td>
<td>0.591</td>
<td>0.841</td>
</tr>
<tr class="odd">
<td>2</td>
<td>train</td>
<td>0.492</td>
<td>0.869</td>
</tr>
<tr class="even">
<td>2</td>
<td>eval</td>
<td>0.587</td>
<td>0.838</td>
</tr>
<tr class="odd">
<td>3</td>
<td>train</td>
<td>0.401</td>
<td>0.882</td>
</tr>
<tr class="even">
<td>3</td>
<td>eval</td>
<td>0.407</td>
<td>0.871</td>
</tr>
<tr class="odd">
<td>4</td>
<td>train</td>
<td>0.340</td>
<td>0.893</td>
</tr>
<tr class="even">
<td>4</td>
<td>eval</td>
<td>0.378</td>
<td>0.873</td>
</tr>
<tr class="odd">
<td>5</td>
<td>train</td>
<td>0.305</td>
<td>0.898</td>
</tr>
<tr class="even">
<td>5</td>
<td>eval</td>
<td>0.349</td>
<td>0.877</td>
</tr>
<tr class="odd">
<td>6</td>
<td>train</td>
<td>0.283</td>
<td>0.902</td>
</tr>
<tr class="even">
<td>6</td>
<td>eval</td>
<td>0.310</td>
<td>0.896</td>
</tr>
<tr class="odd">
<td>7</td>
<td>train</td>
<td>0.251</td>
<td>0.912</td>
</tr>
<tr class="even">
<td>7</td>
<td>eval</td>
<td>0.335</td>
<td>0.883</td>
</tr>
<tr class="odd">
<td>8</td>
<td>train</td>
<td>0.239</td>
<td>0.915</td>
</tr>
<tr class="even">
<td>8</td>
<td>eval</td>
<td>0.279</td>
<td>0.901</td>
</tr>
<tr class="odd">
<td>9</td>
<td>train</td>
<td>0.219</td>
<td>0.923</td>
</tr>
<tr class="even">
<td>9</td>
<td>eval</td>
<td>0.253</td>
<td>0.913</td>
</tr>
<tr class="odd">
<td>10</td>
<td>train</td>
<td>0.210</td>
<td>0.926</td>
</tr>
<tr class="even">
<td>10</td>
<td>eval</td>
<td>0.248</td>
<td>0.911</td>
</tr>
<tr class="odd">
<td>11</td>
<td>train</td>
<td>0.192</td>
<td>0.932</td>
</tr>
<tr class="even">
<td>11</td>
<td>eval</td>
<td>0.260</td>
<td>0.909</td>
</tr>
<tr class="odd">
<td>12</td>
<td>train</td>
<td>0.179</td>
<td>0.937</td>
</tr>
<tr class="even">
<td>12</td>
<td>eval</td>
<td>0.214</td>
<td>0.926</td>
</tr>
<tr class="odd">
<td>13</td>
<td>train</td>
<td>0.167</td>
<td>0.940</td>
</tr>
<tr class="even">
<td>13</td>
<td>eval</td>
<td>0.214</td>
<td>0.922</td>
</tr>
<tr class="odd">
<td>14</td>
<td>train</td>
<td>0.157</td>
<td>0.945</td>
</tr>
<tr class="even">
<td>14</td>
<td>eval</td>
<td>0.197</td>
<td>0.933</td>
</tr>
<tr class="odd">
<td>15</td>
<td>train</td>
<td>0.142</td>
<td>0.950</td>
</tr>
<tr class="even">
<td>15</td>
<td>eval</td>
<td>0.190</td>
<td>0.935</td>
</tr>
<tr class="odd">
<td>16</td>
<td>train</td>
<td>0.133</td>
<td>0.954</td>
</tr>
<tr class="even">
<td>16</td>
<td>eval</td>
<td>0.187</td>
<td>0.935</td>
</tr>
<tr class="odd">
<td>17</td>
<td>train</td>
<td>0.123</td>
<td>0.958</td>
</tr>
<tr class="even">
<td>17</td>
<td>eval</td>
<td>0.186</td>
<td>0.937</td>
</tr>
<tr class="odd">
<td>18</td>
<td>train</td>
<td>0.120</td>
<td>0.959</td>
</tr>
<tr class="even">
<td>18</td>
<td>eval</td>
<td>0.187</td>
<td>0.938</td>
</tr>
<tr class="odd">
<td>19</td>
<td>train</td>
<td>0.117</td>
<td>0.960</td>
</tr>
<tr class="even">
<td>19</td>
<td>eval</td>
<td>0.185</td>
<td>0.939</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-25-output-3.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="final-remarks" class="level2">
<h2 class="anchored" data-anchor-id="final-remarks">Final Remarks</h2>
<p>And that‚Äôs it. We have created a model that is performing with close to 94% accuracy on the fashion-mnist dataset, according to <a href="https://paperswithcode.com/sota/image-classification-on-fashion-mnist">papers with code</a>, that puts us in the top 10 of papers that are written about this dataset, and we did so with just 20 epochs of training and limitted data augmentation.</p>
<p>In the previous posts in this series, we have build up a small library <a href="https://lucasvw.github.io/nntrain/"><code>nntrain</code></a> in which we have build more or less everything from scratch, from data loading, training loop, activation tracking, initialization, convolutions, optimizers, schedulers and finally ResNets and data augmentation. We understand in detail how it all works, and it‚Äôs fairly straight forward to extend the framework to different kind of Machine Learning challenges.</p>
<p>I can‚Äôt thank Jeremy Howard and the people from FastAI enough for all the wonderful things they are doing for the machine learning community, it truly is spectacular and inspiring ü§ó. To close off, I would like to share a small snippet from the last ‚Äúbase‚Äù lecture in which Jeremy Howard speaks some words about this himself and reinforces the idea that nothing in machine learning is magic, that everything can be understood with common sense and be build from scratch with enough perseverance and tenacity.</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/nlVOG2Nzc3k?si=QAzDjVCkOO5CM5GD?&amp;start=7260&amp;end=7335" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="lucasvw/BlogComments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>