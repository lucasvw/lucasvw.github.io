<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Lucas van Walstijn">
<meta name="dcterms.date" content="2023-09-11">

<title>Lucas van Walstijn - Multilayer Perceptron language model</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../profile.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="../../site_libs/quarto-contrib/iconify-1.0.0-beta.2/iconify-icon.min.js"></script>
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Lucas van Walstijn</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/lucasvw" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/lvWal" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/lucasvanwalstijn/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://www.kaggle.com/lucasvw" rel="" target="">
 <span class="menu-text"><iconify-icon inline="" icon="fa6-brands:kaggle"></iconify-icon></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://lucasvw.github.io/index.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#data" id="toc-data" class="nav-link active" data-scroll-target="#data">Data</a></li>
  <li><a href="#embeddings-relation-to-integer-and-one-hot-encoding" id="toc-embeddings-relation-to-integer-and-one-hot-encoding" class="nav-link" data-scroll-target="#embeddings-relation-to-integer-and-one-hot-encoding">Embeddings: relation to integer and one-hot encoding</a>
  <ul class="collapse">
  <li><a href="#integer-encoding" id="toc-integer-encoding" class="nav-link" data-scroll-target="#integer-encoding">Integer encoding</a></li>
  <li><a href="#one-hot-encoding" id="toc-one-hot-encoding" class="nav-link" data-scroll-target="#one-hot-encoding">One-hot encoding</a></li>
  <li><a href="#embedding-encoding" id="toc-embedding-encoding" class="nav-link" data-scroll-target="#embedding-encoding">Embedding encoding</a></li>
  </ul></li>
  <li><a href="#embeddings-mapping-inputs-into-n-dimensional-space" id="toc-embeddings-mapping-inputs-into-n-dimensional-space" class="nav-link" data-scroll-target="#embeddings-mapping-inputs-into-n-dimensional-space">Embeddings: mapping inputs into n-dimensional space</a></li>
  <li><a href="#embeddings-the-key-to-generalization" id="toc-embeddings-the-key-to-generalization" class="nav-link" data-scroll-target="#embeddings-the-key-to-generalization">Embeddings: the key to generalization</a></li>
  <li><a href="#using-embeddings-effectively" id="toc-using-embeddings-effectively" class="nav-link" data-scroll-target="#using-embeddings-effectively">Using Embeddings effectively</a></li>
  <li><a href="#tanh-activation-function" id="toc-tanh-activation-function" class="nav-link" data-scroll-target="#tanh-activation-function">Tanh activation function</a></li>
  <li><a href="#sampling-names" id="toc-sampling-names" class="nav-link" data-scroll-target="#sampling-names">Sampling names</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Multilayer Perceptron language model</h1>
  <div class="quarto-categories">
    <div class="quarto-category">code</div>
    <div class="quarto-category">neural network</div>
    <div class="quarto-category">nlp</div>
    <div class="quarto-category">embedding</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Lucas van Walstijn </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 11, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>In the previous <a href="https://lucasvw.github.io/posts/13_ngram/">post</a> we had a first look at language models and discussed the n-gram model. Specifically, we:</p>
<ul>
<li>created a count based model</li>
<li>created a neural network based model</li>
<li>established an equivalence between both (hand-wavingly theoretical and emperical)</li>
<li>discussed the curse of dimensionality and the sparse count matrix for n-grams with increasing values for n</li>
</ul>
<p>In this post we are going to have a look at a neural network that is described in Bengio et al: <a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">A Neural Probabilistic Language Model</a> which aims to circumvent this curse of dimensionality by making use of embeddings.</p>
<p>This post is inspired by the <a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">series</a> from Andrej Karpathy’s on neural networks.</p>
<section id="data" class="level2">
<h2 class="anchored" data-anchor-id="data">Data</h2>
<p>Everything starts with training data, for a description see the previous <a href="https://lucasvw.github.io/posts/13_ngram/">post</a></p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> <span class="bu">reduce</span>, partial</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> urllib.request <span class="im">import</span> urlretrieve</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torcheval.metrics <span class="im">as</span> tem</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> fastcore.<span class="bu">all</span> <span class="im">as</span> fc</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nntrain.dataloaders <span class="im">import</span> DataLoaders</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nntrain.learner <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nntrain.activations <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nntrain.acceleration <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nntrain.ngram <span class="im">import</span> <span class="op">*</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">########### Load the data ###########</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> Path(<span class="st">'./data'</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>path.mkdir(parents<span class="op">=</span><span class="va">True</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> path <span class="op">/</span> <span class="st">'names.txt'</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">'https://raw.githubusercontent.com/karpathy/makemore/master/names.txt'</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> urlretrieve(url, path)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(path, <span class="st">'r'</span>) <span class="im">as</span> f:</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    lines <span class="op">=</span> f.read().splitlines()</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">42</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>random.shuffle(lines)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="co">### Create vocabulary and mappings </span><span class="al">###</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>unique_chars <span class="op">=</span> <span class="bu">list</span>(<span class="bu">set</span>(<span class="st">""</span>.join(lines)))</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>unique_chars.sort()</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>vocabulary <span class="op">=</span> [<span class="st">'.'</span>] <span class="op">+</span> unique_chars</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>c2i <span class="op">=</span> {c:i <span class="cf">for</span> i, c <span class="kw">in</span> <span class="bu">enumerate</span>(vocabulary)}</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>i2c <span class="op">=</span> {i:c <span class="cf">for</span> i, c <span class="kw">in</span> <span class="bu">enumerate</span>(vocabulary)}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="embeddings-relation-to-integer-and-one-hot-encoding" class="level2">
<h2 class="anchored" data-anchor-id="embeddings-relation-to-integer-and-one-hot-encoding">Embeddings: relation to integer and one-hot encoding</h2>
<p>In the previous post we introduced embeddings in the following way:</p>
<div class="callout-embedding-layer">
<p>An <strong>embedding layer</strong> is a layer that is used to “encode” our data, which roughly translates to the way we input our data into a neural network. We already saw above that we numericalized our characters (a-z) as integers. We will have to do something similar for our neural network, since we can’t input characters into a neural network. However, instead of feeding integers into our network, we can also employ an embedding layer, which will basically create a vector out of our integers. Each integer value will be mapped to it’s own vector, and the values contained in this vector will be learned during training. The advantage of this, is that the model can easily learn different “attributes” that make up the individual tokens. For example, it <em>could</em> use the first dimension in the vector to denote whether the token is a vowel (a, e, i, o, u) and the second dimension to represent the likelihood of starting a sentence. The emphasis in the last sentence is on <em>could</em>, since these things have to be learned by the network itself during the training-process.</p>
</div>
<section id="integer-encoding" class="level3">
<h3 class="anchored" data-anchor-id="integer-encoding">Integer encoding</h3>
<p>This is all well and good, but let’s add some more detail and intuition. Let’s go back to ultimate basics and see what happens if we feed numericalized tokens (integers) directly into a model using context length of 1, and a first linear layer consisting of 4 neurons (for simplicity we ignore the bias term). Since we have a context length of 1, one single sample consists of one value (the integer encoded value of our token) and is depicted in blue. We have 4 neurons, which weights are also just a single value, depicted in green. The activations of the 4 neurons are depicted in yellow.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mm2.png" class="img-fluid figure-img" width="300"></p>
<figcaption class="figure-caption">Matrix multiplication of one sample consisting of 1 token (blue matrix) with a 4 neuron linear layer (green matrix). On the bottom: the simple multiplication associated with the activation of the first neuron</figcaption>
</figure>
</div>
<p>As you can see, the integer values are just being multiplied by the weights of the linear layer. This establishes an implicit relation between our inputs (blue matrix) in terms of the activation (yellow matrix). Consider a context <em>e</em> (numericalized 4) and another context <em>a</em> (numericalized 1). Because of the linearity, the activations of the former will always be 4 times as large as the activations of the latter. No matter how we change the weights in our network, this implicit relation will always be there, and there is no good reason why this should be the case. Moreover, the factor (4) is a result from the ordering of our vocabulary. If instead we would have <span class="math inline">\(a\)</span> in 10<sup>th</sup> position, and <span class="math inline">\(e\)</span> in 20<sup>th</sup> position, the difference would have been a factor of 2. It should be clear that these kind of implicit linear relations don’t fit to our (<em>nominal</em>) data, and should only be used for <em>ratio</em>, <em>interval</em> and possibly <em>ordinal</em> data.</p>
</section>
<section id="one-hot-encoding" class="level3">
<h3 class="anchored" data-anchor-id="one-hot-encoding">One-hot encoding</h3>
<p>So how then should we treat these variables? If you are familiar with linear regression you have probably heard of <em>one hot encoding</em>. This is a technique in which we replace integer values with a vector. This vector will be of length equal to the amount of possible values the integer can take on and consists of zeros in all but one position, in that nonzero position the value will be <code>1</code>. Each integer will have this <code>1</code> in a different position. For a vocabulary of length 4, the 4 vectors will be:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(F.one_hot(torch.tensor(i), <span class="dv">4</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([1, 0, 0, 0])
tensor([0, 1, 0, 0])
tensor([0, 0, 1, 0])
tensor([0, 0, 0, 1])</code></pre>
</div>
</div>
<p>So we have turned our single integer variable, into 4 distinct (binary) variables. Each of these 4 variables will have it’s own associated weight (parameter) in linear regression and the contributions to the output are thus totally independant of another. To see this, let’s assume we fit a linear function: <span class="math inline">\(y = b \cdot x\)</span> where <span class="math inline">\(x\)</span> is an integer variable taking on 4 values and is transformed into 4 binary variables as shown above <span class="math inline">\(z_1, .. , z_4\)</span>. We then have:</p>
<p><span class="math display">\[\begin{align}
y &amp;= b_1 z_1 + b_2 z_2 + b_3 z_3 + b_4 z_4  \\
y &amp;= \left.
  \begin{cases}
    b_1, &amp; \text{for } x = 1 (z_1 = 1, z_2 = 0, z_3 = 0, z_4 = 0) \\
    b_2, &amp; \text{for } x = 2 (z_1 = 0, z_2 = 1, z_3 = 0, z_4 = 0) \\
    b_3, &amp; \text{for } x = 3 (z_1 = 0, z_2 = 0, z_3 = 1, z_4 = 0) \\
    b_4, &amp; \text{for } x = 4 (z_1 = 0, z_2 = 0, z_3 = 0, z_4 = 1) \\
  \end{cases}
  \right\}
\end{align}\]</span></p>
<p>Since <span class="math inline">\(b_1\)</span> through <span class="math inline">\(b_4\)</span> can take on any value during fitting, we got rid of our implicit relation between our tokens!</p>
</section>
<section id="embedding-encoding" class="level3">
<h3 class="anchored" data-anchor-id="embedding-encoding">Embedding encoding</h3>
<p>For a neural network, something very similar happens. Here an example when we feed these 4 vectors into a linear layer consisting of <strong>2 neurons</strong>:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>integer_value <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>one_hot <span class="op">=</span> F.one_hot(torch.tensor(integer_value), <span class="dv">4</span>).to(torch.float32)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>linear <span class="op">=</span> torch.rand(<span class="dv">4</span>,<span class="dv">2</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span><span class="st">"integer value"</span><span class="sc">:&lt;25}</span><span class="ss">: </span><span class="sc">{</span>integer_value<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span><span class="st">"one hot representation"</span><span class="sc">:&lt;25}</span><span class="ss">: </span><span class="sc">{</span>one_hot<span class="sc">.</span>tolist()<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'weight matrix of linear layer:'</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(linear)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'activations of one hot encoded vector of integer "0" with linear layer:'</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(one_hot<span class="op">@</span>linear)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>integer value            : 0
one hot representation   : [1.0, 0.0, 0.0, 0.0]
weight matrix of linear layer:
tensor([[0.1354, 0.9329],
        [0.0975, 0.1335],
        [0.4640, 0.6912],
        [0.3926, 0.5246]])
activations of one hot encoded vector of integer "0" with linear layer:
tensor([0.1354, 0.9329])</code></pre>
</div>
</div>
<p>So the activations are just the weights associated with the first input to both neurons! If you are familiar with the drawing of neural networks as connected neurons, the activations are equal to the highlighted weights in the drawing below:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./mm3.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">The highlighted weights that get pulled out of the weight matrix for a one-hot representation of integer <code>0</code></figcaption>
</figure>
</div>
<p>We conclude: <strong>multiplying a one-hot encoded vector with a weight matrix, simply returns the weights of the associated row of that weight matrix</strong>.</p>
<p>Mathematically this is thus equivalent to indexing into the weight matrix at the row equal to the integer value:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>linear[integer_value]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([0.1354, 0.9329])</code></pre>
</div>
</div>
<p>If we compare now the inputs (integer values) with the outputs (vector of floats), we have verified the statement we made earlier: <em>However, instead of feeding integers into our network, we can also employ an embedding layer, which will basically create a vector out of our integers</em>.</p>
<p>And now let’s also do this explicitly with an <code>nn.Embedding</code> layer:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>emb <span class="op">=</span> nn.Embedding(<span class="dv">4</span>, <span class="dv">2</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'weight matrix of embedding layer:'</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(emb.weight.data)<span class="op">;</span> <span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'activations of embedding layer of integer "0":'</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(emb(torch.tensor(integer_value)).data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>weight matrix of embedding layer:
tensor([[-0.5065, -0.5940],
        [-0.9815, -0.3719],
        [-1.2932, -1.1999],
        [ 0.3102,  0.2803]])


activations of embedding layer of integer "0":
tensor([-0.5065, -0.5940])</code></pre>
</div>
</div>
<p>Since the parameters of this layer are learned through backpropagation, Bengio et al refer to this as <em>learning a distributed representation for words</em> (in our case we use it for <em>characters</em> instead of <em>words</em>, but the idea is the same). The distributed representation (in the example above a vector of length 2), is what we previously referred to as the different <em>attributes</em> the model can learn that are associated with the tokens.</p>
</section>
</section>
<section id="embeddings-mapping-inputs-into-n-dimensional-space" class="level2">
<h2 class="anchored" data-anchor-id="embeddings-mapping-inputs-into-n-dimensional-space">Embeddings: mapping inputs into n-dimensional space</h2>
<p>These different <em>attributes</em> is also what we refer to when saying that embedding layers <em>map inputs into an n-dimensional space</em>. With n being the amount of attributes, generally smaller then the size of the vocabulary. For example, if we would create an embedding layer with depth of (number of <em>attributes</em>) 4 we would have:</p>
<ul>
<li>integer encoded tokens: 1D (27 possible values)</li>
<li>one-hot encoding: 27D (binary values: either zero or one)</li>
<li>embedding: 4D (real / float valued)</li>
</ul>
<p>And because the weights are learned, the model can place our inputs into this 4D space in positions it finds useful (for learning the task at hand, that is). Inputs that are similar can be placed closely together, and input that are very different can be placed further apart. Moreover, since this space can (and generally is) high dimensional, it can place inputs close together on certain dimensions, but far apart in other dimensions. This allows for a differentiated representation of our inputs, where on some <em>attributes</em> inputs might be similar, but on other <em>attributes</em> very different.</p>
</section>
<section id="embeddings-the-key-to-generalization" class="level2">
<h2 class="anchored" data-anchor-id="embeddings-the-key-to-generalization">Embeddings: the key to generalization</h2>
<p>A final intuition I would like to share, has to do with being “out of distribution”. Let’s say during training you have encountered the sentences (for this example we switch to sentences and words, instead of names and characters):</p>
<ul>
<li>a dog is walking through the kitchen</li>
<li>a dog is walking through the living room</li>
<li>a dog is walking through the garden</li>
</ul>
<p>But never a similar sentence for a cat. If an n-gram (<span class="math inline">\(n &gt;= 6\)</span>) model has to fill in the next word in the sequence: “a cat is walking through the …” it wouldn’t know what to reply, because we are <em>out of distribution</em>: during training we never encountered such a context so it has nothing to go on. The learned probability distribution of an n-gram model is only possible when the model has seen examples of the exact context (possibly with different labels, which are the basis for the <em>probability distribution</em>).</p>
<p>However, for a model using a learned distributed representation, we ideally would like the model to be able to use it’s knowledge of cats being similar to dogs, and thus use the training data it has seen on dogs to be able to answer: <em>kitchen</em>, <em>living room</em> or <em>garden</em>. This kind of generalization becomes possible with embeddings since the model can learn that the vector for cats is similar to the vector for dogs.</p>
<p>This similarity in vector space, will lead to similar outputs. In the words of Bengio et al:</p>
<p><em>“In the proposed model, it will so generalize because “similar” words are expected to have a similar feature vector, and because the probability function is a smooth function of these feature values, a small change in the features will induce a small change in the probability. Therefore, the presence of only one of the above sentences in the training data will increase the probability, not only of that sentence, but also of its combinatorial number of “neighbors” in sentence space (as represented by sequences of feature vectors)“</em>.</p>
</section>
<section id="using-embeddings-effectively" class="level2">
<h2 class="anchored" data-anchor-id="using-embeddings-effectively">Using Embeddings effectively</h2>
<p>Now that we have a better understanding of embeddings both mathematically and conceptually, let’s turn to the paper of Bengio et al.&nbsp;Where they make smart use of embeddings to circumvent the problems with huge weight matrices for n-grams with large n.</p>
<p>The following (simplified) architecture diagram is from that paper and describes pretty well what’s going on.</p>
<p>On the bottom we see three inputs, so we are using a context of length 3. These inputs are feeded into <em>the same</em> embedding layer C. Also, this embedding layer as a depth <em>smaller then the size of the vocabulary</em></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>For the n-gram neural network model we actually also used an embedding layer, but it has a depth equal to the size of the vocabulary. We were thus mapping our integers into a 27D space, you can imagine that with just 27 integers that space was super sparse!</p>
</div>
</div>
<p>For an n-gram model we would have already <span class="math inline">\(27^3\)</span> parameters to accomodate all the different possibilities in the context. However, here we just have <code>vocabulary length x embedding depth</code> parameters. Since all the elements of the context are feed through the same embedding layer, it’s not dependant on the context length, which allows us to increase the context length without having a penalty on the amount of parameters. This implicitly means, <em>that the position of the token is ignored as far as the embedding layer is concerned</em>. That is, a letter <em>e</em> in first or second position will give the same embedding activation.</p>
<p>Next, the three embedding vectors are concatenated and passed through a hidden linear layer with a tanh activation function. This is thus where the <em>positional information</em> is dealt with in this model.</p>
<p>Finally the activations get passed through another linear layer, which maps the activations to the correct amount of classes (i.e.&nbsp;the number of tokens in the vocabulary).</p>
<p><img src="arch.png" class="img-fluid" width="600"></p>
<p>Let’s build this model and train it with <code>nntrain</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Datasets and loaders</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>train_size<span class="op">=</span><span class="fl">0.8</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>val_size<span class="op">=</span><span class="fl">0.1</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>train_lines <span class="op">=</span> lines[<span class="dv">0</span>:<span class="bu">int</span>(train_size <span class="op">*</span> <span class="bu">len</span>(lines))]</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>val_lines <span class="op">=</span> lines[<span class="bu">int</span>(train_size <span class="op">*</span> <span class="bu">len</span>(lines)): <span class="bu">int</span>((train_size <span class="op">+</span> val_size) <span class="op">*</span> <span class="bu">len</span>(lines))]</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>train_ds <span class="op">=</span> NgramDataset(train_lines, c2i, n<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>val_ds <span class="op">=</span> NgramDataset(val_lines, c2i, n<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>bs <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> torch.utils.data.DataLoader(train_ds, shuffle<span class="op">=</span><span class="va">True</span>, batch_size<span class="op">=</span>bs, num_workers<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> torch.utils.data.DataLoader(val_ds, batch_size<span class="op">=</span>bs<span class="op">*</span><span class="dv">2</span>, num_workers<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> DataLoaders(train_loader, val_loader)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As a reminder, let’s have a look at the samples for the first name in the dataset (“Yuheng”)</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>xb, yb <span class="op">=</span> train_ds[<span class="dv">0</span>:<span class="dv">6</span>]</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> x,y <span class="kw">in</span> <span class="bu">zip</span>(xb, yb):</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="bu">list</span>(i2c[i.item()] <span class="cf">for</span> i <span class="kw">in</span> x), <span class="st">'--&gt;'</span> ,i2c[y.item()])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['.', '.', '.'] --&gt; y
['.', '.', 'y'] --&gt; u
['.', 'y', 'u'] --&gt; h
['y', 'u', 'h'] --&gt; e
['u', 'h', 'e'] --&gt; n
['h', 'e', 'n'] --&gt; g</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLP(nn.Module):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, c2i, emb_dim, ctx_len, n_hidden):</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ctx_len <span class="op">=</span> ctx_len</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.emb    <span class="op">=</span> nn.Embedding(<span class="bu">len</span>(c2i), emb_dim)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden <span class="op">=</span> nn.Linear(emb_dim<span class="op">*</span>ctx_len, n_hidden)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tanh   <span class="op">=</span> nn.Tanh()</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out    <span class="op">=</span> nn.Linear(n_hidden, <span class="bu">len</span>(c2i))</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.emb(x)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> out.view(out.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.hidden(out)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.tanh(out)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.out(out)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s find a good learning rate:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>mlp <span class="op">=</span> MLP(c2i, emb_dim<span class="op">=</span><span class="dv">2</span>, ctx_len<span class="op">=</span><span class="dv">3</span>, n_hidden<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>subs <span class="op">=</span> [DeviceS(device)]</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>lrfind <span class="op">=</span> LRFindS()</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> Learner(mlp, dls, F.cross_entropy, torch.optim.Adam, <span class="va">None</span>, subs<span class="op">=</span>subs <span class="op">+</span> [lrfind])</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>l.fit(<span class="dv">5</span>, lr<span class="op">=</span><span class="fl">1e-4</span>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>lrfind.plot()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-11-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Let’s take a learning rate of <code>3e-2</code>, and use the <code>OneCycleLR</code> scheduler together with an <code>Adam</code> optimizer:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="bu">len</span>(train_loader)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">3e-2</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>mlp <span class="op">=</span> MLP(c2i, emb_dim<span class="op">=</span><span class="dv">2</span>, ctx_len<span class="op">=</span><span class="dv">3</span>, n_hidden<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>sch <span class="op">=</span> partial(torch.optim.lr_scheduler.OneCycleLR, max_lr<span class="op">=</span>lr, epochs<span class="op">=</span>epochs, steps_per_epoch<span class="op">=</span>N)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>subs <span class="op">=</span> [ProgressS(<span class="va">True</span>),</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>        MetricsS(),</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>        DeviceS(device)]</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> Learner(mlp, dls, F.cross_entropy, torch.optim.Adam, <span class="va">None</span>, subs<span class="op">=</span>subs <span class="op">+</span> [SchedulerS(sch)])</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>l.fit(epochs, lr<span class="op">=</span>lr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">mode</th>
<th data-quarto-table-cell-role="th">loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>train</td>
<td>2.697</td>
</tr>
<tr class="even">
<td>0</td>
<td>eval</td>
<td>2.444</td>
</tr>
<tr class="odd">
<td>1</td>
<td>train</td>
<td>2.395</td>
</tr>
<tr class="even">
<td>1</td>
<td>eval</td>
<td>2.358</td>
</tr>
<tr class="odd">
<td>2</td>
<td>train</td>
<td>2.336</td>
</tr>
<tr class="even">
<td>2</td>
<td>eval</td>
<td>2.312</td>
</tr>
<tr class="odd">
<td>3</td>
<td>train</td>
<td>2.304</td>
</tr>
<tr class="even">
<td>3</td>
<td>eval</td>
<td>2.288</td>
</tr>
<tr class="odd">
<td>4</td>
<td>train</td>
<td>2.285</td>
</tr>
<tr class="even">
<td>4</td>
<td>eval</td>
<td>2.280</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-12-output-3.png" class="img-fluid"></p>
</div>
</div>
<p>Next, let’s see if we can tweak the hyperparameters to get the best performance. Below I show the result of trying for around 10 minutes to get the best results:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_dls(bs, context_length):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    train_ds <span class="op">=</span> NgramDataset(train_lines, c2i, n<span class="op">=</span>context_length<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    val_ds <span class="op">=</span> NgramDataset(val_lines, c2i, n<span class="op">=</span>context_length<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    train_loader <span class="op">=</span> torch.utils.data.DataLoader(train_ds, shuffle<span class="op">=</span><span class="va">True</span>, batch_size<span class="op">=</span>bs, num_workers<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    val_loader <span class="op">=</span> torch.utils.data.DataLoader(val_ds, batch_size<span class="op">=</span>bs<span class="op">*</span><span class="dv">2</span>, num_workers<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    dls <span class="op">=</span> DataLoaders(train_loader, val_loader)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dls</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train parameters</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>lr     <span class="op">=</span> <span class="fl">3e-2</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>bs     <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameters</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>context_length <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>nh             <span class="op">=</span> <span class="dv">350</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>emb_dim        <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> get_dls(bs, context_length)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>mlp <span class="op">=</span> MLP(c2i, emb_dim, context_length, nh)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> partial(torch.optim.lr_scheduler.OneCycleLR, max_lr<span class="op">=</span>lr, epochs<span class="op">=</span>epochs, steps_per_epoch<span class="op">=</span><span class="bu">len</span>(dls.train))</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> Learner(mlp, dls, F.cross_entropy, torch.optim.Adam, <span class="va">None</span>, subs<span class="op">=</span>subs <span class="op">+</span> [SchedulerS(scheduler)])</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>l.fit(epochs, lr<span class="op">=</span>lr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">mode</th>
<th data-quarto-table-cell-role="th">loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>train</td>
<td>2.405</td>
</tr>
<tr class="even">
<td>0</td>
<td>eval</td>
<td>2.284</td>
</tr>
<tr class="odd">
<td>1</td>
<td>train</td>
<td>2.254</td>
</tr>
<tr class="even">
<td>1</td>
<td>eval</td>
<td>2.238</td>
</tr>
<tr class="odd">
<td>2</td>
<td>train</td>
<td>2.150</td>
</tr>
<tr class="even">
<td>2</td>
<td>eval</td>
<td>2.104</td>
</tr>
<tr class="odd">
<td>3</td>
<td>train</td>
<td>2.011</td>
</tr>
<tr class="even">
<td>3</td>
<td>eval</td>
<td>2.022</td>
</tr>
<tr class="odd">
<td>4</td>
<td>train</td>
<td>1.890</td>
</tr>
<tr class="even">
<td>4</td>
<td>eval</td>
<td>1.990</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-14-output-3.png" class="img-fluid"></p>
</div>
</div>
<p>And this is looking pretty good. The loss we obtained from n-gram models are around 2.139, with this model we are around 2.</p>
</section>
<section id="tanh-activation-function" class="level2">
<h2 class="anchored" data-anchor-id="tanh-activation-function">Tanh activation function</h2>
<p>The MLP model we have used, is employing a tanh activation function which we haven’t encountered so far. So let’s have a quick look at it:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">100</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.tanh(x)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">5</span>))</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>axs.plot(x,y)<span class="op">;</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>axs.grid()<span class="op">;</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>axs.axhline(<span class="dv">0</span>, c<span class="op">=</span><span class="st">'black'</span>)<span class="op">;</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>axs.axvline(<span class="dv">0</span>, c<span class="op">=</span><span class="st">'black'</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-15-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Observe that the tanh activation function has y limits at -1 and 1: however large or small the input, the output is always between -1 and 1. These asymptotes are very flat, meaning that the derivatives in these areas are extremely small. Thinking back at the learning mechanism of neural networks, and remembering that gradients are flowing back through the network by making use of the chain rule: having very small local gradients of a tanh activation function leads to the cancelling-out of whatever gradient has been accumulated so far. This means that the weight updates of all upstream weights is impacted. Previously, this behavior was mentioned in terms of the ReLU activation function, see <a href="https://lucasvw.github.io/posts/11_nntrain_activations/#activations">here</a>.</p>
<p>This becomes a serious problem, if for all our data (in a batch, or even worse in an epoch) this is the case. So let’s check how we are doing by creating a <a href="https://github.com/lucasvw/nntrain/blob/main/nntrain/activations.py#L39"><code>ActivationStatsS</code></a> subclass tailored towards tracking stats for the tanh activation.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TanhActivationS(ActivationStatsS):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> record_stats(<span class="va">self</span>, learn, hook, layer, inp, outp):</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> learn.model.training:</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="kw">not</span> <span class="bu">hasattr</span>(hook, <span class="st">'stats'</span>): hook.stats <span class="op">=</span> ([], [])</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>            acts <span class="op">=</span> outp.detach().cpu()</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>            hook.stats[<span class="dv">0</span>].append(acts.histc(<span class="dv">100</span>,<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))           <span class="co"># get the histogram counts with 100 bins in the range (-1,1)</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>            <span class="co"># computation of the not_firing_rate_per_act</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>            N <span class="op">=</span> acts.shape[<span class="dv">0</span>]                 </span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>            flat <span class="op">=</span> acts.view(N, <span class="op">-</span><span class="dv">1</span>)                              <span class="co"># flatten the activations: matrix of [samples, activations]</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>            nf_rate_p_act <span class="op">=</span> (flat.<span class="bu">abs</span>() <span class="op">&gt;</span> <span class="fl">0.99</span>).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">0</span>) <span class="op">/</span> N   <span class="co"># compute not firing rate per activations (so across the samples)</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>            hook.stats[<span class="dv">1</span>].append(nf_rate_p_act)   </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train parameters</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>lr     <span class="op">=</span> <span class="fl">3e-2</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>bs     <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameters</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>context_length <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>nh             <span class="op">=</span> <span class="dv">350</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>emb_dim        <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> get_dls(bs, context_length)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>mlp <span class="op">=</span> MLP(c2i, emb_dim, context_length, nh)</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> partial(torch.optim.lr_scheduler.OneCycleLR, max_lr<span class="op">=</span>lr, epochs<span class="op">=</span>epochs, steps_per_epoch<span class="op">=</span><span class="bu">len</span>(dls.train))</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>tanhS <span class="op">=</span> TanhActivationS([m <span class="cf">for</span> m <span class="kw">in</span> mlp.modules() <span class="cf">if</span> <span class="bu">isinstance</span>(m, nn.Tanh)])</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> Learner(mlp, dls, F.cross_entropy, torch.optim.Adam, <span class="va">None</span>, subs<span class="op">=</span>subs <span class="op">+</span> [SchedulerS(scheduler)] <span class="op">+</span> [tanhS])</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>l.fit(epochs, lr<span class="op">=</span>lr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">mode</th>
<th data-quarto-table-cell-role="th">loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>train</td>
<td>2.383</td>
</tr>
<tr class="even">
<td>0</td>
<td>eval</td>
<td>2.262</td>
</tr>
<tr class="odd">
<td>1</td>
<td>train</td>
<td>2.257</td>
</tr>
<tr class="even">
<td>1</td>
<td>eval</td>
<td>2.232</td>
</tr>
<tr class="odd">
<td>2</td>
<td>train</td>
<td>2.150</td>
</tr>
<tr class="even">
<td>2</td>
<td>eval</td>
<td>2.126</td>
</tr>
<tr class="odd">
<td>3</td>
<td>train</td>
<td>2.015</td>
</tr>
<tr class="even">
<td>3</td>
<td>eval</td>
<td>2.035</td>
</tr>
<tr class="odd">
<td>4</td>
<td>train</td>
<td>1.902</td>
</tr>
<tr class="even">
<td>4</td>
<td>eval</td>
<td>1.997</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-17-output-3.png" class="img-fluid"></p>
</div>
</div>
<p>And, let’s now plot all histograms of the activations during training as a heatmap:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">2</span>,<span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">15</span>,<span class="dv">10</span>))</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>hist <span class="op">=</span> torch.stack(tanhS.hooks[<span class="dv">0</span>].stats[<span class="dv">0</span>]).T</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].imshow(hist, cmap<span class="op">=</span><span class="st">'Blues'</span>, origin<span class="op">=</span><span class="st">'lower'</span>, aspect<span class="op">=</span><span class="st">'auto'</span>)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_yticks(np.arange(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">100.5</span>, <span class="dv">10</span>), [<span class="bu">round</span>(i, <span class="dv">1</span>) <span class="cf">for</span> i <span class="kw">in</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">11</span>)])<span class="op">;</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].imshow(hist[:,:<span class="dv">20</span>], cmap<span class="op">=</span><span class="st">'Blues'</span>, origin<span class="op">=</span><span class="st">'lower'</span>, aspect<span class="op">=</span><span class="st">'auto'</span>)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_yticks(np.arange(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">100.5</span>, <span class="dv">10</span>), [<span class="bu">round</span>(i, <span class="dv">1</span>) <span class="cf">for</span> i <span class="kw">in</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">11</span>)])<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-18-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>On the top, the histograms are shown for all of training (800+ batches), and on the bottom the histograms for the first 20 batches are shown. We observe that during the beginning of training, we don’t have over-saturated tanh activations, but as training progresses the activations become more and more saturated. We can also look at the dead rate per neuron: which is the amount of activations with absolute value &gt; 0.99 across a batch per neuron:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">15</span>,<span class="dv">5</span>))</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>dead_rate <span class="op">=</span> torch.stack(tanhS.hooks[<span class="dv">0</span>].stats[<span class="dv">1</span>]).T</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>ax.imshow(dead_rate, cmap<span class="op">=</span><span class="st">'Greys'</span>, origin<span class="op">=</span><span class="st">'lower'</span>, aspect<span class="op">=</span><span class="st">'auto'</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-19-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>However, this doesn’t seem to be problematic. Foremost, we need to make sure that we don’t initialize in a way that causes over-saturated tanh activations. And as mentioned, during the beginning of training, this is not the case. Also, when looking at a histogram of the dead rate per neuron across the very last training batch we see that most neurons have a low dead-rate, and we don’t have any neuron for which the dead-rate is higher then 75%:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>plt.hist(dead_rate[:,<span class="op">-</span><span class="dv">1</span>], bins<span class="op">=</span><span class="dv">20</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-20-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="sampling-names" class="level2">
<h2 class="anchored" data-anchor-id="sampling-names">Sampling names</h2>
<p>To conclude, let’s sample from this model:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="at">@fc.patch</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate(<span class="va">self</span>:MLP, n<span class="op">=</span><span class="dv">10</span>, generator<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    names <span class="op">=</span> []</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>        name <span class="op">=</span> <span class="st">'.'</span> <span class="op">*</span> (<span class="va">self</span>.ctx_len)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>            idx <span class="op">=</span> torch.tensor([[c2i[i] <span class="cf">for</span> i <span class="kw">in</span> name[<span class="op">-</span>(<span class="va">self</span>.ctx_len):]]]).cuda()</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> <span class="va">self</span>.forward(idx)</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>            s <span class="op">=</span> torch.multinomial(F.softmax(logits, dim<span class="op">=</span><span class="dv">1</span>), <span class="dv">1</span>, generator<span class="op">=</span>generator)</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>            c <span class="op">=</span> i2c[s.item()]</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>            name <span class="op">+=</span> c</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> c <span class="op">==</span> <span class="st">'.'</span>:</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>                names.append(name)</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> names</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>names <span class="op">=</span> mlp.generate()</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>[name[<span class="dv">15</span>:<span class="op">-</span><span class="dv">1</span>] <span class="cf">for</span> name <span class="kw">in</span> names]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>['xenleigh',
 'samyah',
 'milena',
 'camrihan',
 'nafraya',
 'aris',
 'marcely',
 'zopierah',
 'serg',
 'oshem']</code></pre>
</div>
</div>
<p>And these neural utterings are starting to sound pretty name-like!</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="lucasvw/BlogComments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>