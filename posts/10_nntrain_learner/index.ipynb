{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5a706372-b4f8-4d32-9470-24b837947fa5",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"`nntrain`: Learner (2/n)\"\n",
    "author: \"Lucas van Walstijn\"\n",
    "date: \"2023-08-16\"\n",
    "categories: [code, neural network, deep learning]\n",
    "image: \"image_2.jpg\"\n",
    "comments:\n",
    "  utterances:\n",
    "    repo: lucasvw/BlogComments\n",
    "format:\n",
    "  html:\n",
    "    code-overflow: wrap\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438a6911-402a-4234-9a58-7e1530bcb1bb",
   "metadata": {},
   "source": [
    "In this series, I want to discuss the creation of a small library for training neural networks: `nntrain`. It's based off the excellent [part 2](https://course.fast.ai/) of Practical Deep Learning for Coders by Jeremy Howard, in which from lessons 13 to 18 (roughly) the development of the `miniai` library is discussed.\n",
    "\n",
    "The library will build upon PyTorch. We'll try as much as possible to build from scratch to understand how it all works. Once the main functionality of components are implemented and verified, we can switch over to PyTorch's version. This is similar to how things are done in the course. However, this is not just a \"copy / paste\" of the course: on many occasions I take a different route, and most of the code is my own. That is not to say that all of this is meant to be extremely innovative, instead I had the following goals:\n",
    "\n",
    "- Deeply understand the training of neural networks with a focus on PyTorch\n",
    "- Try to create an even better narrative then what's presented in FastAI üôâü§∑‚Äç‚ôÇÔ∏èüôà\n",
    "- Get hands-on experience with creating a library with [`nb_dev`](https://nbdev.fast.ai/)\n",
    "\n",
    "`nb_dev` is another great project from the fastai community, which allows python libraries to be written in jupyter notebooks. This may sound a bit weird since the mainstream paradigm is to only do experimental work in notebooks. It has the advantage though that we can create the source code for our library in the very same environment in which we want to experiment and interact with our methods, objects and structure **while we are building the library**. For more details on why this is a good idea and other nice features of `nb_dev`, see [here](https://www.fast.ai/posts/2022-07-28-nbdev2.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc98b7c4-9002-407d-90f6-ea9d8c07b8ae",
   "metadata": {},
   "source": [
    "So without further ado, let's start with where we left off in the previous [post](https://lucasvw.github.io/posts/08_nntrain_setup/):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de55ab2-bd58-40fe-8d16-7e9507741c5e",
   "metadata": {},
   "source": [
    "## End of last post:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358815f3-be5d-4afa-b1d3-60f7f5524988",
   "metadata": {},
   "source": [
    "We finished the last post with exporting the `dataloaders` module into the `nntrain` library, which helps transforming a huggingface dataset dictionary into PyTorch dataloaders, so let's use [that](https://lucasvw.github.io/nntrain/dataloaders.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1872102-8494-432b-814c-8af6f423581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,load_dataset_builder\n",
    "\n",
    "from nntrain.dataloaders import DataLoaders, hf_ds_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3532a28f-a116-44be-93bf-be02c7f12b82",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from operator import attrgetter\n",
    "import fastcore.all as fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e89e8e3-da05-4b23-91db-36909044152e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset fashion_mnist (/root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f8dca32da90427483e89bc842d0a4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader>,\n",
       " <torch.utils.data.dataloader.DataLoader>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = \"fashion_mnist\"\n",
    "ds_builder = load_dataset_builder(name)\n",
    "hf_dd = load_dataset(name)\n",
    "\n",
    "bs = 1024\n",
    "dls = DataLoaders.from_hf_dd(hf_dd, batch_size=bs)\n",
    "\n",
    "# As a reminder, `DataLoaders` expose a PyTorch train and validation dataloader as `train` and `valid` attributes:\n",
    "\n",
    "dls.train, dls.valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b72c6f9-c39a-486e-8e07-04a316ab7366",
   "metadata": {},
   "source": [
    "## Learner Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f536dc9f-79fb-4e3d-b5fa-af288fddd28e",
   "metadata": {},
   "source": [
    "Let's continue to formalize our training loop into a `Learner` class with a `fit()` method. So far the training loop looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81c24e6-4c86-4787-828d-26fd10d9d9da",
   "metadata": {},
   "source": [
    "```{.python code-line-numbers='true'}\n",
    "def fit(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()                                       \n",
    "        n_t = train_loss_s = 0                              \n",
    "        for xb, yb in dls.train:\n",
    "            preds = model(xb)\n",
    "            train_loss = loss_func(preds, yb)\n",
    "            train_loss.backward()\n",
    "            \n",
    "            n_t += len(xb)\n",
    "            train_loss_s += train_loss.item() * len(xb)\n",
    "            \n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        model.eval()                                        \n",
    "        n_v = valid_loss_s = acc_s = 0                      \n",
    "        for xb, yb in dls.valid: \n",
    "            with torch.no_grad():                           \n",
    "                preds = model(xb)\n",
    "                valid_loss = loss_func(preds, yb)\n",
    "                \n",
    "                n_v += len(xb)\n",
    "                valid_loss_s += valid_loss.item() * len(xb)\n",
    "                acc_s += accuracy(preds, yb) * len(xb)\n",
    "        \n",
    "        train_loss = train_loss_s / n_t                     \n",
    "        valid_loss = valid_loss_s / n_v\n",
    "        acc = acc_s / n_v\n",
    "        print(f'{epoch=} | {train_loss=:.3f} | {valid_loss=:.3f} | {acc=:.3f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada0541-b9ed-4f83-b4c0-b6da58e1bde3",
   "metadata": {},
   "source": [
    "Let's build this class in steps. Initialization is straigh forward: pass in everything the class needs to have access to. Note that we pass the optimizer class in, and instantiate it during initialization to be able to pass the model parameters and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9061dc72-815a-46b4-ac96-e8a962198987",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, model, dls, loss_fn, metric_fn, optim_class, lr):\n",
    "        self.model = model\n",
    "        self.dls = dls\n",
    "        self.loss_fn = loss_fn\n",
    "        self.metric_fn = metric_fn\n",
    "        self.optim = optim_class(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3e10b8-8839-4d36-9ac9-3b17a6508345",
   "metadata": {},
   "source": [
    "Next, let's define the outer most call: `fit()`. The main improvement here is to call `one_epoch` twice, once for the training and once for the validation. Both passes are fairly similar as can be seen from comparing lines 3-8 with 16-21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459bc051-d2bb-4646-84f6-7abbb589f69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.patch\n",
    "def fit(self:Learner, epochs):\n",
    "    for epoch in range(epochs):                # iterate through the epochs\n",
    "        self.one_epoch(epoch, train=True)      # one epoch through the training dataloader\n",
    "        with torch.no_grad():                  # for the validation epoch we don't need grads\n",
    "            self.one_epoch(epoch, train=False) # one epoch through the validation dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e798c-3fd6-40e0-9acd-21d8cf9cd795",
   "metadata": {},
   "source": [
    "Next, let's implement `one_epoch()`. To keep the functionality nice and small, we factor `do_batch()` out into it's own method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115d5332-5af6-4850-8074-7027dad9c16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.patch\n",
    "def one_epoch(self:Learner, epoch, train):\n",
    "    self.reset_stats()                         # reset the stats at beginning of each epoch\n",
    "    self.model.train(train)                    # put the model either in train or validation mode\n",
    "    self.dl = self.dls.train if train else self.dls.valid # reference to the active dataloader\n",
    "    for self.batch in self.dl:                 # iterate through the active dataloader\n",
    "        self.one_batch(train)                  # do one batch\n",
    "    self.print_stats(epoch, train)             # print stats at the end of the epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657c2fde-b3c7-4b26-98f6-5a01631511f7",
   "metadata": {},
   "source": [
    "And finally the method responsible for dealing with a single batch of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65820af0-8a76-420d-8e1c-eece09760efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.patch\n",
    "def one_batch(self:Learner, train):\n",
    "    self.xb, self.yb = self.batch\n",
    "    self.preds = self.model(self.xb)           # forward pass through the model\n",
    "    self.loss = self.loss_fn(self.preds, self.yb)  # loss\n",
    "    if train:                                  # only do a backward and weight update if train\n",
    "        self.loss.backward()\n",
    "        self.optim.step()\n",
    "        self.optim.zero_grad()\n",
    "    self.update_stats()                        # update stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82106f7-b44a-4fd4-948f-1c2b73618cbd",
   "metadata": {},
   "source": [
    "We also add the methods related to the computation of the statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69c03a2-8b97-4bc3-8a67-36d8aab7309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.patch\n",
    "def update_stats(self:Learner):\n",
    "    n = len(self.xb)\n",
    "    self.loss_s += self.loss.item() * n\n",
    "    self.metric_s += self.metric_fn(self.preds, self.yb).item() * n\n",
    "    self.counter += n\n",
    "\n",
    "@fc.patch\n",
    "def reset_stats(self:Learner):\n",
    "    self.counter = 0\n",
    "    self.loss_s = 0\n",
    "    self.metric_s = 0\n",
    "\n",
    "@fc.patch\n",
    "def print_stats(self:Learner, epoch, train):\n",
    "    loss = self.loss_s / self.counter\n",
    "    metric = self.metric_s / self.counter\n",
    "    print(f'{epoch=:02d} | {\"train\" if train else \"eval\":<5} | {loss=:.3f} | {metric=:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ca1a78-61f8-408e-9506-cccbae000129",
   "metadata": {},
   "source": [
    "And let's do a round of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ec7d3c-118b-45be-bf1a-773196c91499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=00 | train | loss=2.171 | metric=0.230\n",
      "epoch=00 | eval  | loss=2.045 | metric=0.385\n",
      "epoch=01 | train | loss=1.923 | metric=0.534\n",
      "epoch=01 | eval  | loss=1.796 | metric=0.606\n",
      "epoch=02 | train | loss=1.671 | metric=0.624\n",
      "epoch=02 | eval  | loss=1.555 | metric=0.625\n",
      "epoch=03 | train | loss=1.449 | metric=0.640\n",
      "epoch=03 | eval  | loss=1.359 | metric=0.640\n",
      "epoch=04 | train | loss=1.278 | metric=0.653\n",
      "epoch=04 | eval  | loss=1.216 | metric=0.650\n"
     ]
    }
   ],
   "source": [
    "n_in = 28*28\n",
    "n_h = 50\n",
    "n_out = 10\n",
    "lr = 0.01\n",
    "\n",
    "def accuracy(preds, targs):\n",
    "    return (preds.argmax(dim=1) == targs).float().mean()\n",
    "\n",
    "def get_model():\n",
    "    layers = [nn.Linear(n_in, n_h), nn.ReLU(), nn.Linear(n_h, n_out)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "l = Learner(model, dls, F.cross_entropy, accuracy, torch.optim.SGD, lr)\n",
    "\n",
    "l.fit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc5f50e-bf9a-4be7-8486-63832ea817b9",
   "metadata": {},
   "source": [
    "## Callbacks, pubsub and event handlers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc5e950-d535-4f77-9adb-bad0bd5c5cb7",
   "metadata": {},
   "source": [
    "On the one side we want to keep the `Learner` and its training loop generic on the other side we need to be able to tweak the dynamics of the training loop depending on the use-case. One way to customize the training loop, without having to re-write the training loop would be to add a publish/subscribe (pubsub) mechanism. In the FastAI course, they are referred to as \"callbacks\", and although callbacks, event handlers and pubsub are all related. I think the mechanism implemented here is best referred to as pubsub. It can be compared to the way front-end development works. Whenever the user takes an action such as clicking a button, or hovering over a button certain events are **published**. The developer can **subscribe** to these events by adding a function (a **callback** or **event handler**) that get's called whenever they occur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded0e1e2-2029-4de9-af9a-7a626f5a3d60",
   "metadata": {},
   "source": [
    "For the purposes of training neural networks we have the following requirements:\n",
    "\n",
    "- The Learner framework defines a number of \"events\" that are published:\n",
    "  - `before_fit`, `after_fit`\n",
    "  - `before_epoch`, `after_epoch`\n",
    "  - `before_batch`, `after_batch`\n",
    "- Subscribers are classes that implement methods (e.g. `before_fit()`) that will be triggered whenever the associated event is published. They also have an `order` attribute which determines the order in which they are called in case multiple Subscribers subscribed to the same event.\n",
    "- As an additional feature, subscribers will be able to redirect flow, but we will come back to that later\n",
    "\n",
    "So let's implement this. First, we will need to store subscribers in the Learner class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83751c28-b93b-4356-9e07-ff50e1e9b425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, model, dls, loss_fn, metric_fn, optim_class, lr, subs):\n",
    "        self.model = model\n",
    "        self.dls = dls\n",
    "        self.loss_fn = loss_fn\n",
    "        self.metric_fn = metric_fn\n",
    "        self.optim = optim(model.parameters(), lr)\n",
    "        self.subs = subs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c65409-cbad-42b6-9783-11f7b631b01b",
   "metadata": {},
   "source": [
    "Next, let's define a method for publishing events. The method will go through the registered subscribers and if a method with the name of the event is declared, call that method passing the `learner` object as an argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb62e89-e1b4-4611-b343-d460ad830d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.patch\n",
    "def publish(self:Learner, event):\n",
    "    for sub in sorted(self.subs, key=attrgetter('order')):\n",
    "        method = getattr(sub, name, None)\n",
    "        if method is not None: method(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b68148a-56d3-4719-b7c1-204d0a326a9c",
   "metadata": {},
   "source": [
    "To publish the events during the training loop, realize that we have three time the same construct:\n",
    "\n",
    "```\n",
    "publish \"before_event\" event\n",
    "do event\n",
    "publish \"after_event\" event\n",
    "```\n",
    "\n",
    "With `event` being either `fit`, `epoch` or `batch`. So instead of adding this construct multiple times in the training loop let's define a class we can use as a decorater wrapping the actual \"event\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb5006-eeec-4f92-8995-c21e164b1a55",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "\n",
    "class PublishEvents():\n",
    "    def __init__(self, event): \n",
    "        self.event = event\n",
    "    \n",
    "    def __call__(self, decorated_fn):\n",
    "        def decorated_fn_with_publishing(learner, *args, **kwargs):\n",
    "            learner.publish(f'before_{self.event}')\n",
    "            decorated_fn(learner, *args, **kwargs)\n",
    "            learner.publish(f'after_{self.event}')\n",
    "        return decorated_fn_with_publishing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a94c0be-b2e1-478e-b567-5019e5833586",
   "metadata": {},
   "source": [
    "To implement this into the `Learner` we have to factor out the exact code we want to be executed in between the publishing of the `before` and `after`, see the additional `_one_epoch()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad45fdf-a763-4065-856f-f85b020ee7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "class Learner():\n",
    "    def __init__(self, model, dls, loss_fn, metric_fn, optim_class, lr, subs):\n",
    "        self.model = model\n",
    "        self.dls = dls\n",
    "        self.loss_fn = loss_fn\n",
    "        self.metric_fn = metric_fn\n",
    "        self.optim = optim_class(model.parameters(), lr)\n",
    "        self.subs = subs\n",
    "    \n",
    "    @PublishEvents('fit')\n",
    "    def fit(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            self.one_epoch(epoch, train=True)\n",
    "            with torch.no_grad():\n",
    "                self.one_epoch(epoch, train=False)\n",
    "\n",
    "    def one_epoch(self, epoch, train):\n",
    "        self.reset_stats()\n",
    "        self.model.train(train)\n",
    "        self.dl = self.dls.train if train else self.dls.valid\n",
    "        self._one_epoch(epoch, train)\n",
    "        \n",
    "    @PublishEvents('epoch')\n",
    "    def _one_epoch(self, epoch, train):\n",
    "        for self.batch in self.dl:\n",
    "            self.xb, self.yb = self.batch\n",
    "            self.one_batch(train)\n",
    "        self.print_stats(epoch, train)\n",
    "    \n",
    "    @PublishEvents('batch')\n",
    "    def one_batch(self, train):\n",
    "        self.preds = self.model(self.xb)           \n",
    "        self.loss = self.loss_fn(self.preds, self.yb)\n",
    "        if train:                                  \n",
    "            self.loss.backward()\n",
    "            self.optim.step()\n",
    "            self.optim.zero_grad()\n",
    "        self.update_stats()\n",
    "        \n",
    "    def publish(self, event):\n",
    "        for sub in sorted(self.subs, key=attrgetter('order')):\n",
    "            method = getattr(sub, event, None)\n",
    "            if method is not None: method(self)\n",
    "            \n",
    "    def update_stats(self):\n",
    "        n = len(self.xb)\n",
    "        self.loss_s += self.loss.detach().cpu().item() * n\n",
    "        self.metric_s += self.metric_fn(self.preds, self.yb).detach().cpu().item() * n\n",
    "        self.counter += n\n",
    "\n",
    "    def reset_stats(self):\n",
    "        self.counter = 0\n",
    "        self.loss_s = 0\n",
    "        self.metric_s = 0\n",
    "\n",
    "    def print_stats(self, epoch, train):\n",
    "        loss = self.loss_s / self.counter\n",
    "        metric = self.metric_s / self.counter\n",
    "        print(f'{epoch=:02d} | {\"train\" if train else \"eval\":<5} | {loss=:.3f} | {metric=:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9017d5-ca52-4582-b602-eb78e2c48d1a",
   "metadata": {},
   "source": [
    "Let's create a dummy subscriber and test it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d858ee-83c4-4e91-8300-42ecb9d3f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subscriber():\n",
    "    order = 0\n",
    "\n",
    "class DummySub(Subscriber):\n",
    "    \n",
    "    def before_fit(self, learn):\n",
    "        print('before fitüëã')\n",
    "        \n",
    "    def after_fit(self, learn):\n",
    "        print('after fitüëã')\n",
    "        \n",
    "    def before_epoch(self, learn):\n",
    "        print('before epoch üí•')\n",
    "        \n",
    "    def after_epoch(self, learn):\n",
    "        print('after epoch üí•')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579664a1-a60f-4864-94a4-992e93ddf062",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Learner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m l \u001b[38;5;241m=\u001b[39m \u001b[43mLearner\u001b[49m(model, dls, F\u001b[38;5;241m.\u001b[39mcross_entropy, accuracy, torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD, lr, [DummySub()])\n\u001b[1;32m      2\u001b[0m l\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Learner' is not defined"
     ]
    }
   ],
   "source": [
    "l = Learner(model, dls, F.cross_entropy, accuracy, torch.optim.SGD, lr, [DummySub()])\n",
    "l.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c65276",
   "metadata": {},
   "source": [
    "## Subscribers can cancel execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f7433f-8214-4ffc-9a93-84a59c0d5689",
   "metadata": {},
   "source": [
    "Now let's add the last component of our pubsub system: subscribers should be able to cancel processing. For example, a a subscriber that would implement Early Stopping, will have to be able to cancel any further epochs when the validation loss starts increasing. One way to implement this, is with the help of `Exceptions` and `try` / `except` blocks:\n",
    "\n",
    "It's actually very easy to implement this logic, we only need to define custom `Exceptions`, and update the `PublishEvents` class we are using as decorater to catch the exceptions that are thrown in any subscriber:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed18b8e-140a-4d85-9541-0438a298bf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CancelFitException(Exception): pass\n",
    "class CancelEpochException(Exception): pass\n",
    "class CancelBatchException(Exception): pass\n",
    "\n",
    "\n",
    "class PublishEvents():\n",
    "    def __init__(self, name): \n",
    "        self.name = name\n",
    "    \n",
    "    def __call__(self, decorated_fn):\n",
    "        def decorated_fn_with_publishing(learner, *args, **kwargs):\n",
    "            try:\n",
    "                learner.publish(f'before_{self.name}')\n",
    "                decorated_fn(learner, *args, **kwargs)\n",
    "                learner.publish(f'after_{self.name}')\n",
    "            except globals()[f'Cancel{self.name.title()}Exception']: pass\n",
    "        return decorated_fn_with_publishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd31bc76-9424-40e3-9b01-8906e450291c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Subscriber' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDummySub\u001b[39;00m(\u001b[43mSubscriber\u001b[49m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbefore_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, learn): \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore fitüëã\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbefore_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m, learn): \u001b[38;5;28;01mraise\u001b[39;00m CancelFitException\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Subscriber' is not defined"
     ]
    }
   ],
   "source": [
    "class DummySub(Subscriber):\n",
    "    \n",
    "    def before_fit(self, learn): print('before fitüëã')\n",
    "        \n",
    "    def before_epoch(self, learn): raise CancelFitException\n",
    "    \n",
    "    def after_fit(self, learn): print('after fit üëã')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192291d9-5019-42af-87bc-9f699d20e840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before fitüëã\n"
     ]
    }
   ],
   "source": [
    "l = Learner(model, dls, F.cross_entropy, accuracy, torch.optim.SGD, lr, [DummySub()])\n",
    "l.fit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ee4b0a-f322-4f8a-a825-f64308e8fe99",
   "metadata": {},
   "source": [
    "And indeed, the `after_fit` event is never published, since the fit was cancelled during `before_epoch`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30012d32-bae6-4465-97b2-20efb0a8a3aa",
   "metadata": {},
   "source": [
    "## Final version of `Learner`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58b28a6-af2d-4f66-8270-ed845d0fdb06",
   "metadata": {},
   "source": [
    "We are going to make some final changes to the Learner class:\n",
    "\n",
    "- remove all the methods related to metrics and statistics, since we will do that in a Subscriber\n",
    "- factor out the computation of the following logic. This is practical to create subclasses of `Learner` with custom behavior:\n",
    "  - prediction: `self.predict()`\n",
    "  - loss: `self.get_loss()`\n",
    "  - backward pass: `self.backward()`\n",
    "  - stepping of weights: `self.step()`\n",
    "  - zeroing of gradients: `self.zero_grad()`\n",
    "- add a Subscriber argument to `fit`, these subs will only be added for the duration of the fit, and afterwards removed\n",
    "- add a couple of additional events (`after_predict`, `after_loss`, `after_backward` and `after_step`) to which subscribers can listen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0895300f-ea30-4cd4-a3f4-d6042dd17aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "class Learner():\n",
    "    def __init__(self, model, dls, loss_fn, optim_class, lr, subs):\n",
    "        self.model = model\n",
    "        self.dls = dls\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optim_class = optim_class\n",
    "        self.lr = lr\n",
    "        self.subs = subs\n",
    "    \n",
    "    def fit(self, epochs, train=True, valid=True, subs=[], lr=None):\n",
    "        for sub in subs: self.subs.append(sub)\n",
    "        self.n_epochs = epochs\n",
    "        self.epochs = range(self.n_epochs)\n",
    "        lr = self.lr if lr is None else lr\n",
    "        self.opt = self.optim_class(self.model.parameters(), lr)\n",
    "        try:\n",
    "            self._fit(train, valid)\n",
    "        finally:\n",
    "            for sub in subs: self.subs.remove(sub)\n",
    "                    \n",
    "    @PublishEvents('fit')\n",
    "    def _fit(self, train, valid):\n",
    "        for self.epoch in self.epochs:\n",
    "            if train: \n",
    "                self.one_epoch(True)\n",
    "            if valid:\n",
    "                with torch.no_grad():\n",
    "                    self.one_epoch(False)\n",
    "        \n",
    "    def one_epoch(self, train):\n",
    "        self.model.train(train)\n",
    "        self.dl = self.dls.train if train else self.dls.valid\n",
    "        self._one_epoch()\n",
    "        \n",
    "    @PublishEvents('epoch')\n",
    "    def _one_epoch(self):\n",
    "        for self.batch in self.dl: \n",
    "            self.one_batch()\n",
    "    \n",
    "    @PublishEvents('batch')\n",
    "    def one_batch(self):\n",
    "        self.predict()\n",
    "        self.publish('after_predict')\n",
    "        self.get_loss()\n",
    "        self.publish('after_loss')\n",
    "        if self.model.training:\n",
    "            self.backward()\n",
    "            self.publish('after_backward')\n",
    "            self.step()\n",
    "            self.publish('after_step')\n",
    "            self.zero_grad()\n",
    "        \n",
    "    def publish(self, event):\n",
    "        for sub in sorted(self.subs, key=attrgetter('order')):\n",
    "            method = getattr(sub, event, None)\n",
    "            if method is not None: method(self)\n",
    "            \n",
    "    def predict(self): \n",
    "        self.preds = self.model(self.batch[0])\n",
    "        \n",
    "    def get_loss(self): \n",
    "        self.loss = self.loss_fn(self.preds, self.batch[1])\n",
    "        \n",
    "    def backward(self): self.loss.backward()\n",
    "    def step(self): self.opt.step()\n",
    "    def zero_grad(self): self.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac23e86-e522-48e0-8818-24e1c57c296d",
   "metadata": {},
   "source": [
    "## Metrics Subscriber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a6d800-ec5d-4b23-bdee-c65f6d94084a",
   "metadata": {},
   "source": [
    "Since we took out the metrics, let's create a subscriber that adds that. We want the subscriber to be generic, to it should be able to accept one or multiple metrics. Let's make sure that it can accept the metrics from the `torcheval` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e240a3-b785-4fd1-b874-22bd33a2da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    " #|export\n",
    "import torcheval.metrics as tem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65095b58-d78e-4977-bb08-718ecbe305fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No calls to update() have been made - returning 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.5000, dtype=torch.float64)\n",
      "tensor(0., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "metric = tem.Mean()\n",
    "\n",
    "metric.update(torch.tensor([1,2,3]))  # update() adds data\n",
    "metric.update(torch.tensor([4,5,6]))  \n",
    "print(metric.compute())               # compute() computes the metric\n",
    "\n",
    "metric.reset()\n",
    "print(metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abe946d-c885-42c8-980a-eb95dda2e7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    " #|export\n",
    "class MetricsSub(Subscriber):\n",
    "    def __init__(self, **metrics):\n",
    "        self.metrics = metrics\n",
    "        self.loss = tem.Mean()\n",
    "        \n",
    "    def before_fit(self, learn): \n",
    "        learn.metrics = self\n",
    "    \n",
    "    def before_epoch(self, learn):\n",
    "        for m in self.metrics.values(): m.reset()\n",
    "        self.loss.reset()\n",
    "    \n",
    "    def after_batch(self, learn):\n",
    "        x,y,*_ = self.to_cpu(learn.batch)\n",
    "        for m in self.metrics.values(): m.update(self.to_cpu(learn.preds), y)\n",
    "        self.loss.update(self.to_cpu(learn.loss), weight=len(x))\n",
    "        \n",
    "    def after_epoch(self, learn):\n",
    "        log = {\n",
    "            'epoch': learn.epoch,\n",
    "            'mode': 'train' if learn.model.training else 'eval',\n",
    "            'loss' : f'{self.loss.compute():.3f}'\n",
    "        }\n",
    "        for k, v in self.metrics.items():\n",
    "            log[k] = f'{v.compute():.3f}'\n",
    "        self.output(log)\n",
    "        \n",
    "    def to_cpu(self, x):\n",
    "        if isinstance(x, list): return (self.to_cpu(el) for el in x)\n",
    "        return x.detach().cpu()\n",
    "        \n",
    "    def output(self, log): print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6892864-24e0-4509-aaf3-63f20ea82e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MetricsSub(accuracy=tem.MulticlassAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bab58f-f480-4874-9bb4-05d0ae48c320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'mode': 'train', 'loss': '2.180', 'accuracy': '0.270'}\n",
      "{'epoch': 0, 'mode': 'eval', 'loss': '2.056', 'accuracy': '0.457'}\n",
      "{'epoch': 1, 'mode': 'train', 'loss': '1.922', 'accuracy': '0.542'}\n",
      "{'epoch': 1, 'mode': 'eval', 'loss': '1.782', 'accuracy': '0.596'}\n",
      "{'epoch': 2, 'mode': 'train', 'loss': '1.645', 'accuracy': '0.623'}\n",
      "{'epoch': 2, 'mode': 'eval', 'loss': '1.520', 'accuracy': '0.631'}\n",
      "{'epoch': 3, 'mode': 'train', 'loss': '1.412', 'accuracy': '0.642'}\n",
      "{'epoch': 3, 'mode': 'eval', 'loss': '1.324', 'accuracy': '0.637'}\n",
      "{'epoch': 4, 'mode': 'train', 'loss': '1.246', 'accuracy': '0.651'}\n",
      "{'epoch': 4, 'mode': 'eval', 'loss': '1.187', 'accuracy': '0.644'}\n"
     ]
    }
   ],
   "source": [
    "l = Learner(get_model(), dls, F.cross_entropy, torch.optim.SGD, lr, [metrics])\n",
    "l.fit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e12e33f-8d74-4ec7-a9bc-cba35018aca8",
   "metadata": {},
   "source": [
    "## Device Subscriber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df01499-ee64-42e3-bbf0-9f36c8f98ec0",
   "metadata": {},
   "source": [
    "It's time we start training on the GPU, to do that we have to move the model (and it's parameters) as well as all the data onto the GPU. We can easily do this with a `DeviceSub`:\n",
    "\n",
    "- move the model to the device before fit\n",
    "- move each batch to the device before each batch get's processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28deeb63-1eea-4197-9bb7-52f119b63937",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "class DeviceSub(Subscriber):\n",
    "    \n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "    \n",
    "    def before_fit(self, learn):\n",
    "        learn.model.to(self.device)\n",
    "    \n",
    "    def before_batch(self, learn):\n",
    "        learn.batch = [x.to(self.device) for x in learn.batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b0fbe6-d595-498e-b430-c2fbb21db715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'mode': 'train', 'loss': '2.204', 'accuracy': '0.316'}\n",
      "{'epoch': 0, 'mode': 'eval', 'loss': '2.088', 'accuracy': '0.466'}\n",
      "{'epoch': 1, 'mode': 'train', 'loss': '1.960', 'accuracy': '0.555'}\n",
      "{'epoch': 1, 'mode': 'eval', 'loss': '1.824', 'accuracy': '0.601'}\n",
      "{'epoch': 2, 'mode': 'train', 'loss': '1.686', 'accuracy': '0.632'}\n",
      "{'epoch': 2, 'mode': 'eval', 'loss': '1.558', 'accuracy': '0.635'}\n",
      "{'epoch': 3, 'mode': 'train', 'loss': '1.446', 'accuracy': '0.647'}\n",
      "{'epoch': 3, 'mode': 'eval', 'loss': '1.352', 'accuracy': '0.645'}\n",
      "{'epoch': 4, 'mode': 'train', 'loss': '1.269', 'accuracy': '0.654'}\n",
      "{'epoch': 4, 'mode': 'eval', 'loss': '1.207', 'accuracy': '0.651'}\n"
     ]
    }
   ],
   "source": [
    "l = Learner(get_model(), dls, F.cross_entropy, torch.optim.SGD, lr, [metrics, DeviceSub(device)])\n",
    "l.fit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c0cd7-e748-47f1-99b6-1543d65cb985",
   "metadata": {},
   "source": [
    "## MomentumLearner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f293ce-5214-4973-b43c-6221bb7bbaf8",
   "metadata": {},
   "source": [
    "Additionally, we can create a subclass of `Learner` that implements Momentum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd08951-b87d-4537-ad19-036bdac4b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumLearner(Learner):\n",
    "    \n",
    "    def __init__(self, model, dls, loss_fn, optim_class, lr, subs, mom=0.85):\n",
    "        self.mom = mom\n",
    "        super().__init__(model, dls, loss_fn, optim_class, lr, subs)\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.model.parameters(): p.grad *= self.mom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016f9530-a940-46d0-bc84-0299857f583d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'mode': 'train', 'loss': '1.722', 'accuracy': '0.503'}\n",
      "{'epoch': 0, 'mode': 'eval', 'loss': '1.148', 'accuracy': '0.649'}\n",
      "{'epoch': 1, 'mode': 'train', 'loss': '0.953', 'accuracy': '0.674'}\n",
      "{'epoch': 1, 'mode': 'eval', 'loss': '0.849', 'accuracy': '0.678'}\n",
      "{'epoch': 2, 'mode': 'train', 'loss': '0.777', 'accuracy': '0.723'}\n",
      "{'epoch': 2, 'mode': 'eval', 'loss': '0.744', 'accuracy': '0.732'}\n",
      "{'epoch': 3, 'mode': 'train', 'loss': '0.694', 'accuracy': '0.759'}\n",
      "{'epoch': 3, 'mode': 'eval', 'loss': '0.681', 'accuracy': '0.761'}\n",
      "{'epoch': 4, 'mode': 'train', 'loss': '0.638', 'accuracy': '0.783'}\n",
      "{'epoch': 4, 'mode': 'eval', 'loss': '0.639', 'accuracy': '0.778'}\n"
     ]
    }
   ],
   "source": [
    "l = MomentumLearner(get_model(), dls, F.cross_entropy, torch.optim.SGD, lr, [metrics, DeviceSub(device)])\n",
    "l.fit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bc7e6e-3631-4235-8988-ae7582542698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
