{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5a706372-b4f8-4d32-9470-24b837947fa5",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"`nntrain`: Learner (2/n)\"\n",
    "author: \"Lucas van Walstijn\"\n",
    "date: \"2023-08-16\"\n",
    "categories: [code, neural network, deep learning]\n",
    "image: \"image_2.jpg\"\n",
    "comments:\n",
    "  utterances:\n",
    "    repo: lucasvw/BlogComments\n",
    "format:\n",
    "  html:\n",
    "    code-overflow: wrap\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438a6911-402a-4234-9a58-7e1530bcb1bb",
   "metadata": {},
   "source": [
    "In this series, I want to discuss the creation of a small library for training neural networks: `nntrain`. It's based off the excellent [part 2](https://course.fast.ai/) of Practical Deep Learning for Coders by Jeremy Howard, in which from lessons 13 to 18 (roughly) the development of the `miniai` library is discussed.\n",
    "\n",
    "The library will build upon PyTorch. We'll try as much as possible to build from scratch to understand how it all works. Once the main functionality of components are implemented and verified, we can switch over to PyTorch's version. This is similar to how things are done in the course. However, this is not just a \"copy / paste\" of the course: on many occasions I take a different route, and most of the code is my own. That is not to say that all of this is meant to be extremely innovative, instead I had the following goals:\n",
    "\n",
    "- Deeply understand the training of neural networks with a focus on PyTorch\n",
    "- Try to create an even better narrative then what's presented in FastAI üôâü§∑‚Äç‚ôÇÔ∏èüôà\n",
    "- Get hands-on experience with creating a library with [`nb_dev`](https://nbdev.fast.ai/)\n",
    "\n",
    "`nb_dev` is another great project from the fastai community, which allows python libraries to be written in jupyter notebooks. This may sound a bit weird since the mainstream paradigm is to only do experimental work in notebooks. It has the advantage though that we can create the source code for our library in the very same environment in which we want to experiment and interact with our methods, objects and structure **while we are building the library**. For more details on why this is a good idea and other nice features of `nb_dev`, see [here](https://www.fast.ai/posts/2022-07-28-nbdev2.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc98b7c4-9002-407d-90f6-ea9d8c07b8ae",
   "metadata": {},
   "source": [
    "So without further ado, let's start with where we left off in the previous [post](https://lucasvw.github.io/posts/08_nntrain_setup/):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de55ab2-bd58-40fe-8d16-7e9507741c5e",
   "metadata": {},
   "source": [
    "## End of last post:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358815f3-be5d-4afa-b1d3-60f7f5524988",
   "metadata": {},
   "source": [
    "We finished the last post with exporting the `dataloaders` module into the `nntrain` library, which helps transforming a huggingface dataset dictionary into PyTorch dataloaders, so let's use [that](https://lucasvw.github.io/nntrain/dataloaders.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1872102-8494-432b-814c-8af6f423581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,load_dataset_builder\n",
    "\n",
    "from nntrain.dataloaders import DataLoaders, hf_ds_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3532a28f-a116-44be-93bf-be02c7f12b82",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from operator import attrgetter\n",
    "import fastcore.all as fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e89e8e3-da05-4b23-91db-36909044152e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset fashion_mnist (/root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49824842cb7a492abef4040eb91b69c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader>,\n",
       " <torch.utils.data.dataloader.DataLoader>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = \"fashion_mnist\"\n",
    "ds_builder = load_dataset_builder(name)\n",
    "hf_dd = load_dataset(name)\n",
    "\n",
    "bs = 1024\n",
    "dls = DataLoaders.from_hf_dd(hf_dd, batch_size=bs)\n",
    "\n",
    "# As a reminder, `DataLoaders` expose a PyTorch train and validation dataloader as `train` and `valid` attributes:\n",
    "\n",
    "dls.train, dls.valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b72c6f9-c39a-486e-8e07-04a316ab7366",
   "metadata": {},
   "source": [
    "## Learner Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f536dc9f-79fb-4e3d-b5fa-af288fddd28e",
   "metadata": {},
   "source": [
    "Let's continue to formalize our training loop into a `Learner` class with a `fit()` method. So far the training loop looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81c24e6-4c86-4787-828d-26fd10d9d9da",
   "metadata": {},
   "source": [
    "```{.python code-line-numbers='true'}\n",
    "def fit(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()                                       \n",
    "        n_t = train_loss_s = 0                              \n",
    "        for xb, yb in dls.train:\n",
    "            preds = model(xb)\n",
    "            train_loss = loss_func(preds, yb)\n",
    "            train_loss.backward()\n",
    "            \n",
    "            n_t += len(xb)\n",
    "            train_loss_s += train_loss.item() * len(xb)\n",
    "            \n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        model.eval()                                        \n",
    "        n_v = valid_loss_s = acc_s = 0                      \n",
    "        for xb, yb in dls.valid: \n",
    "            with torch.no_grad():                           \n",
    "                preds = model(xb)\n",
    "                valid_loss = loss_func(preds, yb)\n",
    "                \n",
    "                n_v += len(xb)\n",
    "                valid_loss_s += valid_loss.item() * len(xb)\n",
    "                acc_s += accuracy(preds, yb) * len(xb)\n",
    "        \n",
    "        train_loss = train_loss_s / n_t                     \n",
    "        valid_loss = valid_loss_s / n_v\n",
    "        acc = acc_s / n_v\n",
    "        print(f'{epoch=} | {train_loss=:.3f} | {valid_loss=:.3f} | {acc=:.3f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada0541-b9ed-4f83-b4c0-b6da58e1bde3",
   "metadata": {},
   "source": [
    "Let's build this class in steps. Initialization is straigh forward: pass in everything the class needs to have access to. Note that we pass the optimizer class in, and instantiate it during initialization to be able to pass the model parameters and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9061dc72-815a-46b4-ac96-e8a962198987",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, model, dls, loss_fn, metric_fn, optim_class, lr):\n",
    "        self.model = model\n",
    "        self.dls = dls\n",
    "        self.loss_fn = loss_fn\n",
    "        self.metric_fn = metric_fn\n",
    "        self.optim = optim_class(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3e10b8-8839-4d36-9ac9-3b17a6508345",
   "metadata": {},
   "source": [
    "Next, let's define the outer most call: `fit()`. The main improvement here is to call `one_epoch` twice, once for the training and once for the validation. Both passes are fairly similar as can be seen from comparing lines 3-8 with 16-21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459bc051-d2bb-4646-84f6-7abbb589f69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.patch\n",
    "def fit(self:Learner, epochs):\n",
    "    for epoch in range(epochs):                # iterate through the epochs\n",
    "        self.one_epoch(epoch, train=True)      # one epoch through the training dataloader\n",
    "        with torch.no_grad():                  # for the validation epoch we don't need grads\n",
    "            self.one_epoch(epoch, train=False) # one epoch through the validation dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e798c-3fd6-40e0-9acd-21d8cf9cd795",
   "metadata": {},
   "source": [
    "Next, let's implement `one_epoch()`. To keep the functionality nice and small, we factor `do_batch()` out into it's own method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115d5332-5af6-4850-8074-7027dad9c16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.patch\n",
    "def one_epoch(self:Learner, epoch, train):\n",
    "    self.reset_stats()                         # reset the stats at beginning of each epoch\n",
    "    self.model.train(train)                    # put the model either in train or validation mode\n",
    "    self.dl = self.dls.train if train else self.dls.valid # reference to the active dataloader\n",
    "    for self.batch in self.dl:                 # iterate through the active dataloader\n",
    "        self.one_batch(train)                  # do one batch\n",
    "    self.print_stats(epoch, train)             # print stats at the end of the epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657c2fde-b3c7-4b26-98f6-5a01631511f7",
   "metadata": {},
   "source": [
    "And finally the method responsible for dealing with a single batch of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65820af0-8a76-420d-8e1c-eece09760efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.patch\n",
    "def one_batch(self:Learner, train):\n",
    "    self.xb, self.yb = self.batch\n",
    "    self.preds = self.model(self.xb)           # forward pass through the model\n",
    "    self.loss = self.loss_fn(self.preds, self.yb)  # loss\n",
    "    if train:                                  # only do a backward and weight update if train\n",
    "        self.loss.backward()\n",
    "        self.optim.step()\n",
    "        self.optim.zero_grad()\n",
    "    self.update_stats()                        # update stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82106f7-b44a-4fd4-948f-1c2b73618cbd",
   "metadata": {},
   "source": [
    "We also add the methods related to the computation of the statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69c03a2-8b97-4bc3-8a67-36d8aab7309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.patch\n",
    "def update_stats(self:Learner):\n",
    "    n = len(self.xb)\n",
    "    self.loss_s += self.loss.item() * n\n",
    "    self.metric_s += self.metric_fn(self.preds, self.yb).item() * n\n",
    "    self.counter += n\n",
    "\n",
    "@fc.patch\n",
    "def reset_stats(self:Learner):\n",
    "    self.counter = 0\n",
    "    self.loss_s = 0\n",
    "    self.metric_s = 0\n",
    "\n",
    "@fc.patch\n",
    "def print_stats(self:Learner, epoch, train):\n",
    "    loss = self.loss_s / self.counter\n",
    "    metric = self.metric_s / self.counter\n",
    "    print(f'{epoch=:02d} | {\"train\" if train else \"eval\":<5} | {loss=:.3f} | {metric=:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ca1a78-61f8-408e-9506-cccbae000129",
   "metadata": {},
   "source": [
    "And let's do a round of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ec7d3c-118b-45be-bf1a-773196c91499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=00 | train | loss=2.177 | metric=0.218\n",
      "epoch=00 | eval  | loss=2.039 | metric=0.339\n",
      "epoch=01 | train | loss=1.905 | metric=0.493\n",
      "epoch=01 | eval  | loss=1.766 | metric=0.588\n",
      "epoch=02 | train | loss=1.640 | metric=0.628\n",
      "epoch=02 | eval  | loss=1.523 | metric=0.637\n",
      "epoch=03 | train | loss=1.423 | metric=0.651\n",
      "epoch=03 | eval  | loss=1.337 | metric=0.652\n",
      "epoch=04 | train | loss=1.261 | metric=0.660\n",
      "epoch=04 | eval  | loss=1.202 | metric=0.657\n"
     ]
    }
   ],
   "source": [
    "n_in = 28*28\n",
    "n_h = 50\n",
    "n_out = 10\n",
    "lr = 0.01\n",
    "\n",
    "def accuracy(preds, targs):\n",
    "    return (preds.argmax(dim=1) == targs).float().mean()\n",
    "\n",
    "layers = [nn.Linear(n_in, n_h), nn.ReLU(), nn.Linear(n_h, n_out)]\n",
    "model = nn.Sequential(*layers)\n",
    "\n",
    "l = Learner(model, dls, F.cross_entropy, accuracy, torch.optim.SGD, lr)\n",
    "\n",
    "l.fit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09d018d-f1f2-45be-b6f3-a176f078f24c",
   "metadata": {},
   "source": [
    "## Training on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfee6bd-1057-41cf-b06e-1da059b8939a",
   "metadata": {},
   "source": [
    "Now let's see if we can train this on the GPU instead of the CPU. For that we have to move all our tensors to the GPU: notably:\n",
    "\n",
    "- the data tensors in the dataloaders\n",
    "- all parameters of our model, i.e. the weight and bias tensors from each layer\n",
    "\n",
    "Let's first define a variable that will represent whether we can train on the GPU or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fe01db-b5c0-48ab-9aed-42d42be271b8",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1db5d5-05d4-47e1-bf13-edbef168923b",
   "metadata": {},
   "source": [
    "We can put the model (parameters) on the GPU when we instantiate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a2600a-3044-4d6a-9c30-7a796aed0f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(*layers).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3125b545-4d8e-4ce2-a3b5-da71a8e718d7",
   "metadata": {},
   "source": [
    "To put all data tensors on the GPU we update our `one_batch()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d92f870-4a5e-4b51-8c45-fc4ffa8f8f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.patch\n",
    "def one_batch(self:Learner, train):\n",
    "    self.xb, self.yb = map(lambda x: x.to(device), self.batch)  # move the batch to the device\n",
    "    self.preds = self.model(self.xb)           \n",
    "    self.loss = self.loss_fn(self.preds, self.yb)\n",
    "    if train:                                  \n",
    "        self.loss.backward()\n",
    "        self.optim.step()\n",
    "        self.optim.zero_grad()\n",
    "    self.update_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac034304-ac42-4427-a22c-bc6abc225cef",
   "metadata": {},
   "source": [
    "We also have to make sure that when we store the loss and the metric, that we detach it from the computational graph. Otherwise, PyTorch will keep a reference to all the autograd history, which unnecessarily fills GPU memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5db4ab4-75af-4bd2-bab7-3ec6f590f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.patch\n",
    "def update_stats(self:Learner):\n",
    "    n = len(self.xb)\n",
    "    self.loss_s += self.loss.detach().cpu().item() * n\n",
    "    self.metric_s += self.metric_fn(self.preds, self.yb).detach().cpu().item() * n\n",
    "    self.counter += n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc5f50e-bf9a-4be7-8486-63832ea817b9",
   "metadata": {},
   "source": [
    "# Callbacks, pubsub and event handlers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc5e950-d535-4f77-9adb-bad0bd5c5cb7",
   "metadata": {},
   "source": [
    "On the one side we want to keep the `Learner` and its training loop generic on the other side we need to be able to tweak the dynamics of the training loop depending on the use-case. One way to customize the training loop, without having to re-write the training loop would be to add a publish/subscribe (pubsub) mechanism. In the FastAI course, they are referred to as \"callbacks\", and although callbacks, event handlers and pubsub are all related. I think the mechanism implemented here is best referred to as pubsub. It can be compared to the way front-end development works. Whenever the user takes an action such as clicking a button, or hovering over a button certain events are **published**. The developer can **subscribe** to these events by adding a function (a **callback** or **event handler**) that get's called whenever they occur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded0e1e2-2029-4de9-af9a-7a626f5a3d60",
   "metadata": {},
   "source": [
    "For the purposes of training neural networks we have the following requirements:\n",
    "\n",
    "- The Learner framework defines a number of \"events\" that are published:\n",
    "  - `before_fit`, `after_fit`\n",
    "  - `before_epoch`, `after_epoch`\n",
    "  - `before_batch`, `after_batch`\n",
    "- Subscribers are classes that implement methods (e.g. `before_fit()`) that will be triggered whenever the associated event is published. They also have an `order` attribute which determines the order in which they are called in case multiple Subscribers subscribed to the same event.\n",
    "- As an additional feature, subscribers will be able to redirect flow, but we will come back to that later\n",
    "\n",
    "So let's implement this. First, we will need to store subscribers in the Learner class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83751c28-b93b-4356-9e07-ff50e1e9b425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, model, dls, loss_fn, metric_fn, optim_class, lr, subs):\n",
    "        self.model = model\n",
    "        self.dls = dls\n",
    "        self.loss_fn = loss_fn\n",
    "        self.metric_fn = metric_fn\n",
    "        self.optim = optim(model.parameters(), lr)\n",
    "        self.subs = subs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c65409-cbad-42b6-9783-11f7b631b01b",
   "metadata": {},
   "source": [
    "Next, let's define a method for publishing events. The method will go through the registered subscribers and if a method with the name of the event is declared, call that method passing the `learner` object as an argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb62e89-e1b4-4611-b343-d460ad830d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.patch\n",
    "def publish(self:Learner, event):\n",
    "    for sub in sorted(self.subs, key=attrgetter('order')):\n",
    "        method = getattr(sub, name, None)\n",
    "        if method is not None: method(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b68148a-56d3-4719-b7c1-204d0a326a9c",
   "metadata": {},
   "source": [
    "To publish the events during the training loop, realize that we have three time the same construct:\n",
    "\n",
    "```\n",
    "publish \"before_event\" event\n",
    "do event\n",
    "publish \"after_event\" event\n",
    "```\n",
    "\n",
    "With `event` being either `fit`, `epoch` or `batch`. So instead of adding this construct multiple times in the training loop let's define a class we can use as a decorater wrapping the actual \"event\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb5006-eeec-4f92-8995-c21e164b1a55",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "\n",
    "class PublishEvents():\n",
    "    def __init__(self, event): \n",
    "        self.event = event\n",
    "    \n",
    "    def __call__(self, decorated_fn):\n",
    "        def decorated_fn_with_publishing(learner, *args, **kwargs):\n",
    "            learner.publish(f'before_{self.event}')\n",
    "            decorated_fn(learner, *args, **kwargs)\n",
    "            learner.publish(f'after_{self.event}')\n",
    "        return decorated_fn_with_publishing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a94c0be-b2e1-478e-b567-5019e5833586",
   "metadata": {},
   "source": [
    "To implement this into the `Learner` we have to factor out the exact code we want to be executed in between the publishing of the `before` and `after`, see the additional `_one_epoch()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad45fdf-a763-4065-856f-f85b020ee7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "class Learner():\n",
    "    def __init__(self, model, dls, loss_fn, metric_fn, optim_class, lr, subs):\n",
    "        self.model = model\n",
    "        self.dls = dls\n",
    "        self.loss_fn = loss_fn\n",
    "        self.metric_fn = metric_fn\n",
    "        self.optim = optim_class(model.parameters(), lr)\n",
    "        self.subs = subs\n",
    "    \n",
    "    @PublishEvents('fit')\n",
    "    def fit(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            self.one_epoch(epoch, train=True)\n",
    "            with torch.no_grad():\n",
    "                self.one_epoch(epoch, train=False)\n",
    "\n",
    "    def one_epoch(self, epoch, train):\n",
    "        self.reset_stats()\n",
    "        self.model.train(train)\n",
    "        self.dl = self.dls.train if train else self.dls.valid\n",
    "        self._one_epoch(epoch, train)\n",
    "        self.print_stats(epoch, train)\n",
    "        \n",
    "    @PublishEvents('epoch')\n",
    "    def _one_epoch(self, epoch, train):\n",
    "        for self.batch in self.dl:\n",
    "            self.xb, self.yb = map(lambda x: x.to(device), self.batch)\n",
    "            self.one_batch(train)\n",
    "    \n",
    "    @PublishEvents('batch')\n",
    "    def one_batch(self, train):\n",
    "        self.preds = self.model(self.xb)           \n",
    "        self.loss = self.loss_fn(self.preds, self.yb)\n",
    "        if train:                                  \n",
    "            self.loss.backward()\n",
    "            self.optim.step()\n",
    "            self.optim.zero_grad()\n",
    "        self.update_stats()\n",
    "        \n",
    "    def publish(self, event):\n",
    "        for sub in sorted(self.subs, key=attrgetter('order')):\n",
    "            method = getattr(sub, event, None)\n",
    "            if method is not None: method(self)\n",
    "            \n",
    "    def update_stats(self):\n",
    "        n = len(self.xb)\n",
    "        self.loss_s += self.loss.detach().cpu().item() * n\n",
    "        self.metric_s += self.metric_fn(self.preds, self.yb).detach().cpu().item() * n\n",
    "        self.counter += n\n",
    "\n",
    "    def reset_stats(self):\n",
    "        self.counter = 0\n",
    "        self.loss_s = 0\n",
    "        self.metric_s = 0\n",
    "\n",
    "    def print_stats(self, epoch, train):\n",
    "        loss = self.loss_s / self.counter\n",
    "        metric = self.metric_s / self.counter\n",
    "        print(f'{epoch=:02d} | {\"train\" if train else \"eval\":<5} | {loss=:.3f} | {metric=:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9017d5-ca52-4582-b602-eb78e2c48d1a",
   "metadata": {},
   "source": [
    "Let's create a dummy subscriber and test it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d858ee-83c4-4e91-8300-42ecb9d3f8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before fitüëã\n",
      "before epoch üí•\n",
      "after epoch üí•\n",
      "epoch=00 | train | loss=1.146 | metric=0.664\n",
      "before epoch üí•\n",
      "after epoch üí•\n",
      "epoch=00 | eval  | loss=1.106 | metric=0.657\n",
      "after fitüëã\n"
     ]
    }
   ],
   "source": [
    "class Subscriber():\n",
    "    order = 0\n",
    "\n",
    "class DummySub(Subscriber):\n",
    "    \n",
    "    def before_fit(self, learn):\n",
    "        print('before fitüëã')\n",
    "        \n",
    "    def after_fit(self, learn):\n",
    "        print('after fitüëã')\n",
    "        \n",
    "    def before_epoch(self, learn):\n",
    "        print('before epoch üí•')\n",
    "        \n",
    "    def after_epoch(self, learn):\n",
    "        print('after epoch üí•')\n",
    "\n",
    "l = Learner(model, dls, F.cross_entropy, accuracy, torch.optim.SGD, lr, [DummySub()])\n",
    "l.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f7433f-8214-4ffc-9a93-84a59c0d5689",
   "metadata": {},
   "source": [
    "Nice! Now let's add the last component of our pubsub system: subscribers should be able to cancel processing of certain events. For example, a a subscriber that would implement Early Stopping, will have to be able to cancel any further epochs when the validation loss starts increasing. One way to implement this, is with the help of `Exceptions` and `try` / `except` blocks:\n",
    "\n",
    "It's actually very easy to implement this logic, we only need to define custom `Exceptions`, and update the `PublishEvents` class we are using as decorater to catch the exceptions that are thrown in any subscriber:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed18b8e-140a-4d85-9541-0438a298bf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CancelFitException(Exception): pass\n",
    "class CancelEpochException(Exception): pass\n",
    "class CancelBatchException(Exception): pass\n",
    "\n",
    "\n",
    "class PublishEvents():\n",
    "    def __init__(self, name): \n",
    "        self.name = name\n",
    "    \n",
    "    def __call__(self, decorated_fn):\n",
    "        def decorated_fn_with_publishing(learner, *args, **kwargs):\n",
    "            try:\n",
    "                learner.publish(f'before_{self.name}')\n",
    "                decorated_fn(learner, *args, **kwargs)\n",
    "                learner.publish(f'after_{self.name}')\n",
    "            except globals()[f'Cancel{self.name.title()}Exception']: pass\n",
    "        return decorated_fn_with_publishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd31bc76-9424-40e3-9b01-8906e450291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummySub(Subscriber):\n",
    "    \n",
    "    def before_fit(self, learn): print('before fitüëã')\n",
    "        \n",
    "    def before_epoch(self, learn): raise CancelFitException\n",
    "    \n",
    "    def after_fit(self, learn): print('after fit üëã')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574948a4-7948-4f2c-b89f-bf6c0d673c84",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "    \n",
    "class Subscriber():\n",
    "    order = 0\n",
    "\n",
    "class Learner():\n",
    "    def __init__(self, model, dls, loss_fn, metric_fn, optim_class, lr, subs):\n",
    "        self.model = model\n",
    "        self.dls = dls\n",
    "        self.loss_fn = loss_fn\n",
    "        self.metric_fn = metric_fn\n",
    "        self.optim = optim_class(model.parameters(), lr)\n",
    "        self.subs = subs\n",
    "    \n",
    "    @PublishEvents('fit')\n",
    "    def fit(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            self.one_epoch(epoch, train=True)\n",
    "            with torch.no_grad():\n",
    "                self.one_epoch(epoch, train=False)\n",
    "\n",
    "    def one_epoch(self, epoch, train):\n",
    "        self.reset_stats()\n",
    "        self.model.train(train)\n",
    "        self.dl = self.dls.train if train else self.dls.valid\n",
    "        self._one_epoch(epoch, train)\n",
    "        \n",
    "    @PublishEvents('epoch')\n",
    "    def _one_epoch(self, epoch, train):\n",
    "        for self.batch in self.dl:\n",
    "            self.xb, self.yb = map(lambda x: x.to(device), self.batch)\n",
    "            self.one_batch(train)\n",
    "        self.print_stats(epoch, train)\n",
    "    \n",
    "    @PublishEvents('batch')\n",
    "    def one_batch(self, train):\n",
    "        self.preds = self.model(self.xb)           \n",
    "        self.loss = self.loss_fn(self.preds, self.yb)\n",
    "        if train:                                  \n",
    "            self.loss.backward()\n",
    "            self.optim.step()\n",
    "            self.optim.zero_grad()\n",
    "        self.update_stats()\n",
    "        \n",
    "    def publish(self, event):\n",
    "        for sub in sorted(self.subs, key=attrgetter('order')):\n",
    "            method = getattr(sub, event, None)\n",
    "            if method is not None: method(self)\n",
    "            \n",
    "    def update_stats(self):\n",
    "        n = len(self.xb)\n",
    "        self.loss_s += self.loss.detach().cpu().item() * n\n",
    "        self.metric_s += self.metric_fn(self.preds, self.yb).detach().cpu().item() * n\n",
    "        self.counter += n\n",
    "\n",
    "    def reset_stats(self):\n",
    "        self.counter = 0\n",
    "        self.loss_s = 0\n",
    "        self.metric_s = 0\n",
    "\n",
    "    def print_stats(self, epoch, train):\n",
    "        loss = self.loss_s / self.counter\n",
    "        metric = self.metric_s / self.counter\n",
    "        print(f'{epoch=:02d} | {\"train\" if train else \"eval\":<5} | {loss=:.3f} | {metric=:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192291d9-5019-42af-87bc-9f699d20e840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before fitüëã\n"
     ]
    }
   ],
   "source": [
    "l = Learner(model, dls, F.cross_entropy, accuracy, torch.optim.SGD, lr, [DummySub()])\n",
    "l.fit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ee4b0a-f322-4f8a-a825-f64308e8fe99",
   "metadata": {},
   "source": [
    "And indeed, the after_fit event is never published, since the fit was cancelled during `before_epoch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5d9e35-78d8-4b2f-9349-480b07211b39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
