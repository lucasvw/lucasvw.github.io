{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5a706372-b4f8-4d32-9470-24b837947fa5",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"`nntrain`: Learner (2/n)\"\n",
    "author: \"Lucas van Walstijn\"\n",
    "date: \"2023-08-16\"\n",
    "categories: [code, neural network, deep learning]\n",
    "image: \"image_2.jpg\"\n",
    "comments:\n",
    "  utterances:\n",
    "    repo: lucasvw/BlogComments\n",
    "format:\n",
    "  html:\n",
    "    code-overflow: wrap\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438a6911-402a-4234-9a58-7e1530bcb1bb",
   "metadata": {},
   "source": [
    "In this series, I want to discuss the creation of a small library for training neural networks: `nntrain`. It's based off the excellent [part 2](https://course.fast.ai/) of Practical Deep Learning for Coders by Jeremy Howard, in which from lessons 13 to 18 (roughly) the development of the `miniai` library is discussed.\n",
    "\n",
    "The library will build upon PyTorch. We'll try as much as possible to build from scratch to understand how it all works. Once the main functionality of components are implemented and verified, we can switch over to PyTorch's version. This is similar to how things are done in the course. However, this is not just a \"copy / paste\" of the course: on many occasions I take a different route, and most of the code is my own. That is not to say that all of this is meant to be extremely innovative, instead I had the following goals:\n",
    "\n",
    "- Deeply understand the training of neural networks with a focus on PyTorch\n",
    "- Try to create an even better narrative then what's presented in FastAI üôâü§∑‚Äç‚ôÇÔ∏èüôà\n",
    "- Get hands-on experience with creating a library with [`nb_dev`](https://nbdev.fast.ai/)\n",
    "\n",
    "`nb_dev` is another great project from the fastai community, which allows python libraries to be written in jupyter notebooks. This may sound a bit weird since the mainstream paradigm is to only do experimental work in notebooks. It has the advantage though that we can create the source code for our library in the very same environment in which we want to experiment and interact with our methods, objects and structure **while we are building the library**. For more details on why this is a good idea and other nice features of `nb_dev`, see [here](https://www.fast.ai/posts/2022-07-28-nbdev2.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc98b7c4-9002-407d-90f6-ea9d8c07b8ae",
   "metadata": {},
   "source": [
    "So without further ado, let's start with where we left off in the previous [post](https://lucasvw.github.io/posts/08_nntrain_setup/):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de55ab2-bd58-40fe-8d16-7e9507741c5e",
   "metadata": {},
   "source": [
    "## End of last post:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358815f3-be5d-4afa-b1d3-60f7f5524988",
   "metadata": {},
   "source": [
    "We finished the last post with exporting the `dataloaders` module into the `nntrain` library, which helps transforming a huggingface dataset dictionary into PyTorch dataloaders, so let's use [that](https://lucasvw.github.io/nntrain/dataloaders.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1872102-8494-432b-814c-8af6f423581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,load_dataset_builder\n",
    "\n",
    "from nntrain.dataloaders import DataLoaders, hf_ds_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3532a28f-a116-44be-93bf-be02c7f12b82",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from operator import attrgetter\n",
    "import fastcore.all as fc\n",
    "import math\n",
    "from fastprogress import progress_bar,master_bar\n",
    "import torcheval.metrics as tem\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e89e8e3-da05-4b23-91db-36909044152e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset fashion_mnist (/root/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0892e52b68f54dc7b71e21cddf536fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader>,\n",
       " <torch.utils.data.dataloader.DataLoader>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = \"fashion_mnist\"\n",
    "ds_builder = load_dataset_builder(name)\n",
    "hf_dd = load_dataset(name)\n",
    "\n",
    "bs = 1024\n",
    "dls = DataLoaders.from_hf_dd(hf_dd, batch_size=bs)\n",
    "\n",
    "# As a reminder, `DataLoaders` expose a PyTorch train and validation dataloader as `train` and `valid` attributes:\n",
    "\n",
    "dls.train, dls.valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b72c6f9-c39a-486e-8e07-04a316ab7366",
   "metadata": {},
   "source": [
    "## Learner Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f536dc9f-79fb-4e3d-b5fa-af288fddd28e",
   "metadata": {},
   "source": [
    "Let's continue to formalize our training loop into a `Learner` class with a `fit()` method. The training loop created so far looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81c24e6-4c86-4787-828d-26fd10d9d9da",
   "metadata": {},
   "source": [
    "```{.python code-line-numbers='true'}\n",
    "def fit(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()                                       \n",
    "        n_t = train_loss_s = 0                              \n",
    "        for xb, yb in dls.train:\n",
    "            preds = model(xb)\n",
    "            train_loss = loss_func(preds, yb)\n",
    "            train_loss.backward()\n",
    "            \n",
    "            n_t += len(xb)\n",
    "            train_loss_s += train_loss.item() * len(xb)\n",
    "            \n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        model.eval()                                        \n",
    "        n_v = valid_loss_s = acc_s = 0                      \n",
    "        for xb, yb in dls.valid: \n",
    "            with torch.no_grad():                           \n",
    "                preds = model(xb)\n",
    "                valid_loss = loss_func(preds, yb)\n",
    "                \n",
    "                n_v += len(xb)\n",
    "                valid_loss_s += valid_loss.item() * len(xb)\n",
    "                acc_s += accuracy(preds, yb) * len(xb)\n",
    "        \n",
    "        train_loss = train_loss_s / n_t                     \n",
    "        valid_loss = valid_loss_s / n_v\n",
    "        acc = acc_s / n_v\n",
    "        print(f'{epoch=} | {train_loss=:.3f} | {valid_loss=:.3f} | {acc=:.3f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada0541-b9ed-4f83-b4c0-b6da58e1bde3",
   "metadata": {},
   "source": [
    "Let's build this class in steps. Initialization is straight forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9061dc72-815a-46b4-ac96-e8a962198987",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, model, dls, loss_fn, metric_fn, optim_class, lr):\n",
    "        self.model = model\n",
    "        self.dls = dls\n",
    "        self.loss_fn = loss_fn\n",
    "        self.metric_fn = metric_fn\n",
    "        self.optim = optim_class(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3e10b8-8839-4d36-9ac9-3b17a6508345",
   "metadata": {},
   "source": [
    "Next, let's implement the training loop. Instead of writing the full loop in a single `fit()` method, let's try to break the training loop down into pieces:\n",
    "\n",
    "- `fit()` iterates through the epochs\n",
    "- Per epoch we do a training round through the training data and an evaluation round through the validation set. Both rounds are quite similar, so let's put this functionality in a separate method `one_epoch()`\n",
    "- In each epoch we iterate through the batches of a dataloader, let's put this functionality in a method `one_batch()`\n",
    "\n",
    "let's define the outer most call: `fit()`. In this method we'll call `one_epoch` twice, once for the training and once for the validation. Both passes are fairly similar as can be seen from comparing lines 3-8 with 16-21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459bc051-d2bb-4646-84f6-7abbb589f69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.patch\n",
    "def fit(self:Learner, epochs):\n",
    "    for epoch in range(epochs):                # iterate through the epochs\n",
    "        self.one_epoch(epoch, train=True)      # one epoch through the training dataloader\n",
    "        with torch.no_grad():                  # for the validation epoch we don't need grads\n",
    "            self.one_epoch(epoch, train=False) # one epoch through the validation dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e798c-3fd6-40e0-9acd-21d8cf9cd795",
   "metadata": {},
   "source": [
    "Next, let's implement `one_epoch()`. To make sure each method does one thing, we factor `do_batch()` out into it's own method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115d5332-5af6-4850-8074-7027dad9c16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.patch\n",
    "def one_epoch(self:Learner, epoch, train):\n",
    "    self.reset_stats()                         # reset the stats at beginning of each epoch\n",
    "    self.model.train(train)                    # put the model either in train or validation mode\n",
    "    self.dl = self.dls.train if train else self.dls.valid # reference to the active dataloader\n",
    "    for self.batch in self.dl:                 # iterate through the active dataloader\n",
    "        self.one_batch(train)                  # do one batch\n",
    "    self.print_stats(epoch, train)             # print stats at the end of the epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657c2fde-b3c7-4b26-98f6-5a01631511f7",
   "metadata": {},
   "source": [
    "And finally the method responsible for dealing with a single batch of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65820af0-8a76-420d-8e1c-eece09760efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.patch\n",
    "def one_batch(self:Learner, train):\n",
    "    self.xb, self.yb = self.batch              # self.batch is either a training or validation batch\n",
    "    self.preds = self.model(self.xb)           # forward pass through the model\n",
    "    self.loss = self.loss_fn(self.preds, self.yb)  # loss\n",
    "    if train:                                  # only do a backward and weight update if train\n",
    "        self.loss.backward()\n",
    "        self.optim.step()\n",
    "        self.optim.zero_grad()\n",
    "    self.update_stats()                        # update stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82106f7-b44a-4fd4-948f-1c2b73618cbd",
   "metadata": {},
   "source": [
    "And the methods related to the computation of the statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69c03a2-8b97-4bc3-8a67-36d8aab7309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.patch\n",
    "def update_stats(self:Learner):\n",
    "    n = len(self.xb)\n",
    "    self.loss_s += self.loss.item() * n\n",
    "    self.metric_s += self.metric_fn(self.preds, self.yb).item() * n\n",
    "    self.counter += n\n",
    "\n",
    "@fc.patch\n",
    "def reset_stats(self:Learner):\n",
    "    self.counter = 0\n",
    "    self.loss_s = 0\n",
    "    self.metric_s = 0\n",
    "\n",
    "@fc.patch\n",
    "def print_stats(self:Learner, epoch, train):\n",
    "    loss = self.loss_s / self.counter\n",
    "    metric = self.metric_s / self.counter\n",
    "    print(f'{epoch=:02d} | {\"train\" if train else \"eval\":<5} | {loss=:.3f} | {metric=:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ca1a78-61f8-408e-9506-cccbae000129",
   "metadata": {},
   "source": [
    "Let's try it out on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ec7d3c-118b-45be-bf1a-773196c91499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=00 | train | loss=2.202 | metric=0.240\n",
      "epoch=00 | eval  | loss=2.078 | metric=0.378\n",
      "epoch=01 | train | loss=1.943 | metric=0.464\n",
      "epoch=01 | eval  | loss=1.802 | metric=0.538\n",
      "epoch=02 | train | loss=1.667 | metric=0.578\n",
      "epoch=02 | eval  | loss=1.543 | metric=0.601\n",
      "epoch=03 | train | loss=1.435 | metric=0.628\n",
      "epoch=03 | eval  | loss=1.346 | metric=0.639\n",
      "epoch=04 | train | loss=1.266 | metric=0.652\n",
      "epoch=04 | eval  | loss=1.207 | metric=0.651\n"
     ]
    }
   ],
   "source": [
    "n_in = 28*28\n",
    "n_h = 50\n",
    "n_out = 10\n",
    "lr = 0.01\n",
    "\n",
    "def accuracy(preds, targs):\n",
    "    return (preds.argmax(dim=1) == targs).float().mean()\n",
    "\n",
    "def get_model():\n",
    "    layers = [nn.Linear(n_in, n_h), nn.ReLU(), nn.Linear(n_h, n_out)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "l = Learner(get_model(), dls, F.cross_entropy, accuracy, torch.optim.SGD, lr)\n",
    "\n",
    "l.fit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc5f50e-bf9a-4be7-8486-63832ea817b9",
   "metadata": {},
   "source": [
    "## Callbacks, pubsub and event handlers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc5e950-d535-4f77-9adb-bad0bd5c5cb7",
   "metadata": {},
   "source": [
    "On the one side we want to keep the `Learner` and its training loop generic on the other side we need to be able to tweak the dynamics of the training loop depending on the use-case. One way to customize the training loop, without having to re-write the training loop would be to add a publish/subscribe (pubsub) mechanism. In the FastAI course, they are referred to as \"callbacks\", and although callbacks, event handlers and pubsub are all related and basically refer to any logic (encapsulated in a function) which we want to specify *now*, and execute at a later point in time whenever some condition arises. In my vocabulary a callback is a function that is passed to another function, and is executed whenever that other function is finished.\n",
    "\n",
    "For the purposes of training neural networks we have the following requirements:\n",
    "\n",
    "- The Learner framework defines a number of \"events\" that are published:\n",
    "  - `before_fit`, `after_fit`\n",
    "  - `before_epoch`, `after_epoch`\n",
    "  - `before_batch`, `after_batch`\n",
    "- Subscribers are classes that implement methods (e.g. `before_fit()`) that will be triggered whenever the associated event is published. They also have an `order` attribute which determines the order in which they are called in case multiple Subscribers subscribed to the same event.\n",
    "- As an additional feature, subscribers will be able to redirect flow, but we will come back to that later\n",
    "\n",
    "So let's implement this. First, we will need to store a list of subscribers in the Learner class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83751c28-b93b-4356-9e07-ff50e1e9b425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, model, dls, loss_fn, metric_fn, optim_class, lr, subs):\n",
    "        self.model = model\n",
    "        self.dls = dls\n",
    "        self.loss_fn = loss_fn\n",
    "        self.metric_fn = metric_fn\n",
    "        self.optim = optim(model.parameters(), lr)\n",
    "        self.subs = subs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c65409-cbad-42b6-9783-11f7b631b01b",
   "metadata": {},
   "source": [
    "Next, let's define a method for publishing events. The method will go through the registered subscribers and if a method with the name of the event is declared, call that method passing the `learner` object as an argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb62e89-e1b4-4611-b343-d460ad830d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.patch\n",
    "def publish(self:Learner, event):\n",
    "    for sub in sorted(self.subs, key=attrgetter('order')):\n",
    "        method = getattr(sub, name, None)\n",
    "        if method is not None: method(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b68148a-56d3-4719-b7c1-204d0a326a9c",
   "metadata": {},
   "source": [
    "With the `before_x` / `after_x` events, realize that we have three times the same construct:\n",
    "\n",
    "```\n",
    "publish \"before_event\" event\n",
    "do event\n",
    "publish \"after_event\" event\n",
    "```\n",
    "\n",
    "With `event` being either `fit`, `epoch` or `batch`. So instead of adding this construct multiple times in the training loop let's define a class we can use as a decorater wrapping the `do_event` logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb5006-eeec-4f92-8995-c21e164b1a55",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "\n",
    "class PublishEvents():\n",
    "    def __init__(self, event): \n",
    "        self.event = event\n",
    "    \n",
    "    def __call__(self, decorated_fn):\n",
    "        def decorated_fn_with_publishing(learner, *args, **kwargs):\n",
    "            learner.publish(f'before_{self.event}')\n",
    "            decorated_fn(learner, *args, **kwargs)\n",
    "            learner.publish(f'after_{self.event}')\n",
    "        return decorated_fn_with_publishing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a94c0be-b2e1-478e-b567-5019e5833586",
   "metadata": {},
   "source": [
    "To implement this into the `Learner` we have to factor out the exact code we want to be executed in between the publishing of the `before` and `after`, see the additional `_one_epoch()` method.\n",
    "\n",
    "Note that we are taking out the logic concerning the statistics, this will be implemented as a Subscriber as we'll see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad45fdf-a763-4065-856f-f85b020ee7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, model, dls, loss_fn, metric_fn, optim_class, lr, subs):\n",
    "        self.model = model\n",
    "        self.dls = dls\n",
    "        self.loss_fn = loss_fn\n",
    "        self.metric_fn = metric_fn\n",
    "        self.optim = optim_class(model.parameters(), lr)\n",
    "        self.subs = subs\n",
    "    \n",
    "    @PublishEvents('fit')\n",
    "    def fit(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            self.one_epoch(epoch, train=True)\n",
    "            with torch.no_grad():\n",
    "                self.one_epoch(epoch, train=False)\n",
    "\n",
    "    def one_epoch(self, epoch, train):\n",
    "        self.model.train(train)\n",
    "        self.dl = self.dls.train if train else self.dls.valid\n",
    "        self._one_epoch(epoch, train)\n",
    "        \n",
    "    @PublishEvents('epoch')\n",
    "    def _one_epoch(self, epoch, train):\n",
    "        for self.batch in self.dl:\n",
    "            self.xb, self.yb = self.batch\n",
    "            self.one_batch(train)\n",
    "    \n",
    "    @PublishEvents('batch')\n",
    "    def one_batch(self, train):\n",
    "        self.preds = self.model(self.xb)           \n",
    "        self.loss = self.loss_fn(self.preds, self.yb)\n",
    "        if train:                                  \n",
    "            self.loss.backward()\n",
    "            self.optim.step()\n",
    "            self.optim.zero_grad()\n",
    "        \n",
    "    def publish(self, event):\n",
    "        for sub in sorted(self.subs, key=attrgetter('order')):\n",
    "            method = getattr(sub, event, None)\n",
    "            if method is not None: method(self)            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9017d5-ca52-4582-b602-eb78e2c48d1a",
   "metadata": {},
   "source": [
    "Let's create a dummy subscriber and test it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d858ee-83c4-4e91-8300-42ecb9d3f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "class Subscriber():\n",
    "    order = 0\n",
    "\n",
    "class DummyS(Subscriber):\n",
    "    \n",
    "    def before_fit(self, learn):\n",
    "        print('before fitüëã')\n",
    "        \n",
    "    def after_fit(self, learn):\n",
    "        print('after fitüëã')\n",
    "        \n",
    "    def before_epoch(self, learn):\n",
    "        print('before epoch üí•')\n",
    "        \n",
    "    def after_epoch(self, learn):\n",
    "        print('after epoch üí•')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579664a1-a60f-4864-94a4-992e93ddf062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before fitüëã\n",
      "before epoch üí•\n",
      "after epoch üí•\n",
      "before epoch üí•\n",
      "after epoch üí•\n",
      "after fitüëã\n"
     ]
    }
   ],
   "source": [
    "l = Learner(get_model(), dls, F.cross_entropy, accuracy, torch.optim.SGD, lr, [DummyS()])\n",
    "l.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c65276",
   "metadata": {},
   "source": [
    "## Subscribers can cancel execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f7433f-8214-4ffc-9a93-84a59c0d5689",
   "metadata": {},
   "source": [
    "Now let's add the last component of our pubsub system: subscribers should be able to cancel processing. For example, a a subscriber that would implement Early Stopping, will have to be able to cancel any further epochs when the validation loss starts increasing. One way to implement this, is with the help of `Exceptions` and `try` / `except` blocks:\n",
    "\n",
    "It's actually very easy to implement this logic, we only need to define custom `Exceptions`, and update the `PublishEvents` class to catch the exceptions that are thrown in any subscriber:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed18b8e-140a-4d85-9541-0438a298bf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CancelFitException(Exception): pass\n",
    "class CancelEpochException(Exception): pass\n",
    "class CancelBatchException(Exception): pass\n",
    "\n",
    "\n",
    "class PublishEvents():\n",
    "    def __init__(self, name): \n",
    "        self.name = name\n",
    "    \n",
    "    def __call__(self, decorated_fn):\n",
    "        def decorated_fn_with_publishing(learner, *args, **kwargs):\n",
    "            try:\n",
    "                learner.publish(f'before_{self.name}')\n",
    "                decorated_fn(learner, *args, **kwargs)\n",
    "                learner.publish(f'after_{self.name}')\n",
    "            except globals()[f'Cancel{self.name.title()}Exception']: pass\n",
    "        return decorated_fn_with_publishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd31bc76-9424-40e3-9b01-8906e450291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyS(Subscriber):\n",
    "    \n",
    "    def before_fit(self, learn): print('before fitüëã')\n",
    "        \n",
    "    def before_epoch(self, learn): raise CancelFitException\n",
    "    \n",
    "    def after_fit(self, learn): print('after fit üëã')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192291d9-5019-42af-87bc-9f699d20e840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before fitüëã\n"
     ]
    }
   ],
   "source": [
    "l = Learner(get_model(), dls, F.cross_entropy, accuracy, torch.optim.SGD, lr, [DummyS()])\n",
    "l.fit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ee4b0a-f322-4f8a-a825-f64308e8fe99",
   "metadata": {},
   "source": [
    "And indeed, the `after_fit` event is never called, since the fit was cancelled during `before_epoch` by the dummy subscriber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30012d32-bae6-4465-97b2-20efb0a8a3aa",
   "metadata": {},
   "source": [
    "## Final version of `Learner`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58b28a6-af2d-4f66-8270-ed845d0fdb06",
   "metadata": {},
   "source": [
    "We are going to make some final changes to the Learner class:\n",
    "\n",
    "- factor out the computation of the following logic. This is practical to create subclasses of `Learner` with custom behavior:\n",
    "  - prediction: `self.predict()`\n",
    "  - loss: `self.get_loss()`\n",
    "  - backward pass: `self.backward()`\n",
    "  - stepping of weights: `self.step()`\n",
    "  - zeroing of gradients: `self.zero_grad()`\n",
    "- add a Subscriber argument to `fit`, these subs will only be added for the duration of the fit, and afterwards removed\n",
    "- add a couple of additional events (`after_predict`, `after_loss`, `after_backward` and `after_step`) to which subscribers can listen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0895300f-ea30-4cd4-a3f4-d6042dd17aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "class Learner():\n",
    "    def __init__(self, model, dls, loss_fn, optim_class, lr, subs):\n",
    "        self.model = model\n",
    "        self.dls = dls\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optim_class = optim_class\n",
    "        self.lr = lr\n",
    "        self.subs = subs\n",
    "    \n",
    "    def fit(self, epochs, train=True, valid=True, subs=[], lr=None):\n",
    "        for sub in subs: self.subs.append(sub)\n",
    "        self.n_epochs = epochs\n",
    "        self.epochs = range(self.n_epochs)\n",
    "        lr = self.lr if lr is None else lr\n",
    "        self.opt = self.optim_class(self.model.parameters(), lr)\n",
    "        try:\n",
    "            self._fit(train, valid)\n",
    "        finally:\n",
    "            for sub in subs: self.subs.remove(sub)\n",
    "                    \n",
    "    @PublishEvents('fit')\n",
    "    def _fit(self, train, valid):\n",
    "        for self.epoch in self.epochs:\n",
    "            if train: \n",
    "                self.one_epoch(True)\n",
    "            if valid:\n",
    "                with torch.no_grad():\n",
    "                    self.one_epoch(False)\n",
    "        \n",
    "    def one_epoch(self, train):\n",
    "        self.model.train(train)\n",
    "        self.dl = self.dls.train if train else self.dls.valid\n",
    "        self._one_epoch()\n",
    "        \n",
    "    @PublishEvents('epoch')\n",
    "    def _one_epoch(self):\n",
    "        for self.batch in self.dl: \n",
    "            self.one_batch()\n",
    "    \n",
    "    @PublishEvents('batch')\n",
    "    def one_batch(self):\n",
    "        self.predict()\n",
    "        self.publish('after_predict')\n",
    "        self.get_loss()\n",
    "        self.publish('after_loss')\n",
    "        if self.model.training:\n",
    "            self.backward()\n",
    "            self.publish('after_backward')\n",
    "            self.step()\n",
    "            self.publish('after_step')\n",
    "            self.zero_grad()\n",
    "        \n",
    "    def publish(self, event):\n",
    "        for sub in sorted(self.subs, key=attrgetter('order')):\n",
    "            method = getattr(sub, event, None)\n",
    "            if method is not None: method(self)\n",
    "            \n",
    "    def predict(self): \n",
    "        self.preds = self.model(self.batch[0])\n",
    "        \n",
    "    def get_loss(self): \n",
    "        self.loss = self.loss_fn(self.preds, self.batch[1])\n",
    "        \n",
    "    def backward(self): self.loss.backward()\n",
    "    def step(self): self.opt.step()\n",
    "    def zero_grad(self): self.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac23e86-e522-48e0-8818-24e1c57c296d",
   "metadata": {},
   "source": [
    "## Metrics Subscriber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a6d800-ec5d-4b23-bdee-c65f6d94084a",
   "metadata": {},
   "source": [
    "Since we took out the metrics, let's create a subscriber that adds that. We want the subscriber to be generic, to it should be able to accept one or multiple metrics. Let's make sure that it can accept the metrics from the `torcheval` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65095b58-d78e-4977-bb08-718ecbe305fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No calls to update() have been made - returning 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.5000, dtype=torch.float64)\n",
      "tensor(0., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "metric = tem.Mean()\n",
    "\n",
    "metric.update(torch.tensor([1,2,3]))  # update() adds data\n",
    "metric.update(torch.tensor([4,5,6]))  \n",
    "print(metric.compute())               # compute() computes the metric\n",
    "\n",
    "metric.reset()                        # remove all data\n",
    "print(metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abe946d-c885-42c8-980a-eb95dda2e7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    " #|export\n",
    "class MetricsS(Subscriber):\n",
    "    def __init__(self, **metrics):\n",
    "        self.metrics = metrics\n",
    "        self.loss = tem.Mean()\n",
    "        \n",
    "    def before_fit(self, learn): \n",
    "        learn.metrics = self\n",
    "    \n",
    "    def before_epoch(self, learn):\n",
    "        for m in self.metrics.values(): m.reset()\n",
    "        self.loss.reset()\n",
    "    \n",
    "    def after_batch(self, learn):\n",
    "        x,y,*_ = self.to_cpu(learn.batch)\n",
    "        for m in self.metrics.values(): m.update(self.to_cpu(learn.preds), y)\n",
    "        self.loss.update(self.to_cpu(learn.loss), weight=len(x))\n",
    "        \n",
    "    def after_epoch(self, learn):\n",
    "        log = {\n",
    "            'epoch': learn.epoch,\n",
    "            'mode': 'train' if learn.model.training else 'eval',\n",
    "            'loss' : f'{self.loss.compute():.3f}'\n",
    "        }\n",
    "        for k, v in self.metrics.items():\n",
    "            log[k] = f'{v.compute():.3f}'\n",
    "        self.output(log)\n",
    "        \n",
    "    def to_cpu(self, x):\n",
    "        if isinstance(x, list): return (self.to_cpu(el) for el in x)\n",
    "        return x.detach().cpu()\n",
    "        \n",
    "    def output(self, log): print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bab58f-f480-4874-9bb4-05d0ae48c320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'mode': 'train', 'loss': '2.220', 'accuracy': '0.206'}\n",
      "{'epoch': 0, 'mode': 'eval', 'loss': '2.121', 'accuracy': '0.352'}\n"
     ]
    }
   ],
   "source": [
    "metrics_s = MetricsS(accuracy=tem.MulticlassAccuracy())\n",
    "\n",
    "l = Learner(get_model(), dls, F.cross_entropy, torch.optim.SGD, lr, [metrics_s])\n",
    "l.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e12e33f-8d74-4ec7-a9bc-cba35018aca8",
   "metadata": {},
   "source": [
    "## Device Subscriber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df01499-ee64-42e3-bbf0-9f36c8f98ec0",
   "metadata": {},
   "source": [
    "It's time we start training on the GPU, to do that we have to move the model (and it's parameters) as well as all the data onto the GPU. We can easily do this with a Subscriber:\n",
    "\n",
    "- move the model (and all it's trainable parameters) to the device **before fit**\n",
    "- move each batch to the device **before batch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28deeb63-1eea-4197-9bb7-52f119b63937",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "class DeviceS(Subscriber):\n",
    "    \n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "    \n",
    "    def before_fit(self, learn):\n",
    "        learn.model.to(self.device)\n",
    "    \n",
    "    def before_batch(self, learn):\n",
    "        learn.batch = [x.to(self.device) for x in learn.batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b0fbe6-d595-498e-b430-c2fbb21db715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'mode': 'train', 'loss': '2.209', 'accuracy': '0.256'}\n",
      "{'epoch': 0, 'mode': 'eval', 'loss': '2.115', 'accuracy': '0.306'}\n"
     ]
    }
   ],
   "source": [
    "device_s = DeviceS(device)\n",
    "\n",
    "l = Learner(get_model(), dls, F.cross_entropy, torch.optim.SGD, lr, [metrics_s, device_s])\n",
    "l.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519fe2c1-cc86-4e95-bd40-cd8639032e42",
   "metadata": {},
   "source": [
    "## Learning Rate Finder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121973e8-ea1a-4fed-892c-ed1722d89f54",
   "metadata": {},
   "source": [
    "The learning rate finder is very simple technique that can be used to find a good learning rate for training a network. It works like this:\n",
    "\n",
    "- Start with a very small learning rate \n",
    "- Do a forward pass through the network of a single batch of data and record the loss\n",
    "- Increase the learning rate with constant factor\n",
    "- Do another forward pass through the network of a single batch and record the loss\n",
    "- Continue to do this until at some point the loss \"explodes\": for example because the current loss is 3 times as large as the minimum loss recorded so far\n",
    "\n",
    "After this, we plot the learning rate vs the recorded losses and look for a learning rate at which the loss is decreasing the most (i.e. the point where the loss has the smallest derivative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c54e67-4395-489c-bad6-75b5450ff947",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "class LRFindS(Subscriber):\n",
    "    \n",
    "    def __init__(self, mult=1.25):\n",
    "        self.mult = mult\n",
    "        self.min = math.inf\n",
    "        \n",
    "    def before_epoch(self, learn):\n",
    "        if not learn.model.training: raise CancelFitException\n",
    "        self.losses = []\n",
    "        self.lrs = []\n",
    "    \n",
    "    def after_loss(self, learn):\n",
    "        lr = learn.opt.param_groups[0]['lr']\n",
    "        self.lrs.append(lr)\n",
    "        loss = learn.loss.detach().cpu()\n",
    "        self.losses.append(loss)\n",
    "        if loss < self.min: self.min = loss\n",
    "        if loss > self.min*3: raise CancelFitException()\n",
    "        for g in learn.opt.param_groups: g['lr'] = lr * self.mult\n",
    "        \n",
    "    def plot(self):\n",
    "        plt.plot(self.lrs, self.losses)\n",
    "        plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c876023-1d58-4675-925f-fb85a07d09e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = Learner(get_model(), dls, F.cross_entropy, torch.optim.SGD, lr, [metrics_s, device_s])\n",
    "\n",
    "lrfind_s = LRFindS()\n",
    "l.fit(5, lr=1e-4, subs=[lrfind_s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7ed384-4de9-4870-9624-0d472f33973f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrfind_s.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7c415c-111c-40c9-9e37-b4a40cec39d1",
   "metadata": {},
   "source": [
    "From which we see that a learning rate of around 0.2 would be best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c0cd7-e748-47f1-99b6-1543d65cb985",
   "metadata": {},
   "source": [
    "## MomentumLearner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f293ce-5214-4973-b43c-6221bb7bbaf8",
   "metadata": {},
   "source": [
    "Additionally, we can easily subclass `Learner`, and implement custom functionality into any of its 5 main functionalities:\n",
    "\n",
    "- prediction: `self.predict()`\n",
    "- loss: `self.get_loss()`\n",
    "- backward pass: `self.backward()`\n",
    "- stepping of weights: `self.step()`\n",
    "- zeroing of gradients: `self.zero_grad()`\n",
    "\n",
    "For example we can create a `MomentumLearner` which doesn't just use the gradient of the last backward pass, but uses an exponentially weighted average of all previously computed gradients. We can do this by not zeroing out the gradients, but just reduce them by a factor between 0 and 1 (the momentum parameter). This way the \"gradient with momentum\" at time $t$ ($m_t$), will be a function of the normal gradient ($g_t$): \n",
    "\n",
    "$$\n",
    "m_t = g_t + c \\cdot g_{t-1} + c^2 \\cdot g_{t-2} + ...\n",
    "$$\n",
    "\n",
    "This is called momentum and the idea is to add a sense of \"inertia\" to the gradients, i.e. if in one step we are moving through the loss manifold in a certain direction, then in the next step we want to keep moving somewhat in that direction irrespective of the gradient of the current step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd08951-b87d-4537-ad19-036bdac4b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "class MomentumLearner(Learner):\n",
    "    \n",
    "    def __init__(self, model, dls, loss_fn, optim_class, lr, subs, mom=0.85):\n",
    "        self.mom = mom\n",
    "        super().__init__(model, dls, loss_fn, optim_class, lr, subs)\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.model.parameters(): p.grad *= self.mom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016f9530-a940-46d0-bc84-0299857f583d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'mode': 'train', 'loss': '1.657', 'accuracy': '0.481'}\n",
      "{'epoch': 0, 'mode': 'eval', 'loss': '1.105', 'accuracy': '0.647'}\n",
      "{'epoch': 1, 'mode': 'train', 'loss': '0.925', 'accuracy': '0.685'}\n",
      "{'epoch': 1, 'mode': 'eval', 'loss': '0.831', 'accuracy': '0.695'}\n",
      "{'epoch': 2, 'mode': 'train', 'loss': '0.763', 'accuracy': '0.732'}\n",
      "{'epoch': 2, 'mode': 'eval', 'loss': '0.736', 'accuracy': '0.739'}\n",
      "{'epoch': 3, 'mode': 'train', 'loss': '0.686', 'accuracy': '0.764'}\n",
      "{'epoch': 3, 'mode': 'eval', 'loss': '0.675', 'accuracy': '0.766'}\n",
      "{'epoch': 4, 'mode': 'train', 'loss': '0.635', 'accuracy': '0.784'}\n",
      "{'epoch': 4, 'mode': 'eval', 'loss': '0.634', 'accuracy': '0.779'}\n"
     ]
    }
   ],
   "source": [
    "l = MomentumLearner(get_model(), dls, F.cross_entropy, torch.optim.SGD, lr, [metrics_s, device_s])\n",
    "l.fit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d574c-8d7e-48da-93c2-991b9af31adc",
   "metadata": {},
   "source": [
    "And this simple technique has a pretty good effect on training our model: the accuracy on the validation set is increasing (even with this simple linear model) from 66% with normal SGD to 78% with momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eb531e-f154-4ff8-83c2-5a0afec5dd0a",
   "metadata": {},
   "source": [
    "## Closing remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f96f48-7ba9-4042-9de4-cc279a89c2be",
   "metadata": {},
   "source": [
    "In this post we have again covered a lot of ground. We have created a very flexible Learner framework, making heavy use of a pubsub system to customize the training loop. As examples we have seen a Subscriber that enables training on the GPU, and another one that takes care of tracking the loss and the metrics while we are training. Additionally we have implement the learning rate finder as a Subscriber, and last but not least we have seen how we can subclass the Learner class to create custom learners that for example implement momentum. Below I have added one additional Subscriber that displays the progress of the loss a bit nicer in a graph, as well as puts the outputs in a table. It has been copied from the miniai library as is (with some minor changes to make it work for `nntrain`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c37493-2b7b-4654-9fc1-ac6ac59d4f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "\n",
    "class ProgressS(Subscriber):\n",
    "    order = MetricsS.order+1\n",
    "    def __init__(self, plot=False): self.plot = plot\n",
    "    def before_fit(self, learn):\n",
    "        learn.epochs = self.mbar = master_bar(learn.epochs)\n",
    "        self.first = True\n",
    "        if hasattr(learn, 'metrics'): learn.metrics.output = self.output\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def output(self, d):\n",
    "        if self.first:\n",
    "            self.mbar.write(list(d), table=True)\n",
    "            self.first = False\n",
    "        self.mbar.write(list(d.values()), table=True)\n",
    "\n",
    "    def before_epoch(self, learn): learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)\n",
    "    \n",
    "    def after_batch(self, learn):\n",
    "        learn.dl.comment = f'{learn.loss:.3f}'\n",
    "        if self.plot and hasattr(learn, 'metrics') and learn.model.training:\n",
    "            self.losses.append(learn.loss.item())\n",
    "            if self.val_losses: self.mbar.update_graph([[fc.L.range(self.losses), self.losses],[fc.L.range(learn.epoch).map(lambda x: (x+1)*len(learn.dls.train)), self.val_losses]])\n",
    "    \n",
    "    def after_epoch(self, learn): \n",
    "        if not learn.model.training:\n",
    "            if self.plot and hasattr(learn, 'metrics'): \n",
    "                self.val_losses.append(learn.metrics.loss.compute())\n",
    "                self.mbar.update_graph([[fc.L.range(self.losses), self.losses],[fc.L.range(learn.epoch+1).map(lambda x: (x+1)*len(learn.dls.train)), self.val_losses]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e7cf7e-ca6b-477a-a3e3-ccc72cacd9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>mode</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>1.818</td>\n",
       "      <td>0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>eval</td>\n",
       "      <td>1.192</td>\n",
       "      <td>0.640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>eval</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>eval</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>eval</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>eval</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "progress_s = ProgressS(True)\n",
    "\n",
    "l = MomentumLearner(get_model(), dls, F.cross_entropy, torch.optim.SGD, lr, [metrics_s, device_s, progress_s])\n",
    "l.fit(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
